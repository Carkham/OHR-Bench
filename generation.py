import argparse, os
from loguru import logger
from src.datasets.dataset import get_task_datasets
from evaluator import StageEvaluator
from src.llms import GPT
from src.llms import Qwen2_7B_Instruct, LLaMA31_8B_Instruct, Qwen25_7B_Instruct, Qwen2VL_7B_Instruct, InternVL2_8B_Instruct

from src.tasks.quest_answer import QuestAnswer, QuestAnswer_image, QuestAnswer_OCR, QuestAnswer_image_OCR

parser = argparse.ArgumentParser()

# Model related options
parser.add_argument('--model_name', default='qwen7b', help="Name of the model to use")
parser.add_argument('--temperature', type=float, default=0.1, help="Controls the randomness of the model's text generation")
parser.add_argument('--max_new_tokens', type=int, default=1280, help="Maximum number of new tokens to be generated by the model")

# Dataset related options
parser.add_argument('--data_path', default='data/qa/MinerU/finance.json', help="Path to the dataset")
parser.add_argument('--shuffle', type=bool, default=True, help="Whether to shuffle the dataset")
parser.add_argument('--ocr_type', type=str, default="MinerU")
parser.add_argument('--doc_type', type=str, default="paper")
parser.add_argument('--output_path', type=str, default="./output/generation")

# Metric related options
parser.add_argument('--quest_eval', action='store_true', help="Whether to use QA metrics(RAGQuestEval)")
parser.add_argument('--bert_score_eval', action='store_true', help="Whether to use bert_score metrics")

# Evaluation related options
parser.add_argument('--task', default='event_summary', help="Task to perform")
parser.add_argument('--num_threads', type=int, default=1, help="Number of threads")
parser.add_argument('--show_progress_bar', action='store', default=True, type=bool, help="Whether to show a progress bar")
parser.add_argument('--contain_original_data', action='store_true', help="Whether to contain original data")

args = parser.parse_args()
logger.info(args)

def setup_seed(seed):
    import torch
    import numpy as np
    import random
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

setup_seed(0)

if args.model_name.startswith("gpt"):
    llm = GPT(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens)
elif args.model_name == "qwen2_7b":
    llm = Qwen2_7B_Instruct(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens,
                            top_p=0.8, top_k=10)
elif args.model_name == "qwen25_7b":
    llm = Qwen25_7B_Instruct(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens,
                            top_p=0.8, top_k=10)
elif args.model_name == "llama3.1_8b":
    llm = LLaMA31_8B_Instruct(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens,
                            top_p=0.8, top_k=10)
elif args.model_name == "qwen2vl_7b":
    llm = Qwen2VL_7B_Instruct(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens,
                            top_p=0.8, top_k=10)
elif args.model_name == "internvl2_8b":
    llm = InternVL2_8B_Instruct(model_name=args.model_name, temperature=args.temperature, max_new_tokens=args.max_new_tokens,
                            top_p=0.8, top_k=10)
elif args.model_name == "mock":
    llm = None

retriever = None

task_mapping = {
    'QA': [QuestAnswer],
    'QA_image': [QuestAnswer_image],
    'QA_OCR': [QuestAnswer_OCR],
    'QA_image_OCR': [QuestAnswer_image_OCR]
}

if args.task not in task_mapping:
    raise ValueError(f"Unknown task: {args.task}")

tasks = [task() for task in task_mapping[args.task]]

datasets = get_task_datasets(args.data_path, args.task)

for task, dataset in zip(tasks, datasets):
    evaluator = StageEvaluator(task, llm, retriever, dataset, 
                               output_dir=os.path.join(args.output_path, args.ocr_type),
                               output_name=args.doc_type,
                               num_threads=args.num_threads, stage="generation")
    results = evaluator.run(show_progress_bar=args.show_progress_bar, contain_original_data=args.contain_original_data)