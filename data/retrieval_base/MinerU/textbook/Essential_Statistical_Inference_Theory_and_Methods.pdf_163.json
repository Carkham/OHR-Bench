[
    {
        "page_idx": 0,
        "text": "Table3.3  $P$  -valuesfor three samples,  $n_{1}=n_{2}=n_{3}=5,$   $\\sigma^{2}=14$  \n\\begin{tabular}{llllll}\n\\hline \n$\\overline{Y}_1$ & $\\overline{Y}_2$ & $\\overline{Y}_3$ & ANOVA & ISO & Reg \\\\\\hline \n2 & 6 & 6 & 0.149 & 0.050 & 0.045\\\\\n2 & 6 & 7 & 0.082 & 0.026 & 0.017\\\\\n2 & 6 & 8 & 0.036 & 0.011 & 0.006\\\\\n\\hline \n\\end{tabular}\n\nrelative ease of using regression is probably why isotonic regression is not used more. \nFinally,althoughthe  $p$  -values reported in Table 3.3arefortheknown-variance case,qualitatively similar results are obtained for the more complicated case of  $\\sigma$  unknown.  $\\diamond$  \nThere is a large literature on order-restricted inference.For testing for ordered alternatives,there has been more emphasis on  $T_{\\mathrm{LR}}$  thanOn  $T_{\\mathrm{{W}}}$  and  $T_{S}$  .Theclassic references are Barlow et al.(1972) andRobertson et al.(1988),whereas a more recent account is Silva pull e and Sen(2005) \n# \nWhen a null hypothesis value,say  $\\pmb{\\theta}_{0}$  liesontheboundaryoftheparameterspace, then maximum likelihood estimators are often truncated at that boundary because by definition  $\\widehat{\\pmb{\\theta}}_{\\mathrm{MLE}}$  mustliein theparameter space of  $\\theta$  .Thus  $\\widehat{\\pmb{\\theta}}_{\\mathrm{MLE}}$  is equal to the boundary value  $\\pmb{\\theta}_{0}$  with positive probability and correspondingly  $T_{\\mathrm{LR}}$  iszero for thosecases.Theresult isthat the limiting distribution of  $T_{\\mathrm{LR}}$  is a mixture ofa point mass at zero and achi-squared distribution.We illustrate first with an artificial example and then consider the one-way random effects model. \n# \nSupposethat  $Y_{1},.\\,.\\,.\\,,Y_{n}~\\sim~N(\\mu,1)$  Usually,  $\\widehat{\\mu}_{\\mathrm{MLE}}\\;=\\;\\overline{{Y}}$  ,but suppose thatwe restrict the parameter space for  $\\mu$  tobe  $[\\mu_{0},\\infty)$  where  $\\mu_{0}$  is some given constant, insteadof  $(-\\infty,\\infty)$  .Then  $\\widehat{\\mu}_{\\mathrm{MLE}}\\,=\\,\\overline{{Y}}$  if  $\\overline{{Y}}\\ \\geq\\ \\mu_{0}$  and  $\\widehat{\\mu}_{\\mathrm{MLE}}\\,=\\,\\mu_{0}$  if  $\\overline{{Y}}\\,<\\,\\mu_{0}$  Now suppose that the null hypothesisis  $H_{0}:\\mu=\\mu_{0}$  We first consider the three likelihood-based test statistics,showingthat only thescore statistichas a limiting\n\n  $\\chi_{1}^{2}$  distribution.Then we provide a simple solution to this testing problem \nUnder  $H_{0}$  ,theWald statisticis  $T_{\\mathrm{W}}=n(\\widehat{\\mu}_{\\mathrm{MLE}}-\\mu_{0})^{2}$  ,whichisthus  $T_{\\mathrm{W}}=0$  if\n\n  $\\widehat{\\mu}_{\\mathrm{MLE}}=\\mu_{0}$  and  $T_{\\mathrm{W}}=n(\\overline{{Y}}\\!-\\!\\mu_{0})^{2}$  if  $Y\\geq\\mu_{0}$  The score statistic is  $T_{\\mathrm{S}}=n(Y\\!-\\!\\mu_{0})^{2}$  and the likelihood ratio statistic is the same as the Wald statistic.Thus,onlythe score statistic converges toa  $\\chi_{1}^{2}$  distribution under  $H_{0}$  .The Wald and the likelihood ratio statistics converge to a distribution that is an equal mixture of a point mass at0anda  $\\chi_{1}^{2}$  distribution,the same distribution asin(3.23)for  $k=2$  Infactthe"
    }
]