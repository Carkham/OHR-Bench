[
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00073cc2-c801-467c-9039-fca63c78c6a9",
        "questions": "What is the role of the pseudo matching matrix $\\mathbf{Y}^{N V}$ in the S-N-V path?",
        "answers": "The pseudo matching matrix $\\mathbf{Y}^{N V}$ is constructed based on the timestamps of narrations.",
        "context": "of narrations, long-term global text-video semantic similarity, and short-term fine-grained text-video semantic similarity.\n\nThe S-N-V path first identifies semantically similar narrations of $\\mathcal{S}$, and then associates $\\mathcal{S}$ with the video frames that are within the timestamps of these narrations. Formally, the semantic similarity distributions between the $\\mathcal{S}$ and $\\mathcal{N}$ of a video are computed as below:\n$$\\mathbf{A}_{S N}=\\operatorname{Softmax}\\left(\\frac{E_{t}(\\mathcal{S}) \\cdot E_{t}(\\mathcal{N})^{\\top}}{\\tau}\\right)$$\nwhere $\\mathbf{A}_{S N} \\in \\mathbb{R}^{L \\times K}, E_{t}(\\mathcal{S}) \\in \\mathbb{R}^{L \\times D}$, and $E_{t}(\\mathcal{N}) \\in \\mathbb{R}^{K \\times D}$. Note that $E_{t}(\\mathcal{S})$ and $E_{t}(\\mathcal{N})$ are the L2 normalized embeddings of the $\\mathcal{S}$ and $\\mathcal{N}$ extracted using a pretrained text encoder $E_{t}$, respectively. $\\tau$ is the temperature. Then, the alignment score between LLM steps and video frames can be obtained as follows:\n$$\\mathbf{A}_{S N V}=\\mathbf{A}_{S N} \\cdot \\mathbf{Y}^{N V} \\in \\mathbb{R}^{L \\times T}$$\n$\\mathbf{Y}^{N V}$ is the pseudo matching matrix constructed based on the timestamps of narrations, as introduced in Equation 2.\n\nSince the temporal alignment derived from timestamps can be noisy, we incorporate additional information by directly aligning LLM steps and video segments. This is achieved by using a long-term video-text alignment model pre-trained using the instructional video data.\n\nSpecifically, we first use the text encoder $E_{t}^{L}$ and video encoder $E_{v}^{L}$ of the model to extract the embeddings of the $\\mathcal{S}$ and $\\mathcal{V}$, respectively. The cosine similarity between each LLM-step and video segment is calculated, resulting in the long-term step-to-video alignment score matrix $\\mathbf{A}_{S V}^{\\text {long }}$ :\n$$\\mathbf{A}_{S V}^{\\text {long }}=E_{t}^{L}(\\mathcal{S}) \\cdot E_{v}^{L}(\\mathcal{V})^{\\top}$$\n\nWe also obtained additional direct alignment between LLM steps and videos, denoted as $\\mathbf{A}_{S V}^{\\text {short }}$, using the video foundation models pre-trained on general short-term video-text data. $\\mathbf{A}_{S V}^{\\text {short }}$ is obtained follow the same way as Equation 6 , but use the text encoders $E_{t}^{S}$ and $E_{v}^{S}$ of the video foundation model. This pathway complements the alignment by focusing on short-term, fine-grained information, such as human-object interactions. Furthermore, this pathway introduces knowledge learned from a broader domain.\n\nThe multiple pathways alignment score matrix $\\mathbf{A}_{\\text {mult }}$ is obtained by meanpooling $\\mathbf{A}_{S N V}, \\mathbf{A}_{S V}^{\\text {long }}$, and $\\mathbf{A}_{S V}^{\\text {short }}$.\n\nDiscussion. Previous works typically generate pseudo-labels by focusing on a single path, either $\\mathbf{A}_{S N V}$ or $\\mathbf{A}_{S V}^{\\text {long }}$. It potentially results in labels that are affected by the noise present in each pathway. In contrast, our method leverages information from multiple paths, thereby enabling the generation of more reliable pseudo-alignments. Furthermore, our proposed $\\mathbf{A}_{S V}^{\\text {short }}$ introduces additional knowledge beyond the instructional video domain. Our method shares the same idea with TAN [21] that uses the mutual agreement between different models to filter noise in pseudo-labels. However, our approach leverages a broader range of complementary information through multiple alignment pathways and models pre-trained on a variety of datasets.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "$\\mathbf{Y}^{N V}$ is the pseudo matching matrix constructed based on the timestamps of narrations, as introduced in Equation 2.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "000b6710-f8b4-4dd4-9913-90c7d424fccf",
        "questions": "What is the purpose of using the long-term video-text alignment model in the document's method?",
        "answers": "The long-term video-text alignment model is used to directly align LLM steps and video segments.",
        "context": "of narrations, long-term global text-video semantic similarity, and short-term fine-grained text-video semantic similarity.\n\nThe S-N-V path first identifies semantically similar narrations of $\\mathcal{S}$, and then associates $\\mathcal{S}$ with the video frames that are within the timestamps of these narrations. Formally, the semantic similarity distributions between the $\\mathcal{S}$ and $\\mathcal{N}$ of a video are computed as below:\n$$\\mathbf{A}_{S N}=\\operatorname{Softmax}\\left(\\frac{E_{t}(\\mathcal{S}) \\cdot E_{t}(\\mathcal{N})^{\\top}}{\\tau}\\right)$$\nwhere $\\mathbf{A}_{S N} \\in \\mathbb{R}^{L \\times K}, E_{t}(\\mathcal{S}) \\in \\mathbb{R}^{L \\times D}$, and $E_{t}(\\mathcal{N}) \\in \\mathbb{R}^{K \\times D}$. Note that $E_{t}(\\mathcal{S})$ and $E_{t}(\\mathcal{N})$ are the L2 normalized embeddings of the $\\mathcal{S}$ and $\\mathcal{N}$ extracted using a pretrained text encoder $E_{t}$, respectively. $\\tau$ is the temperature. Then, the alignment score between LLM steps and video frames can be obtained as follows:\n$$\\mathbf{A}_{S N V}=\\mathbf{A}_{S N} \\cdot \\mathbf{Y}^{N V} \\in \\mathbb{R}^{L \\times T}$$\n$\\mathbf{Y}^{N V}$ is the pseudo matching matrix constructed based on the timestamps of narrations, as introduced in Equation 2.\n\nSince the temporal alignment derived from timestamps can be noisy, we incorporate additional information by directly aligning LLM steps and video segments. This is achieved by using a long-term video-text alignment model pre-trained using the instructional video data.\n\nSpecifically, we first use the text encoder $E_{t}^{L}$ and video encoder $E_{v}^{L}$ of the model to extract the embeddings of the $\\mathcal{S}$ and $\\mathcal{V}$, respectively. The cosine similarity between each LLM-step and video segment is calculated, resulting in the long-term step-to-video alignment score matrix $\\mathbf{A}_{S V}^{\\text {long }}$ :\n$$\\mathbf{A}_{S V}^{\\text {long }}=E_{t}^{L}(\\mathcal{S}) \\cdot E_{v}^{L}(\\mathcal{V})^{\\top}$$\n\nWe also obtained additional direct alignment between LLM steps and videos, denoted as $\\mathbf{A}_{S V}^{\\text {short }}$, using the video foundation models pre-trained on general short-term video-text data. $\\mathbf{A}_{S V}^{\\text {short }}$ is obtained follow the same way as Equation 6 , but use the text encoders $E_{t}^{S}$ and $E_{v}^{S}$ of the video foundation model. This pathway complements the alignment by focusing on short-term, fine-grained information, such as human-object interactions. Furthermore, this pathway introduces knowledge learned from a broader domain.\n\nThe multiple pathways alignment score matrix $\\mathbf{A}_{\\text {mult }}$ is obtained by meanpooling $\\mathbf{A}_{S N V}, \\mathbf{A}_{S V}^{\\text {long }}$, and $\\mathbf{A}_{S V}^{\\text {short }}$.\n\nDiscussion. Previous works typically generate pseudo-labels by focusing on a single path, either $\\mathbf{A}_{S N V}$ or $\\mathbf{A}_{S V}^{\\text {long }}$. It potentially results in labels that are affected by the noise present in each pathway. In contrast, our method leverages information from multiple paths, thereby enabling the generation of more reliable pseudo-alignments. Furthermore, our proposed $\\mathbf{A}_{S V}^{\\text {short }}$ introduces additional knowledge beyond the instructional video domain. Our method shares the same idea with TAN [21] that uses the mutual agreement between different models to filter noise in pseudo-labels. However, our approach leverages a broader range of complementary information through multiple alignment pathways and models pre-trained on a variety of datasets.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Since the temporal alignment derived from timestamps can be noisy, we incorporate additional information by directly aligning LLM steps and video segments. This is achieved by using a long-term video-text alignment model pre-trained using the instructional video data.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00183cfe-ceb0-4220-b984-f33f61c61ae4",
        "questions": "Does the document's method rely solely on the alignment score matrix $\\mathbf{A}_{S N V}$ for generating pseudo-labels?",
        "answers": "No",
        "context": "of narrations, long-term global text-video semantic similarity, and short-term fine-grained text-video semantic similarity.\n\nThe S-N-V path first identifies semantically similar narrations of $\\mathcal{S}$, and then associates $\\mathcal{S}$ with the video frames that are within the timestamps of these narrations. Formally, the semantic similarity distributions between the $\\mathcal{S}$ and $\\mathcal{N}$ of a video are computed as below:\n$$\\mathbf{A}_{S N}=\\operatorname{Softmax}\\left(\\frac{E_{t}(\\mathcal{S}) \\cdot E_{t}(\\mathcal{N})^{\\top}}{\\tau}\\right)$$\nwhere $\\mathbf{A}_{S N} \\in \\mathbb{R}^{L \\times K}, E_{t}(\\mathcal{S}) \\in \\mathbb{R}^{L \\times D}$, and $E_{t}(\\mathcal{N}) \\in \\mathbb{R}^{K \\times D}$. Note that $E_{t}(\\mathcal{S})$ and $E_{t}(\\mathcal{N})$ are the L2 normalized embeddings of the $\\mathcal{S}$ and $\\mathcal{N}$ extracted using a pretrained text encoder $E_{t}$, respectively. $\\tau$ is the temperature. Then, the alignment score between LLM steps and video frames can be obtained as follows:\n$$\\mathbf{A}_{S N V}=\\mathbf{A}_{S N} \\cdot \\mathbf{Y}^{N V} \\in \\mathbb{R}^{L \\times T}$$\n$\\mathbf{Y}^{N V}$ is the pseudo matching matrix constructed based on the timestamps of narrations, as introduced in Equation 2.\n\nSince the temporal alignment derived from timestamps can be noisy, we incorporate additional information by directly aligning LLM steps and video segments. This is achieved by using a long-term video-text alignment model pre-trained using the instructional video data.\n\nSpecifically, we first use the text encoder $E_{t}^{L}$ and video encoder $E_{v}^{L}$ of the model to extract the embeddings of the $\\mathcal{S}$ and $\\mathcal{V}$, respectively. The cosine similarity between each LLM-step and video segment is calculated, resulting in the long-term step-to-video alignment score matrix $\\mathbf{A}_{S V}^{\\text {long }}$ :\n$$\\mathbf{A}_{S V}^{\\text {long }}=E_{t}^{L}(\\mathcal{S}) \\cdot E_{v}^{L}(\\mathcal{V})^{\\top}$$\n\nWe also obtained additional direct alignment between LLM steps and videos, denoted as $\\mathbf{A}_{S V}^{\\text {short }}$, using the video foundation models pre-trained on general short-term video-text data. $\\mathbf{A}_{S V}^{\\text {short }}$ is obtained follow the same way as Equation 6 , but use the text encoders $E_{t}^{S}$ and $E_{v}^{S}$ of the video foundation model. This pathway complements the alignment by focusing on short-term, fine-grained information, such as human-object interactions. Furthermore, this pathway introduces knowledge learned from a broader domain.\n\nThe multiple pathways alignment score matrix $\\mathbf{A}_{\\text {mult }}$ is obtained by meanpooling $\\mathbf{A}_{S N V}, \\mathbf{A}_{S V}^{\\text {long }}$, and $\\mathbf{A}_{S V}^{\\text {short }}$.\n\nDiscussion. Previous works typically generate pseudo-labels by focusing on a single path, either $\\mathbf{A}_{S N V}$ or $\\mathbf{A}_{S V}^{\\text {long }}$. It potentially results in labels that are affected by the noise present in each pathway. In contrast, our method leverages information from multiple paths, thereby enabling the generation of more reliable pseudo-alignments. Furthermore, our proposed $\\mathbf{A}_{S V}^{\\text {short }}$ introduces additional knowledge beyond the instructional video domain. Our method shares the same idea with TAN [21] that uses the mutual agreement between different models to filter noise in pseudo-labels. However, our approach leverages a broader range of complementary information through multiple alignment pathways and models pre-trained on a variety of datasets.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Previous works typically generate pseudo-labels by focusing on a single path, either $\\mathbf{A}_{S N V}$ or $\\mathbf{A}_{S V}^{\\text {long }}$. It potentially results in labels that are affected by the noise present in each pathway. In contrast, our method leverages information from multiple paths, thereby enabling the generation of more reliable pseudo-alignments.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "001b10c4-0ec1-48c9-a059-0a44159119bd",
        "questions": "Which university is associated with Yuxiao Chen and Dimitris N. Metaxas in the research on localizing actions in instructional videos?",
        "answers": "Rutgers University",
        "context": "Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment\n\n\n\nYuxiao Chen ${ ^{1}$, Kai Li ${ }^{2}$, Wentao Bao ${ }^{4}$, Deep Patel ${ }^{3}$, Yu Kong ${ }^{4}$, Martin Renqiang Min ${ }^{3}$, and Dimitris N. Metaxas ${ }^{1}$ \\\\ ${ }^{1}$ Rutgers University \\{yc984, dnm\\}@cs.rutgers.edu, \\\\ ${ }^{2}$ Meta \\{li.gml.kai@gmail.com\\} \\\\ ${ }^{3}$ NEC Labs America-Princeton \\{dpatel, renqiang\\}@nec-labs.com \\\\ ${ }^{4}$ Michigan State University \\{baowenta, yukong\\}@msu.edu\n}\n\n\nLearning to localize temporal boundaries of procedure steps in instructional videos is challenging due to the limited availability of annotated large-scale training videos. Recent works focus on learning the cross-modal alignment between video segments and ASR-transcripted narration texts through contrastive learning. However, these methods fail to account for the alignment noise, i.e., irrelevant narrations to the instructional task in videos and unreliable timestamps in narrations. To address these challenges, this work proposes a novel training framework. Motivated by the strong capabilities of Large Language Models (LLMs) in procedure understanding and text summarization, we first apply an LLM to filter out task-irrelevant information and summarize task-related procedure steps (LLM-steps) from narrations. To further generate reliable pseudo-matching between the LLM-steps and the video for training, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy. The key idea is to measure alignment between LLM-steps and videos via multiple pathways, including: (1) step-narration-video alignment using narration timestamps, (2) direct step-to-video alignment based on their long-term semantic similarity, and (3) direct step-to-video alignment focusing on short-term fine-grained semantic similarity learned from general video domains. The results from different pathways are fused to generate reliable pseudo step-video matching. We conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach surpasses state-of-the-art methods in three downstream tasks: procedure step grounding, step localization, and narration grounding by $5.9 \\%, 3.1 \\%$, and $2.8 \\%$.\n\n\n1 Introduction\n\nInstructional videos are a type of educational content presented in video format. They demonstrate the steps required to perform specific tasks, such as cooking, repairing cars, and applying makeup $[17,18,36,44]$. These videos are invaluable resources for individuals seeking to learn new skills or knowledge in various",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Yuxiao Chen ${ }^{1}$, Kai Li ${ }^{2}$, Wentao Bao ${ }^{4}$, Deep Patel ${ }^{3}$, Yu Kong ${ }^{4}$, Martin Renqiang Min ${ }^{3}$, and Dimitris N. Metaxas ${ }^{1}$ \\ ${ }^{1}$ Rutgers University \\ {yc984, dnm}@cs.rutgers.edu,",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "001cfd71-1996-4ffe-9382-4e7cd476ef88",
        "questions": "What percentage improvement does the proposed method achieve in the task of procedure step grounding compared to state-of-the-art methods?",
        "answers": "5.9%",
        "context": "Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment\n\n\n\nYuxiao Chen ${ ^{1}$, Kai Li ${ }^{2}$, Wentao Bao ${ }^{4}$, Deep Patel ${ }^{3}$, Yu Kong ${ }^{4}$, Martin Renqiang Min ${ }^{3}$, and Dimitris N. Metaxas ${ }^{1}$ \\\\ ${ }^{1}$ Rutgers University \\{yc984, dnm\\}@cs.rutgers.edu, \\\\ ${ }^{2}$ Meta \\{li.gml.kai@gmail.com\\} \\\\ ${ }^{3}$ NEC Labs America-Princeton \\{dpatel, renqiang\\}@nec-labs.com \\\\ ${ }^{4}$ Michigan State University \\{baowenta, yukong\\}@msu.edu\n}\n\n\nLearning to localize temporal boundaries of procedure steps in instructional videos is challenging due to the limited availability of annotated large-scale training videos. Recent works focus on learning the cross-modal alignment between video segments and ASR-transcripted narration texts through contrastive learning. However, these methods fail to account for the alignment noise, i.e., irrelevant narrations to the instructional task in videos and unreliable timestamps in narrations. To address these challenges, this work proposes a novel training framework. Motivated by the strong capabilities of Large Language Models (LLMs) in procedure understanding and text summarization, we first apply an LLM to filter out task-irrelevant information and summarize task-related procedure steps (LLM-steps) from narrations. To further generate reliable pseudo-matching between the LLM-steps and the video for training, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy. The key idea is to measure alignment between LLM-steps and videos via multiple pathways, including: (1) step-narration-video alignment using narration timestamps, (2) direct step-to-video alignment based on their long-term semantic similarity, and (3) direct step-to-video alignment focusing on short-term fine-grained semantic similarity learned from general video domains. The results from different pathways are fused to generate reliable pseudo step-video matching. We conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach surpasses state-of-the-art methods in three downstream tasks: procedure step grounding, step localization, and narration grounding by $5.9 \\%, 3.1 \\%$, and $2.8 \\%$.\n\n\n1 Introduction\n\nInstructional videos are a type of educational content presented in video format. They demonstrate the steps required to perform specific tasks, such as cooking, repairing cars, and applying makeup $[17,18,36,44]$. These videos are invaluable resources for individuals seeking to learn new skills or knowledge in various",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Our approach surpasses state-of-the-art methods in three downstream tasks: procedure step grounding, step localization, and narration grounding by $5.9 \\%, 3.1 \\%$, and $2.8 \\%$.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0021b632-3246-47d9-bb29-66398e4a295d",
        "questions": "Does the proposed Multi-Pathway Text-Video Alignment strategy include a pathway that focuses on short-term fine-grained semantic similarity learned from general video domains?",
        "answers": "Yes",
        "context": "Learning to Localize Actions in Instructional Videos with LLM-Based Multi-Pathway Text-Video Alignment\n\n\n\nYuxiao Chen ${ ^{1}$, Kai Li ${ }^{2}$, Wentao Bao ${ }^{4}$, Deep Patel ${ }^{3}$, Yu Kong ${ }^{4}$, Martin Renqiang Min ${ }^{3}$, and Dimitris N. Metaxas ${ }^{1}$ \\\\ ${ }^{1}$ Rutgers University \\{yc984, dnm\\}@cs.rutgers.edu, \\\\ ${ }^{2}$ Meta \\{li.gml.kai@gmail.com\\} \\\\ ${ }^{3}$ NEC Labs America-Princeton \\{dpatel, renqiang\\}@nec-labs.com \\\\ ${ }^{4}$ Michigan State University \\{baowenta, yukong\\}@msu.edu\n}\n\n\nLearning to localize temporal boundaries of procedure steps in instructional videos is challenging due to the limited availability of annotated large-scale training videos. Recent works focus on learning the cross-modal alignment between video segments and ASR-transcripted narration texts through contrastive learning. However, these methods fail to account for the alignment noise, i.e., irrelevant narrations to the instructional task in videos and unreliable timestamps in narrations. To address these challenges, this work proposes a novel training framework. Motivated by the strong capabilities of Large Language Models (LLMs) in procedure understanding and text summarization, we first apply an LLM to filter out task-irrelevant information and summarize task-related procedure steps (LLM-steps) from narrations. To further generate reliable pseudo-matching between the LLM-steps and the video for training, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy. The key idea is to measure alignment between LLM-steps and videos via multiple pathways, including: (1) step-narration-video alignment using narration timestamps, (2) direct step-to-video alignment based on their long-term semantic similarity, and (3) direct step-to-video alignment focusing on short-term fine-grained semantic similarity learned from general video domains. The results from different pathways are fused to generate reliable pseudo step-video matching. We conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach surpasses state-of-the-art methods in three downstream tasks: procedure step grounding, step localization, and narration grounding by $5.9 \\%, 3.1 \\%$, and $2.8 \\%$.\n\n\n1 Introduction\n\nInstructional videos are a type of educational content presented in video format. They demonstrate the steps required to perform specific tasks, such as cooking, repairing cars, and applying makeup $[17,18,36,44]$. These videos are invaluable resources for individuals seeking to learn new skills or knowledge in various",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "The key idea is to measure alignment between LLM-steps and videos via multiple pathways, including: (1) step-narration-video alignment using narration timestamps, (2) direct step-to-video alignment based on their long-term semantic similarity, and (3) direct step-to-video alignment focusing on short-term fine-grained semantic similarity learned from general video domains.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "002b2f92-8dc5-4bd9-a689-ef79f8c3c461",
        "questions": "What is the purpose of the pseudomatching matrix $\\mathbf{Y}^{S V}$ in the context of learning LLM-step video alignment?",
        "answers": "The pseudomatching matrix $\\mathbf{Y}^{S V}$ is used as the supervision to train $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$.",
        "context": "Learning LLM-step Video Alignment Given $\\mathbf{A}_{\\text {mult }}$, we construct the pseudomatching matrix $\\mathbf{Y}^{S V}$ as follows: For an LLM step $s_{i}$, we first identify the video segment $v_{k}$ that has the highest alignment score with $s_{i} . v_{k}$ is treated as the matched video segment for $s_{i}$, and thus we set $\\mathbf{Y}^{S V}(i, k)$ to one. Additionally, for any video segment $v_{j}$, if its temporal distance from $v_{k}$ (defined as $|j-k|$ ) falls within a predetermined window size $W$, it is also labeled as a matched segment to $s_{i}$, leading us to mark $\\mathbf{Y}^{S V}(i, j)$ as ones accordingly. To ensure the reliability of pseudo-labels, we exclude LLM steps for which the highest score in $\\mathbf{A}_{\\text {mult }}$ falls below a specified threshold $\\gamma$.\n\nWe use $\\mathbf{Y}^{S V}$ as the supervision to train $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$. The model $\\Phi$ takes $\\mathcal{S}$ and $\\mathcal{V}$ as inputs, and output a similarity matrix $\\hat{\\mathbf{A}}_{\\mathbf{S V}}$ between $\\mathcal{S}$ and $\\mathcal{V}$ :\n$$\\hat{\\mathbf{A}}_{\\mathbf{S V}}=\\Phi(\\mathcal{S}, \\mathcal{V})$$\nwhere $\\hat{\\mathbf{A}}_{\\mathbf{S V}} \\in \\mathbb{R}^{L \\times T}$. The model $\\Phi$ is trained to pull matched video segments and LLM-steps closely while pushing way unmatched ones, by minimizing the MIL-NCE loss $\\mathcal{L}\\left(\\mathbf{Y}^{S V}, \\hat{\\mathbf{A}}_{\\mathbf{S V}}\\right)$ shown in Equation 2.\n\n\n3.3 Architecture\n\n\nFigure 2 illustrates the architecture of $\\Phi$. It consists of the video backbone and text encoder, the unimodal Transformers encoder [48] for video and text, and a joint-modal Transformer. Given the input video $\\mathcal{V}$ and LLM-steps $\\mathcal{S}$, we first extract their embeddings by using the video backbone $f_{b}$ and text backbone $g_{b}$ :\n$$\\mathbf{F}_{V}=f_{b}(\\mathcal{V}), \\quad \\mathbf{F}_{S}=g_{b}(\\mathcal{S})$$\nwhere $\\mathbf{F}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\mathbf{F}_{S} \\in \\mathbb{R}^{L \\times D}$. The extracted video segments and text features from the backbone are subsequently fed into the model-specific unimodal Transformers, $f_{\\text {trs }}$ for video and $g_{\\text {trs }}$ for text, to refine the features by modeling the intra-modal relationships:\n$$\\overline{\\mathbf{F}}_{V}=f_{t r s}\\left(\\mathbf{F}_{V}+\\mathbf{F}_{\\mathrm{PE}}\\right), \\quad \\overline{\\mathbf{F}}_{S}=g_{t r s}\\left(\\mathbf{F}_{S}\\right)$$\nwhere $\\overline{\\mathbf{F}}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\overline{\\mathbf{F}}_{S} \\in \\mathbb{R}^{L \\times D}$. Note that we add positional embedding (PE), denoted as $\\mathbf{F}_{\\text {PE }}$, into the video segments to represent the time orders of video segments. We do not use PE with LLM-steps since the input procedure steps may not have sequential orders on downstream tasks $[18,34]$.\n\nFinally, the joint Transformer $h_{\\text {trs }}$ takes the sequential concatenation of $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ as inputs to further model the fine-grained cross-modal relationship between video segments and LLM-steps. It outputs the final feature representations of video segments $\\hat{\\mathbf{F}}_{V}$ and LLM-steps $\\hat{\\mathbf{F}}_{S}$.\n$$\\left[\\hat{\\mathbf{F}}_{V} ; \\hat{\\mathbf{F}}_{S}\\right]=h_{t r s}\\left(\\left[\\overline{\\mathbf{F}}_{V} ; \\overline{\\mathbf{F}}_{S}\\right]\\right)$$\n\nThe similarity matrix $\\hat{\\mathbf{A}}_{S V}$ between LLM-steps and video segments are obtained by calculating the cosine similarity between $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ :\n$$\\hat{\\mathbf{A}}_{S V}=\\hat{\\mathbf{F}}_{S} \\cdot \\hat{\\mathbf{F}}_{V}^{\\top}$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We use $\\mathbf{Y}^{S V}$ as the supervision to train $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "002f9cc4-096b-4aff-b5b7-751f497e28aa",
        "questions": "What is the role of the joint Transformer $h_{\text {trs }}$ in the architecture of $\\Phi$?",
        "answers": "The joint Transformer $h_{\text {trs }}$ takes the sequential concatenation of $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ as inputs to further model the fine-grained cross-modal relationship between video segments and LLM-steps.",
        "context": "Learning LLM-step Video Alignment Given $\\mathbf{A}_{\\text {mult }}$, we construct the pseudomatching matrix $\\mathbf{Y}^{S V}$ as follows: For an LLM step $s_{i}$, we first identify the video segment $v_{k}$ that has the highest alignment score with $s_{i} . v_{k}$ is treated as the matched video segment for $s_{i}$, and thus we set $\\mathbf{Y}^{S V}(i, k)$ to one. Additionally, for any video segment $v_{j}$, if its temporal distance from $v_{k}$ (defined as $|j-k|$ ) falls within a predetermined window size $W$, it is also labeled as a matched segment to $s_{i}$, leading us to mark $\\mathbf{Y}^{S V}(i, j)$ as ones accordingly. To ensure the reliability of pseudo-labels, we exclude LLM steps for which the highest score in $\\mathbf{A}_{\\text {mult }}$ falls below a specified threshold $\\gamma$.\n\nWe use $\\mathbf{Y}^{S V}$ as the supervision to train $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$. The model $\\Phi$ takes $\\mathcal{S}$ and $\\mathcal{V}$ as inputs, and output a similarity matrix $\\hat{\\mathbf{A}}_{\\mathbf{S V}}$ between $\\mathcal{S}$ and $\\mathcal{V}$ :\n$$\\hat{\\mathbf{A}}_{\\mathbf{S V}}=\\Phi(\\mathcal{S}, \\mathcal{V})$$\nwhere $\\hat{\\mathbf{A}}_{\\mathbf{S V}} \\in \\mathbb{R}^{L \\times T}$. The model $\\Phi$ is trained to pull matched video segments and LLM-steps closely while pushing way unmatched ones, by minimizing the MIL-NCE loss $\\mathcal{L}\\left(\\mathbf{Y}^{S V}, \\hat{\\mathbf{A}}_{\\mathbf{S V}}\\right)$ shown in Equation 2.\n\n\n3.3 Architecture\n\n\nFigure 2 illustrates the architecture of $\\Phi$. It consists of the video backbone and text encoder, the unimodal Transformers encoder [48] for video and text, and a joint-modal Transformer. Given the input video $\\mathcal{V}$ and LLM-steps $\\mathcal{S}$, we first extract their embeddings by using the video backbone $f_{b}$ and text backbone $g_{b}$ :\n$$\\mathbf{F}_{V}=f_{b}(\\mathcal{V}), \\quad \\mathbf{F}_{S}=g_{b}(\\mathcal{S})$$\nwhere $\\mathbf{F}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\mathbf{F}_{S} \\in \\mathbb{R}^{L \\times D}$. The extracted video segments and text features from the backbone are subsequently fed into the model-specific unimodal Transformers, $f_{\\text {trs }}$ for video and $g_{\\text {trs }}$ for text, to refine the features by modeling the intra-modal relationships:\n$$\\overline{\\mathbf{F}}_{V}=f_{t r s}\\left(\\mathbf{F}_{V}+\\mathbf{F}_{\\mathrm{PE}}\\right), \\quad \\overline{\\mathbf{F}}_{S}=g_{t r s}\\left(\\mathbf{F}_{S}\\right)$$\nwhere $\\overline{\\mathbf{F}}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\overline{\\mathbf{F}}_{S} \\in \\mathbb{R}^{L \\times D}$. Note that we add positional embedding (PE), denoted as $\\mathbf{F}_{\\text {PE }}$, into the video segments to represent the time orders of video segments. We do not use PE with LLM-steps since the input procedure steps may not have sequential orders on downstream tasks $[18,34]$.\n\nFinally, the joint Transformer $h_{\\text {trs }}$ takes the sequential concatenation of $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ as inputs to further model the fine-grained cross-modal relationship between video segments and LLM-steps. It outputs the final feature representations of video segments $\\hat{\\mathbf{F}}_{V}$ and LLM-steps $\\hat{\\mathbf{F}}_{S}$.\n$$\\left[\\hat{\\mathbf{F}}_{V} ; \\hat{\\mathbf{F}}_{S}\\right]=h_{t r s}\\left(\\left[\\overline{\\mathbf{F}}_{V} ; \\overline{\\mathbf{F}}_{S}\\right]\\right)$$\n\nThe similarity matrix $\\hat{\\mathbf{A}}_{S V}$ between LLM-steps and video segments are obtained by calculating the cosine similarity between $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ :\n$$\\hat{\\mathbf{A}}_{S V}=\\hat{\\mathbf{F}}_{S} \\cdot \\hat{\\mathbf{F}}_{V}^{\\top}$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Finally, the joint Transformer $h_{\text {trs }}$ takes the sequential concatenation of $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ as inputs to further model the fine-grained cross-modal relationship between video segments and LLM-steps.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0030f3fb-01f7-4027-955c-7e9f7a793d3e",
        "questions": "Does the architecture of $\\Phi$ use positional embedding (PE) with LLM-steps?",
        "answers": "No",
        "context": "Learning LLM-step Video Alignment Given $\\mathbf{A}_{\\text {mult }}$, we construct the pseudomatching matrix $\\mathbf{Y}^{S V}$ as follows: For an LLM step $s_{i}$, we first identify the video segment $v_{k}$ that has the highest alignment score with $s_{i} . v_{k}$ is treated as the matched video segment for $s_{i}$, and thus we set $\\mathbf{Y}^{S V}(i, k)$ to one. Additionally, for any video segment $v_{j}$, if its temporal distance from $v_{k}$ (defined as $|j-k|$ ) falls within a predetermined window size $W$, it is also labeled as a matched segment to $s_{i}$, leading us to mark $\\mathbf{Y}^{S V}(i, j)$ as ones accordingly. To ensure the reliability of pseudo-labels, we exclude LLM steps for which the highest score in $\\mathbf{A}_{\\text {mult }}$ falls below a specified threshold $\\gamma$.\n\nWe use $\\mathbf{Y}^{S V}$ as the supervision to train $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$. The model $\\Phi$ takes $\\mathcal{S}$ and $\\mathcal{V}$ as inputs, and output a similarity matrix $\\hat{\\mathbf{A}}_{\\mathbf{S V}}$ between $\\mathcal{S}$ and $\\mathcal{V}$ :\n$$\\hat{\\mathbf{A}}_{\\mathbf{S V}}=\\Phi(\\mathcal{S}, \\mathcal{V})$$\nwhere $\\hat{\\mathbf{A}}_{\\mathbf{S V}} \\in \\mathbb{R}^{L \\times T}$. The model $\\Phi$ is trained to pull matched video segments and LLM-steps closely while pushing way unmatched ones, by minimizing the MIL-NCE loss $\\mathcal{L}\\left(\\mathbf{Y}^{S V}, \\hat{\\mathbf{A}}_{\\mathbf{S V}}\\right)$ shown in Equation 2.\n\n\n3.3 Architecture\n\n\nFigure 2 illustrates the architecture of $\\Phi$. It consists of the video backbone and text encoder, the unimodal Transformers encoder [48] for video and text, and a joint-modal Transformer. Given the input video $\\mathcal{V}$ and LLM-steps $\\mathcal{S}$, we first extract their embeddings by using the video backbone $f_{b}$ and text backbone $g_{b}$ :\n$$\\mathbf{F}_{V}=f_{b}(\\mathcal{V}), \\quad \\mathbf{F}_{S}=g_{b}(\\mathcal{S})$$\nwhere $\\mathbf{F}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\mathbf{F}_{S} \\in \\mathbb{R}^{L \\times D}$. The extracted video segments and text features from the backbone are subsequently fed into the model-specific unimodal Transformers, $f_{\\text {trs }}$ for video and $g_{\\text {trs }}$ for text, to refine the features by modeling the intra-modal relationships:\n$$\\overline{\\mathbf{F}}_{V}=f_{t r s}\\left(\\mathbf{F}_{V}+\\mathbf{F}_{\\mathrm{PE}}\\right), \\quad \\overline{\\mathbf{F}}_{S}=g_{t r s}\\left(\\mathbf{F}_{S}\\right)$$\nwhere $\\overline{\\mathbf{F}}_{V} \\in \\mathbb{R}^{T \\times D}$ and $\\overline{\\mathbf{F}}_{S} \\in \\mathbb{R}^{L \\times D}$. Note that we add positional embedding (PE), denoted as $\\mathbf{F}_{\\text {PE }}$, into the video segments to represent the time orders of video segments. We do not use PE with LLM-steps since the input procedure steps may not have sequential orders on downstream tasks $[18,34]$.\n\nFinally, the joint Transformer $h_{\\text {trs }}$ takes the sequential concatenation of $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ as inputs to further model the fine-grained cross-modal relationship between video segments and LLM-steps. It outputs the final feature representations of video segments $\\hat{\\mathbf{F}}_{V}$ and LLM-steps $\\hat{\\mathbf{F}}_{S}$.\n$$\\left[\\hat{\\mathbf{F}}_{V} ; \\hat{\\mathbf{F}}_{S}\\right]=h_{t r s}\\left(\\left[\\overline{\\mathbf{F}}_{V} ; \\overline{\\mathbf{F}}_{S}\\right]\\right)$$\n\nThe similarity matrix $\\hat{\\mathbf{A}}_{S V}$ between LLM-steps and video segments are obtained by calculating the cosine similarity between $\\hat{\\mathbf{F}}_{V}$ and $\\hat{\\mathbf{F}}_{S}$ :\n$$\\hat{\\mathbf{A}}_{S V}=\\hat{\\mathbf{F}}_{S} \\cdot \\hat{\\mathbf{F}}_{V}^{\\top}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We do not use PE with LLM-steps since the input procedure steps may not have sequential orders on downstream tasks $[18,34]$.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "003c6ab8-2d19-4cf0-8d43-8259815f9e34",
        "questions": "What percentage improvement does the method exhibit over previous state-of-the-art methods for action step localization?",
        "answers": "3.1%",
        "context": "action step localization, and narration grounding. Notably, our method exhibits improvements over previous state-of-the-art methods by $5.9 \\%, 2.8 \\%$, and $3.1 \\%$ for procedure step grounding, narration grounding, and action step localization. We also draw the following key observations: (1) Models trained with our LLMsteps significantly outperform those trained with wikiHow-steps and a combination of wikiHow-steps and narrations by $10.7 \\%$ and $6 \\%$, demonstrating the superior effectiveness of our LLM-steps. (2) When employing the supervision generated by our proposed MPTVA, models trained with narrations outperform those trained with wikiHow-steps. This underscores the limitations of using wikiHow-steps for training (3) Our ablation study reveals that applying all three alignment pathways improves the performance by $4.2 \\%$ and $5.4 \\%$ compared to the best model utilizing a single pathway, and by $3 \\%$ and $4.5 \\%$ compared to the best model utilizing two pathways. This indicates that the complementary information contributed by each pathway results in more reliable pseudo-labels.\n\n\n2 Related Work\n\n\nStep Localization in Instructional Videos Previous studies on step localization in instructional videos can be classified into fully-supervised $[4,5,26,44]$ and weakly-supervised $[7,10,18,32]$ methods. Fully-supervised methods train models to directly predict the annotated boundaries of procedure steps. However, the main limitation of these approaches is the high cost to collect temporal annotations. On the other hand, weakly supervised methods only require the order or occurrence information of procedure steps for training, thus reducing the cost of annotation. However, collecting such annotations remains costly, as it requires annotators to watch entire videos. Recent work $[16,21,34,41]$ attempts to eliminate labeling efforts by using narrated instructional videos. Their main focus is on how to leverage the noisy supervision signals provided by narrations. For example, some works $[16,41]$ proposed learning an optimal sequence-to-sequence alignment between narrations and video segments by utilizing DTW [15, 40]. However, these methods are not suited for the downstream tasks, such as step grounding, where the order of steps is unknown. Our method is more related to VINA [34] and TAN [21]. TAN proposed filtering out noisy alignments through mutual agreement between two models with distinct architectures. VINA extracted task-specific procedure steps from the WikiHow knowledge base to obtain auxiliary clean text descriptions. It aslo explored generating pseudo-alignments between procedure steps and videos using pre-trained long-term video-text alignment models. In contrast, our method leverages LLMs to extract video-specific task procedure steps, which align more closely with video contents than taskspecific procedures used by VINA. Furthermore, our proposed MPTVA leverages a broader range of information which is extracted from various alignment paths and models pre-trained on diverse datasets, to generate pseudo-labels.\nLearning from Noisily-Aligned Visual-Text Data. Recent studies have achieved remarkable progress in extracting knowledge from large-scale, weakly aligned visual-text datasets. In the field of images $[11,19,22,38]$, it has been",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Notably, our method exhibits improvements over previous state-of-the-art methods by $5.9 \\%, 2.8 \\%$, and $3.1 \\%$ for procedure step grounding, narration grounding, and action step localization.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0042d740-0c34-439f-ad44-e0f06a9e72f8",
        "questions": "How much more effective are models trained with LLM-steps compared to those trained with a combination of wikiHow-steps and narrations?",
        "answers": "6%",
        "context": "action step localization, and narration grounding. Notably, our method exhibits improvements over previous state-of-the-art methods by $5.9 \\%, 2.8 \\%$, and $3.1 \\%$ for procedure step grounding, narration grounding, and action step localization. We also draw the following key observations: (1) Models trained with our LLMsteps significantly outperform those trained with wikiHow-steps and a combination of wikiHow-steps and narrations by $10.7 \\%$ and $6 \\%$, demonstrating the superior effectiveness of our LLM-steps. (2) When employing the supervision generated by our proposed MPTVA, models trained with narrations outperform those trained with wikiHow-steps. This underscores the limitations of using wikiHow-steps for training (3) Our ablation study reveals that applying all three alignment pathways improves the performance by $4.2 \\%$ and $5.4 \\%$ compared to the best model utilizing a single pathway, and by $3 \\%$ and $4.5 \\%$ compared to the best model utilizing two pathways. This indicates that the complementary information contributed by each pathway results in more reliable pseudo-labels.\n\n\n2 Related Work\n\n\nStep Localization in Instructional Videos Previous studies on step localization in instructional videos can be classified into fully-supervised $[4,5,26,44]$ and weakly-supervised $[7,10,18,32]$ methods. Fully-supervised methods train models to directly predict the annotated boundaries of procedure steps. However, the main limitation of these approaches is the high cost to collect temporal annotations. On the other hand, weakly supervised methods only require the order or occurrence information of procedure steps for training, thus reducing the cost of annotation. However, collecting such annotations remains costly, as it requires annotators to watch entire videos. Recent work $[16,21,34,41]$ attempts to eliminate labeling efforts by using narrated instructional videos. Their main focus is on how to leverage the noisy supervision signals provided by narrations. For example, some works $[16,41]$ proposed learning an optimal sequence-to-sequence alignment between narrations and video segments by utilizing DTW [15, 40]. However, these methods are not suited for the downstream tasks, such as step grounding, where the order of steps is unknown. Our method is more related to VINA [34] and TAN [21]. TAN proposed filtering out noisy alignments through mutual agreement between two models with distinct architectures. VINA extracted task-specific procedure steps from the WikiHow knowledge base to obtain auxiliary clean text descriptions. It aslo explored generating pseudo-alignments between procedure steps and videos using pre-trained long-term video-text alignment models. In contrast, our method leverages LLMs to extract video-specific task procedure steps, which align more closely with video contents than taskspecific procedures used by VINA. Furthermore, our proposed MPTVA leverages a broader range of information which is extracted from various alignment paths and models pre-trained on diverse datasets, to generate pseudo-labels.\nLearning from Noisily-Aligned Visual-Text Data. Recent studies have achieved remarkable progress in extracting knowledge from large-scale, weakly aligned visual-text datasets. In the field of images $[11,19,22,38]$, it has been",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Models trained with our LLMsteps significantly outperform those trained with wikiHow-steps and a combination of wikiHow-steps and narrations by $10.7 \\%$ and $6 \\%$, demonstrating the superior effectiveness of our LLM-steps.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "004cdaf0-0ed9-4a32-8f0f-a9db4b6a3fea",
        "questions": "What is the improvement percentage when applying all three alignment pathways compared to the best model utilizing two pathways?",
        "answers": "3% and 4.5%",
        "context": "action step localization, and narration grounding. Notably, our method exhibits improvements over previous state-of-the-art methods by $5.9 \\%, 2.8 \\%$, and $3.1 \\%$ for procedure step grounding, narration grounding, and action step localization. We also draw the following key observations: (1) Models trained with our LLMsteps significantly outperform those trained with wikiHow-steps and a combination of wikiHow-steps and narrations by $10.7 \\%$ and $6 \\%$, demonstrating the superior effectiveness of our LLM-steps. (2) When employing the supervision generated by our proposed MPTVA, models trained with narrations outperform those trained with wikiHow-steps. This underscores the limitations of using wikiHow-steps for training (3) Our ablation study reveals that applying all three alignment pathways improves the performance by $4.2 \\%$ and $5.4 \\%$ compared to the best model utilizing a single pathway, and by $3 \\%$ and $4.5 \\%$ compared to the best model utilizing two pathways. This indicates that the complementary information contributed by each pathway results in more reliable pseudo-labels.\n\n\n2 Related Work\n\n\nStep Localization in Instructional Videos Previous studies on step localization in instructional videos can be classified into fully-supervised $[4,5,26,44]$ and weakly-supervised $[7,10,18,32]$ methods. Fully-supervised methods train models to directly predict the annotated boundaries of procedure steps. However, the main limitation of these approaches is the high cost to collect temporal annotations. On the other hand, weakly supervised methods only require the order or occurrence information of procedure steps for training, thus reducing the cost of annotation. However, collecting such annotations remains costly, as it requires annotators to watch entire videos. Recent work $[16,21,34,41]$ attempts to eliminate labeling efforts by using narrated instructional videos. Their main focus is on how to leverage the noisy supervision signals provided by narrations. For example, some works $[16,41]$ proposed learning an optimal sequence-to-sequence alignment between narrations and video segments by utilizing DTW [15, 40]. However, these methods are not suited for the downstream tasks, such as step grounding, where the order of steps is unknown. Our method is more related to VINA [34] and TAN [21]. TAN proposed filtering out noisy alignments through mutual agreement between two models with distinct architectures. VINA extracted task-specific procedure steps from the WikiHow knowledge base to obtain auxiliary clean text descriptions. It aslo explored generating pseudo-alignments between procedure steps and videos using pre-trained long-term video-text alignment models. In contrast, our method leverages LLMs to extract video-specific task procedure steps, which align more closely with video contents than taskspecific procedures used by VINA. Furthermore, our proposed MPTVA leverages a broader range of information which is extracted from various alignment paths and models pre-trained on diverse datasets, to generate pseudo-labels.\nLearning from Noisily-Aligned Visual-Text Data. Recent studies have achieved remarkable progress in extracting knowledge from large-scale, weakly aligned visual-text datasets. In the field of images $[11,19,22,38]$, it has been",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Our ablation study reveals that applying all three alignment pathways improves the performance by $4.2 \\%$ and $5.4 \\%$ compared to the best model utilizing a single pathway, and by $3 \\%$ and $4.5 \\%$ compared to the best model utilizing two pathways.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00513063-020a-4a12-855d-fed45fa7cb00",
        "questions": "What is the R@1 performance on the HT-Step dataset when using the N-MPTVA training data?",
        "answers": "32.1",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "N-MPTVA & 32.1 & 38.0",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "005476c7-d8f9-49cc-bed6-1fd50aeaebe2",
        "questions": "Which training data combination achieved the highest R@1 performance on the CrossTask dataset?",
        "answers": "N-MPTVA + S-MPTVA",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "N-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00650544-7a10-4917-ab53-0a1536681492",
        "questions": "Does the model trained with LLM-steps outperform the combination of narration and wikiHow articles for step-grounding and action step localization tasks?",
        "answers": "Yes",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "006baa01-fdbc-46e7-8734-baefc2e4866f",
        "questions": "What is the R@1 score achieved by the method 'Ours' on the HTM-Align dataset?",
        "answers": "69.3",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Ours & \\mathbf{6 9 . 3} & \\mathbf{4 3 . 3}",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "007b0a78-f278-4163-9312-8e5cbea3351d",
        "questions": "Which method achieved the highest Avg. R@1 score on the CrossTask Dataset according to Table 5?",
        "answers": "Ours",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Ours & \\mathbf{4 7 . 9}",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0097c49a-5570-4dc0-8805-fb35cba9748d",
        "questions": "What is the effect on performance when the filtering threshold gamma is increased from 0.65 in the pseudo-label generation process?",
        "answers": "Performance significantly drops since a large percent of data (97%) is excluded.",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In addition, increasing \\gamma from 0.65 significantly drops the performances, since a large percent of data (97 \\%) is excluded.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00a1402e-57e7-4f7b-8486-10cfd21276c2",
        "questions": "What is the purpose of using a Large Language Model (LLM) in the proposed framework for training procedure steps localization models on narrated instructional videos?",
        "answers": "To filter out task-irrelevant information and summarize the procedure steps from narrations.",
        "context": "knowledge base. For example, as shown in Figure 1, the wikiHow-step \"cutting tomato\" is absent in the videos, while some steps performed in the videos, such as \"take a drizzle of olive oil\", are not included in the knowledge bases. Additionally, previous stuies $[21,34]$ employs self-training technologies to tackle the issue of temporal misalignment. They first train models with supervision signals derived from narration timestamps, and then use the trained models to filter out unmatched narrations and establish new pseudo-alignment between narrations and videos. However, the effectiveness of these methods may be constrained, since the models employed to filter noises are trained on noisy narrations.\n\nTo address the above challenges, we propose a novel framework for training procedure steps localization models on narrated instructional videos. Figure 2 gives an overview of our proposed methods. Initially, we utilize a Large Language Model (LLM) $[3,39,45,46]$ to filter out task-irrelevant information and summarize the procedure steps from narrations. LLMs have demonstrated strong capabilities in understanding procedures $[1,14,54]$, thus serving as a knowledge base for instructional tasks. Additionally, LLMs possess robust text summarization capabilities $[37,47]$. Therefore, by using LLM, we can extract video-specific taskrelated procedure steps, denoted as LLM-steps. Compared with wikiHow-steps, which is designed to provide a general introduction to a task, our video-specific LLM-steps can align more closely with the video contents, as shown in Figure 1.\n\nFurthermore, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-matching between videos and LLM-steps. Our key insight is to measure the alignment between LLM-steps and video segments using different pathways, each of which captures their relationships from different aspects and provides complementary information. Consequently, combining those information can effectively filter out noise. Specifically, we leverage three distinct alignment pathways. The first pathway is designed to leverage the temporal correspondence derived from the timestamps of narrations. It first identifies narrations semantically similar to the LLM-steps and subsequently matches each LLM-step with video segments that fall within the timestamps of their semantically similar narrations. To mitigate the impact of timestamp noise, we further directly align LLM-steps with videos using a video-text alignment model pre-trained on long-duration instructional video data [34]. This pathway measures the alignment based on long-term global text-video semantic similarity. Additionally, we directly align LLM-steps with video segments utilizing a videotext foundation model [49] pre-trained on short-duration video-text datasets from various domains. This pathway not only captures fine-grained, short-term relationships but also incorporates broader knowledge learned from various video domains. We combine the alignment scores from the three pathways using meanpooling, and then generate pseudo-labels from the obtained alignment score. It effectively leverages the mutual agreement among different pathways, which has been demonstrated to be effective in filtering noise for pseudo-labeling [21].\n\nWe conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach significantly outperforms state-of-the-art methods in three downstream tasks: procedure step grounding,",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Initially, we utilize a Large Language Model (LLM) $[3,39,45,46]$ to filter out task-irrelevant information and summarize the procedure steps from narrations.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00a594c2-0d4e-4d06-8c16-4ccabbfcc661",
        "questions": "How does the Multi-Pathway Text-Video Alignment (MPTVA) strategy enhance the alignment between LLM-steps and video segments?",
        "answers": "By using different pathways, each capturing relationships from different aspects and providing complementary information.",
        "context": "knowledge base. For example, as shown in Figure 1, the wikiHow-step \"cutting tomato\" is absent in the videos, while some steps performed in the videos, such as \"take a drizzle of olive oil\", are not included in the knowledge bases. Additionally, previous stuies $[21,34]$ employs self-training technologies to tackle the issue of temporal misalignment. They first train models with supervision signals derived from narration timestamps, and then use the trained models to filter out unmatched narrations and establish new pseudo-alignment between narrations and videos. However, the effectiveness of these methods may be constrained, since the models employed to filter noises are trained on noisy narrations.\n\nTo address the above challenges, we propose a novel framework for training procedure steps localization models on narrated instructional videos. Figure 2 gives an overview of our proposed methods. Initially, we utilize a Large Language Model (LLM) $[3,39,45,46]$ to filter out task-irrelevant information and summarize the procedure steps from narrations. LLMs have demonstrated strong capabilities in understanding procedures $[1,14,54]$, thus serving as a knowledge base for instructional tasks. Additionally, LLMs possess robust text summarization capabilities $[37,47]$. Therefore, by using LLM, we can extract video-specific taskrelated procedure steps, denoted as LLM-steps. Compared with wikiHow-steps, which is designed to provide a general introduction to a task, our video-specific LLM-steps can align more closely with the video contents, as shown in Figure 1.\n\nFurthermore, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-matching between videos and LLM-steps. Our key insight is to measure the alignment between LLM-steps and video segments using different pathways, each of which captures their relationships from different aspects and provides complementary information. Consequently, combining those information can effectively filter out noise. Specifically, we leverage three distinct alignment pathways. The first pathway is designed to leverage the temporal correspondence derived from the timestamps of narrations. It first identifies narrations semantically similar to the LLM-steps and subsequently matches each LLM-step with video segments that fall within the timestamps of their semantically similar narrations. To mitigate the impact of timestamp noise, we further directly align LLM-steps with videos using a video-text alignment model pre-trained on long-duration instructional video data [34]. This pathway measures the alignment based on long-term global text-video semantic similarity. Additionally, we directly align LLM-steps with video segments utilizing a videotext foundation model [49] pre-trained on short-duration video-text datasets from various domains. This pathway not only captures fine-grained, short-term relationships but also incorporates broader knowledge learned from various video domains. We combine the alignment scores from the three pathways using meanpooling, and then generate pseudo-labels from the obtained alignment score. It effectively leverages the mutual agreement among different pathways, which has been demonstrated to be effective in filtering noise for pseudo-labeling [21].\n\nWe conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach significantly outperforms state-of-the-art methods in three downstream tasks: procedure step grounding,",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Our key insight is to measure the alignment between LLM-steps and video segments using different pathways, each of which captures their relationships from different aspects and provides complementary information.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00a89d2b-e1f4-4a00-a5ca-92bd745dc319",
        "questions": "Does the proposed method outperform state-of-the-art methods in downstream tasks such as procedure step grounding?",
        "answers": "Yes",
        "context": "knowledge base. For example, as shown in Figure 1, the wikiHow-step \"cutting tomato\" is absent in the videos, while some steps performed in the videos, such as \"take a drizzle of olive oil\", are not included in the knowledge bases. Additionally, previous stuies $[21,34]$ employs self-training technologies to tackle the issue of temporal misalignment. They first train models with supervision signals derived from narration timestamps, and then use the trained models to filter out unmatched narrations and establish new pseudo-alignment between narrations and videos. However, the effectiveness of these methods may be constrained, since the models employed to filter noises are trained on noisy narrations.\n\nTo address the above challenges, we propose a novel framework for training procedure steps localization models on narrated instructional videos. Figure 2 gives an overview of our proposed methods. Initially, we utilize a Large Language Model (LLM) $[3,39,45,46]$ to filter out task-irrelevant information and summarize the procedure steps from narrations. LLMs have demonstrated strong capabilities in understanding procedures $[1,14,54]$, thus serving as a knowledge base for instructional tasks. Additionally, LLMs possess robust text summarization capabilities $[37,47]$. Therefore, by using LLM, we can extract video-specific taskrelated procedure steps, denoted as LLM-steps. Compared with wikiHow-steps, which is designed to provide a general introduction to a task, our video-specific LLM-steps can align more closely with the video contents, as shown in Figure 1.\n\nFurthermore, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-matching between videos and LLM-steps. Our key insight is to measure the alignment between LLM-steps and video segments using different pathways, each of which captures their relationships from different aspects and provides complementary information. Consequently, combining those information can effectively filter out noise. Specifically, we leverage three distinct alignment pathways. The first pathway is designed to leverage the temporal correspondence derived from the timestamps of narrations. It first identifies narrations semantically similar to the LLM-steps and subsequently matches each LLM-step with video segments that fall within the timestamps of their semantically similar narrations. To mitigate the impact of timestamp noise, we further directly align LLM-steps with videos using a video-text alignment model pre-trained on long-duration instructional video data [34]. This pathway measures the alignment based on long-term global text-video semantic similarity. Additionally, we directly align LLM-steps with video segments utilizing a videotext foundation model [49] pre-trained on short-duration video-text datasets from various domains. This pathway not only captures fine-grained, short-term relationships but also incorporates broader knowledge learned from various video domains. We combine the alignment scores from the three pathways using meanpooling, and then generate pseudo-labels from the obtained alignment score. It effectively leverages the mutual agreement among different pathways, which has been demonstrated to be effective in filtering noise for pseudo-labeling [21].\n\nWe conducted extensive experiments across various tasks and problem settings to evaluate our proposed method. Our approach significantly outperforms state-of-the-art methods in three downstream tasks: procedure step grounding,",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Our approach significantly outperforms state-of-the-art methods in three downstream tasks: procedure step grounding.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00ac317a-dd82-4b3e-a868-5bdd02c81d11",
        "questions": "What is the purpose of the model Phi in the proposed method for video and narration alignment?",
        "answers": "The model Phi is trained to maximize the similarity between matched video segments and narrations, while decrease those between unmatched ones.",
        "context": "Fig. 2: Overview of our proposed method. (a): We first use an LLM to summarize taskrelevant LLM-steps from narrations. (b): We then extract the pseudo-matching between LLM-steps and video segments using our proposed MPTVA. (c): The extracted pseudoalignments are used as supervision to train the model to minimize the MIL-NCE loss. (d): The illustration of the proposed MPTVA strategy for pseudo-label generation.\n$\\mathcal{V}$ and $\\mathcal{N}$ :\n$$\\hat{\\mathbf{A}}=\\phi(\\mathcal{N}, \\mathcal{V}) \\in \\mathbb{R}^{K \\times T}$$\n\nThe model $\\Phi$ is trained to maximize the similarity between matched video segments and narrations, while decrease those between unmatched ones. This is achieved by minimizing the MIL-NCE loss [35]:\n$$\\mathcal{L}\\left(\\mathbf{Y}^{N V}, \\hat{\\mathbf{A}}\\right)=-\\frac{1}{K} \\sum_{k=1}^{K} \\log \\left(\\frac{\\sum_{t=1}^{T} Y_{k, t}^{N V} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}{\\sum_{t=1}^{T} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}\\right)$$\nwhere $\\mathbf{Y}^{N V}$ is the pseudo matching matrix between $\\mathcal{N}$ and $\\mathcal{V}$ constructed by the timestamps of narrations. $Y_{k, t}^{N V}$ is the element at the $k$-th row and $t$-th column. It is equal to one if the $v_{t}$ is within the timestamps of $n_{k}$, and zero otherwise. $\\eta$ is the softmax temperature parameter.\n\nHowever, the solution is sub-optimal, since $n_{i}$ might not be relevant to any visual content of $V$, and the matching relationship encoded in $\\mathbf{Y}^{N V}$ is unreliable, due to the temporal misalignment problem of timestamps.\n\n\n3.2 Proposed Framework\n\n\nFigure 2 gives an overview of our proposed methods. We first apply an LLM to process narrations $\\mathcal{N}$, filtering out task-unrelated information and summarizing",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The model $\\Phi$ is trained to maximize the similarity between matched video segments and narrations, while decrease those between unmatched ones.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00afecfa-6ea0-41cc-9ec7-d2715034e9ff",
        "questions": "What is the role of the pseudo matching matrix Y^NV in the MIL-NCE loss function?",
        "answers": "The pseudo matching matrix Y^NV is constructed by the timestamps of narrations and is used to determine if a video segment is within the timestamps of a narration.",
        "context": "Fig. 2: Overview of our proposed method. (a): We first use an LLM to summarize taskrelevant LLM-steps from narrations. (b): We then extract the pseudo-matching between LLM-steps and video segments using our proposed MPTVA. (c): The extracted pseudoalignments are used as supervision to train the model to minimize the MIL-NCE loss. (d): The illustration of the proposed MPTVA strategy for pseudo-label generation.\n$\\mathcal{V}$ and $\\mathcal{N}$ :\n$$\\hat{\\mathbf{A}}=\\phi(\\mathcal{N}, \\mathcal{V}) \\in \\mathbb{R}^{K \\times T}$$\n\nThe model $\\Phi$ is trained to maximize the similarity between matched video segments and narrations, while decrease those between unmatched ones. This is achieved by minimizing the MIL-NCE loss [35]:\n$$\\mathcal{L}\\left(\\mathbf{Y}^{N V}, \\hat{\\mathbf{A}}\\right)=-\\frac{1}{K} \\sum_{k=1}^{K} \\log \\left(\\frac{\\sum_{t=1}^{T} Y_{k, t}^{N V} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}{\\sum_{t=1}^{T} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}\\right)$$\nwhere $\\mathbf{Y}^{N V}$ is the pseudo matching matrix between $\\mathcal{N}$ and $\\mathcal{V}$ constructed by the timestamps of narrations. $Y_{k, t}^{N V}$ is the element at the $k$-th row and $t$-th column. It is equal to one if the $v_{t}$ is within the timestamps of $n_{k}$, and zero otherwise. $\\eta$ is the softmax temperature parameter.\n\nHowever, the solution is sub-optimal, since $n_{i}$ might not be relevant to any visual content of $V$, and the matching relationship encoded in $\\mathbf{Y}^{N V}$ is unreliable, due to the temporal misalignment problem of timestamps.\n\n\n3.2 Proposed Framework\n\n\nFigure 2 gives an overview of our proposed methods. We first apply an LLM to process narrations $\\mathcal{N}$, filtering out task-unrelated information and summarizing",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$\\mathbf{Y}^{N V}$ is the pseudo matching matrix between $\\mathcal{N}$ and $\\mathcal{V}$ constructed by the timestamps of narrations. $Y_{k, t}^{N V}$ is the element at the $k$-th row and $t$-th column. It is equal to one if the $v_{t}$ is within the timestamps of $n_{k}$, and zero otherwise.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00b425aa-e7eb-433c-ba08-79fbdc7843b3",
        "questions": "Is the matching relationship encoded in the pseudo matching matrix Y^NV considered reliable in the proposed method?",
        "answers": "No",
        "context": "Fig. 2: Overview of our proposed method. (a): We first use an LLM to summarize taskrelevant LLM-steps from narrations. (b): We then extract the pseudo-matching between LLM-steps and video segments using our proposed MPTVA. (c): The extracted pseudoalignments are used as supervision to train the model to minimize the MIL-NCE loss. (d): The illustration of the proposed MPTVA strategy for pseudo-label generation.\n$\\mathcal{V}$ and $\\mathcal{N}$ :\n$$\\hat{\\mathbf{A}}=\\phi(\\mathcal{N}, \\mathcal{V}) \\in \\mathbb{R}^{K \\times T}$$\n\nThe model $\\Phi$ is trained to maximize the similarity between matched video segments and narrations, while decrease those between unmatched ones. This is achieved by minimizing the MIL-NCE loss [35]:\n$$\\mathcal{L}\\left(\\mathbf{Y}^{N V}, \\hat{\\mathbf{A}}\\right)=-\\frac{1}{K} \\sum_{k=1}^{K} \\log \\left(\\frac{\\sum_{t=1}^{T} Y_{k, t}^{N V} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}{\\sum_{t=1}^{T} \\exp \\left(\\hat{A}_{k, t} / \\eta\\right)}\\right)$$\nwhere $\\mathbf{Y}^{N V}$ is the pseudo matching matrix between $\\mathcal{N}$ and $\\mathcal{V}$ constructed by the timestamps of narrations. $Y_{k, t}^{N V}$ is the element at the $k$-th row and $t$-th column. It is equal to one if the $v_{t}$ is within the timestamps of $n_{k}$, and zero otherwise. $\\eta$ is the softmax temperature parameter.\n\nHowever, the solution is sub-optimal, since $n_{i}$ might not be relevant to any visual content of $V$, and the matching relationship encoded in $\\mathbf{Y}^{N V}$ is unreliable, due to the temporal misalignment problem of timestamps.\n\n\n3.2 Proposed Framework\n\n\nFigure 2 gives an overview of our proposed methods. We first apply an LLM to process narrations $\\mathcal{N}$, filtering out task-unrelated information and summarizing",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "However, the solution is sub-optimal, since $n_{i}$ might not be relevant to any visual content of $V$, and the matching relationship encoded in $\\mathbf{Y}^{N V}$ is unreliable, due to the temporal misalignment problem of timestamps.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00b5888d-5f7c-441a-abf5-5e065ae88bd8",
        "questions": "What is the purpose of using LLMs in the process of extracting task-related procedural steps from narrations?",
        "answers": "To filter video-irrelevant information and extract task-related procedural steps from narrations.",
        "context": "task-relevant procedure steps $\\mathcal{S}$. Subsequently, we extract the pseudo matching matrix $\\mathbf{Y}^{S V}$ between $\\mathcal{S}$ and $\\mathcal{V}$ by exploring multiple alignment pathways. The pseudo-alignments $\\mathbf{Y}^{S V}$ are used as supervision to train the model $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$.\n\nExtracting Procedure Steps Using LLM. Recent studies demonstrate that LLMs possess a strong capability for text summarization $[37,47]$ and understanding task procedures $[1,14,54]$. Motivated by this, we propose using LLMs to filter video-irrelevant information and extract task-related procedural steps from narrations. To this end, we design the following prompt $\\mathbf{P}$, which instructs the LLM model to (1) summarize the procedure steps relevant to the task shown in the video and (2) filter out irrelevant information, such as colloquial sentences:\n\"I will give you the text narrations extracted from an instruction video. Your task is to summarize the procedure steps that are relevant to the task of the video from the inputs. Please filter colloquial sentences in the speech.\"\n\nThis prompt, along with the narrations $\\mathcal{N}$, is concatenated and then fed into an LLM model, resulting in the LLM-generated procedural steps $\\mathcal{S}$ (denoted LLM-steps). The process is formulated as follows:\n$$\\mathcal{S}=L L M([\\mathbf{P} ; \\mathcal{N}])$$\n\nWe denote $\\mathcal{S}=\\left\\{s_{1}, s_{2} \\ldots, s_{L}\\right\\}$ where $s_{i}$ is the $i$-th out of $L$ LLM-steps.\nFigure 1 shows examples of obtained LLM-steps. We can see that (1) $\\mathcal{S}$ are more descriptive than $\\mathcal{N}$, and (2) most of the information unrelated to the video task contained in $\\mathcal{N}$, such as \"You might hear a lawnmower\", have been filtered out in $\\mathcal{S}$. In addition, we observe $\\mathcal{S}$ has retained most of the procedure information related to the task, and can align more contents of the videos than the wikiHow-steps. which are widely in the previous work [28,34,55]. Please refer to the supplementary material for more examples.\n\nMulti-Pathway Text-Video Alignment The obtained LLM-steps are not accompanied by annotations indicating temporal alignment with videos. To tackle this challenge, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-alignment between LLM-steps and video segments for training. The key idea is to measure the alignment between $\\mathcal{S}$ and $\\mathcal{V}$ in different pathways, each of which provides complementary information to others. The results from different pathways are then fused to extract the pseudomatching relationship.\n\nSpecifically, we measure their alignment using three different pathways. These include: (1) aligning $\\mathcal{S}$ with narrations $\\mathcal{N}$ and then to $\\mathcal{V}$ (S-N-V), (2) directly aligning $S$ to $\\mathcal{V}$ using the long-term video-text alignment models pre-trained in the instructional video domain (S-V-long), and (3) direct alignment using a video-text foundation model pre-trained on short-term video-text datasets from various domains (S-V-short). The three pathways capture the relationship between $\\mathcal{S}$ and $\\mathcal{V}$ based on different information, including the timestamps",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Motivated by this, we propose using LLMs to filter video-irrelevant information and extract task-related procedural steps from narrations.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00c08143-c5a4-48c6-995f-908dbd9695c9",
        "questions": "How many different pathways are used in the Multi-Pathway Text-Video Alignment strategy to measure the alignment between LLM-steps and video segments?",
        "answers": "Three",
        "context": "task-relevant procedure steps $\\mathcal{S}$. Subsequently, we extract the pseudo matching matrix $\\mathbf{Y}^{S V}$ between $\\mathcal{S}$ and $\\mathcal{V}$ by exploring multiple alignment pathways. The pseudo-alignments $\\mathbf{Y}^{S V}$ are used as supervision to train the model $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$.\n\nExtracting Procedure Steps Using LLM. Recent studies demonstrate that LLMs possess a strong capability for text summarization $[37,47]$ and understanding task procedures $[1,14,54]$. Motivated by this, we propose using LLMs to filter video-irrelevant information and extract task-related procedural steps from narrations. To this end, we design the following prompt $\\mathbf{P}$, which instructs the LLM model to (1) summarize the procedure steps relevant to the task shown in the video and (2) filter out irrelevant information, such as colloquial sentences:\n\"I will give you the text narrations extracted from an instruction video. Your task is to summarize the procedure steps that are relevant to the task of the video from the inputs. Please filter colloquial sentences in the speech.\"\n\nThis prompt, along with the narrations $\\mathcal{N}$, is concatenated and then fed into an LLM model, resulting in the LLM-generated procedural steps $\\mathcal{S}$ (denoted LLM-steps). The process is formulated as follows:\n$$\\mathcal{S}=L L M([\\mathbf{P} ; \\mathcal{N}])$$\n\nWe denote $\\mathcal{S}=\\left\\{s_{1}, s_{2} \\ldots, s_{L}\\right\\}$ where $s_{i}$ is the $i$-th out of $L$ LLM-steps.\nFigure 1 shows examples of obtained LLM-steps. We can see that (1) $\\mathcal{S}$ are more descriptive than $\\mathcal{N}$, and (2) most of the information unrelated to the video task contained in $\\mathcal{N}$, such as \"You might hear a lawnmower\", have been filtered out in $\\mathcal{S}$. In addition, we observe $\\mathcal{S}$ has retained most of the procedure information related to the task, and can align more contents of the videos than the wikiHow-steps. which are widely in the previous work [28,34,55]. Please refer to the supplementary material for more examples.\n\nMulti-Pathway Text-Video Alignment The obtained LLM-steps are not accompanied by annotations indicating temporal alignment with videos. To tackle this challenge, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-alignment between LLM-steps and video segments for training. The key idea is to measure the alignment between $\\mathcal{S}$ and $\\mathcal{V}$ in different pathways, each of which provides complementary information to others. The results from different pathways are then fused to extract the pseudomatching relationship.\n\nSpecifically, we measure their alignment using three different pathways. These include: (1) aligning $\\mathcal{S}$ with narrations $\\mathcal{N}$ and then to $\\mathcal{V}$ (S-N-V), (2) directly aligning $S$ to $\\mathcal{V}$ using the long-term video-text alignment models pre-trained in the instructional video domain (S-V-long), and (3) direct alignment using a video-text foundation model pre-trained on short-term video-text datasets from various domains (S-V-short). The three pathways capture the relationship between $\\mathcal{S}$ and $\\mathcal{V}$ based on different information, including the timestamps",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Specifically, we measure their alignment using three different pathways.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00c99f27-6e4d-442f-b306-f416b09b8163",
        "questions": "Does the Multi-Pathway Text-Video Alignment strategy include a pathway that aligns LLM-steps directly to video segments using a video-text foundation model pre-trained on short-term video-text datasets?",
        "answers": "Yes",
        "context": "task-relevant procedure steps $\\mathcal{S}$. Subsequently, we extract the pseudo matching matrix $\\mathbf{Y}^{S V}$ between $\\mathcal{S}$ and $\\mathcal{V}$ by exploring multiple alignment pathways. The pseudo-alignments $\\mathbf{Y}^{S V}$ are used as supervision to train the model $\\Phi$ to learn the alignment between $\\mathcal{S}$ and $\\mathcal{V}$.\n\nExtracting Procedure Steps Using LLM. Recent studies demonstrate that LLMs possess a strong capability for text summarization $[37,47]$ and understanding task procedures $[1,14,54]$. Motivated by this, we propose using LLMs to filter video-irrelevant information and extract task-related procedural steps from narrations. To this end, we design the following prompt $\\mathbf{P}$, which instructs the LLM model to (1) summarize the procedure steps relevant to the task shown in the video and (2) filter out irrelevant information, such as colloquial sentences:\n\"I will give you the text narrations extracted from an instruction video. Your task is to summarize the procedure steps that are relevant to the task of the video from the inputs. Please filter colloquial sentences in the speech.\"\n\nThis prompt, along with the narrations $\\mathcal{N}$, is concatenated and then fed into an LLM model, resulting in the LLM-generated procedural steps $\\mathcal{S}$ (denoted LLM-steps). The process is formulated as follows:\n$$\\mathcal{S}=L L M([\\mathbf{P} ; \\mathcal{N}])$$\n\nWe denote $\\mathcal{S}=\\left\\{s_{1}, s_{2} \\ldots, s_{L}\\right\\}$ where $s_{i}$ is the $i$-th out of $L$ LLM-steps.\nFigure 1 shows examples of obtained LLM-steps. We can see that (1) $\\mathcal{S}$ are more descriptive than $\\mathcal{N}$, and (2) most of the information unrelated to the video task contained in $\\mathcal{N}$, such as \"You might hear a lawnmower\", have been filtered out in $\\mathcal{S}$. In addition, we observe $\\mathcal{S}$ has retained most of the procedure information related to the task, and can align more contents of the videos than the wikiHow-steps. which are widely in the previous work [28,34,55]. Please refer to the supplementary material for more examples.\n\nMulti-Pathway Text-Video Alignment The obtained LLM-steps are not accompanied by annotations indicating temporal alignment with videos. To tackle this challenge, we propose the Multi-Pathway Text-Video Alignment (MPTVA) strategy to generate reliable pseudo-alignment between LLM-steps and video segments for training. The key idea is to measure the alignment between $\\mathcal{S}$ and $\\mathcal{V}$ in different pathways, each of which provides complementary information to others. The results from different pathways are then fused to extract the pseudomatching relationship.\n\nSpecifically, we measure their alignment using three different pathways. These include: (1) aligning $\\mathcal{S}$ with narrations $\\mathcal{N}$ and then to $\\mathcal{V}$ (S-N-V), (2) directly aligning $S$ to $\\mathcal{V}$ using the long-term video-text alignment models pre-trained in the instructional video domain (S-V-long), and (3) direct alignment using a video-text foundation model pre-trained on short-term video-text datasets from various domains (S-V-short). The three pathways capture the relationship between $\\mathcal{S}$ and $\\mathcal{V}$ based on different information, including the timestamps",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "and (3) direct alignment using a video-text foundation model pre-trained on short-term video-text datasets from various domains (S-V-short).",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00cbcbf6-71b5-451b-93df-c7bf5966aa79",
        "questions": "What optimizer is used in the model training process described in the document, and what is the learning rate set to?",
        "answers": "AdamW optimizer with a learning rate of $2 \\times 10^{-4}$",
        "context": "A. 4 Model Training\n\n\nFollowing the common practice $[21,34]$, we divide each input video into a sequence of 1 -second non-overlapped segments and set the frame rate to 16 FPS. The video backbone, $f_{b}[35]$, is used to extract a feature embedding from each segment. The maximal input video segment length is set to 1024 . We set the batch size to 32 . We apply the AdamW optimizer [31] with a learning rate of $2 \\times 10^{-4}$ and weight decay ratio $1 \\times 10^{-5}$ for training. We train the models for 12 epochs. The learning rate is linearly warmed up for the first 1000 iterations and followed by a cosine decay.\n\nA. 5 Downstream Tasks\n\nFollowing the common setting [34], the trained models are directly tested on the downstream datasets without additional fine-tuning. The trained models take the text descriptions of procedural steps (action steps or narrations) and video segments as input, and output the similarity scores between procedural steps and video segments. The metric for each downstream task is calculated from these similarity scores.\n\nB Additional Experiment Results\n\nB. 1 Comparison of different types of text information\n\nWe additionally show three videos and their narrations, wikiHow-Steps extracted following [21], and LLM-steps in Figure 1, 2, and 3. These examples further demonstrate that our LLM steps are more descriptive and contain less information that is unrelated to the video task than narrations. In addition, we observe that LLM-steps align the task contents better than wikiHow-steps, demonstrating the effectiveness of our LLM-steps.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We apply the AdamW optimizer [31] with a learning rate of $2 \\times 10^{-4}$ and weight decay ratio $1 \\times 10^{-5}$ for training.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00ce4721-4ff0-494d-b15f-efa4d84c1087",
        "questions": "How many epochs are the models trained for in the described training process?",
        "answers": "12",
        "context": "A. 4 Model Training\n\n\nFollowing the common practice $[21,34]$, we divide each input video into a sequence of 1 -second non-overlapped segments and set the frame rate to 16 FPS. The video backbone, $f_{b}[35]$, is used to extract a feature embedding from each segment. The maximal input video segment length is set to 1024 . We set the batch size to 32 . We apply the AdamW optimizer [31] with a learning rate of $2 \\times 10^{-4}$ and weight decay ratio $1 \\times 10^{-5}$ for training. We train the models for 12 epochs. The learning rate is linearly warmed up for the first 1000 iterations and followed by a cosine decay.\n\nA. 5 Downstream Tasks\n\nFollowing the common setting [34], the trained models are directly tested on the downstream datasets without additional fine-tuning. The trained models take the text descriptions of procedural steps (action steps or narrations) and video segments as input, and output the similarity scores between procedural steps and video segments. The metric for each downstream task is calculated from these similarity scores.\n\nB Additional Experiment Results\n\nB. 1 Comparison of different types of text information\n\nWe additionally show three videos and their narrations, wikiHow-Steps extracted following [21], and LLM-steps in Figure 1, 2, and 3. These examples further demonstrate that our LLM steps are more descriptive and contain less information that is unrelated to the video task than narrations. In addition, we observe that LLM-steps align the task contents better than wikiHow-steps, demonstrating the effectiveness of our LLM-steps.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We train the models for 12 epochs.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00d1e139-b8cd-469b-8e7e-6db1ef4c618d",
        "questions": "Does the document indicate that the trained models are fine-tuned on downstream datasets before testing?",
        "answers": "No",
        "context": "A. 4 Model Training\n\n\nFollowing the common practice $[21,34]$, we divide each input video into a sequence of 1 -second non-overlapped segments and set the frame rate to 16 FPS. The video backbone, $f_{b}[35]$, is used to extract a feature embedding from each segment. The maximal input video segment length is set to 1024 . We set the batch size to 32 . We apply the AdamW optimizer [31] with a learning rate of $2 \\times 10^{-4}$ and weight decay ratio $1 \\times 10^{-5}$ for training. We train the models for 12 epochs. The learning rate is linearly warmed up for the first 1000 iterations and followed by a cosine decay.\n\nA. 5 Downstream Tasks\n\nFollowing the common setting [34], the trained models are directly tested on the downstream datasets without additional fine-tuning. The trained models take the text descriptions of procedural steps (action steps or narrations) and video segments as input, and output the similarity scores between procedural steps and video segments. The metric for each downstream task is calculated from these similarity scores.\n\nB Additional Experiment Results\n\nB. 1 Comparison of different types of text information\n\nWe additionally show three videos and their narrations, wikiHow-Steps extracted following [21], and LLM-steps in Figure 1, 2, and 3. These examples further demonstrate that our LLM steps are more descriptive and contain less information that is unrelated to the video task than narrations. In addition, we observe that LLM-steps align the task contents better than wikiHow-steps, demonstrating the effectiveness of our LLM-steps.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Following the common setting [34], the trained models are directly tested on the downstream datasets without additional fine-tuning.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00d2870b-4b5c-41ed-b2bb-ccfc34ff496a",
        "questions": "What is the main challenge in using narrated instructional videos for training models to localize procedural steps?",
        "answers": "The noise mainly originates from two aspects. Firstly, some narrations may describe the information that is irrelevant to the task of the video.",
        "context": "Fig. 1: Illustration of different text information in narrated instruction videos. The orange and green bars along the time $(\\mathrm{t})$ axis denote the temporal boundaries of procedure steps and timestamps of narrations, respectively. Sentences highlighted in green indicate task-relevant information, while those in red are task-irrelevant.\ndomains. Instructional videos are in long duration since they aim to provide comprehensive visual guidance and instructions on task execution. Therefore, developing an automated system that can temporally localize procedural steps within instructional videos is crucial to facilitate the accessibility and utility of these educational resources.\n\nPrevious works addressed the task of localizing procedure steps either in a fully supervised $[4,5,26,44]$ or weakly supervised $[7,18,32]$ manners. These methods train models with annotated temporal boundaries or sequence orders of procedure steps. Although these methods have achieved notable performance, they require massive training videos with annotations, which are expensive and labor-intensive to collect. Recent methods $[16,21,34,41]$ have explored training models using narrated instructional videos. Specifically, they train models to learn the cross-modal alignment between the text narrations and video segments through contrastive learning [35], where the temporally overlapped video segments and narrations are treated as positive pairs.\n\nHowever, this approach is suboptimal since narrations are noisily aligned with video contents [21]. The noise mainly originates from two aspects. Firstly, some narrations may describe the information that is irrelevant to the task of the video. For instance, as illustrated in Figure 1, narrations highlighted in red boxes discuss the demonstrator's interest in a food or background sounds, which are not related to the task of making a salad. Furthermore, the temporal correspondence between video frames and narrations derived from narrations' timestamps may not be consistently reliable since demonstrators may introduce steps before or after executing them. As illustrated in Figure 1, the demonstrator introduces the procedure \"take a drizzle of olive oil\" before actually performing it.\n\nPrevious methods $[28,34,55]$ propose to mitigate the first problem by leveraging the procedure steps defined by the human-constructed knowledge base wikiHow [25], denoted as wikiHow-steps, to supplement or replace narrations. However, wikiHow-steps may not always align with the actions demonstrated in the videos, as a task can be executed in ways that are not outlined in the",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The noise mainly originates from two aspects. Firstly, some narrations may describe the information that is irrelevant to the task of the video.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00d3470f-abed-43b1-89e6-6c0421125186",
        "questions": "What is one method proposed to address the issue of irrelevant narrations in instructional videos?",
        "answers": "Previous methods propose to mitigate the first problem by leveraging the procedure steps defined by the human-constructed knowledge base wikiHow.",
        "context": "Fig. 1: Illustration of different text information in narrated instruction videos. The orange and green bars along the time $(\\mathrm{t})$ axis denote the temporal boundaries of procedure steps and timestamps of narrations, respectively. Sentences highlighted in green indicate task-relevant information, while those in red are task-irrelevant.\ndomains. Instructional videos are in long duration since they aim to provide comprehensive visual guidance and instructions on task execution. Therefore, developing an automated system that can temporally localize procedural steps within instructional videos is crucial to facilitate the accessibility and utility of these educational resources.\n\nPrevious works addressed the task of localizing procedure steps either in a fully supervised $[4,5,26,44]$ or weakly supervised $[7,18,32]$ manners. These methods train models with annotated temporal boundaries or sequence orders of procedure steps. Although these methods have achieved notable performance, they require massive training videos with annotations, which are expensive and labor-intensive to collect. Recent methods $[16,21,34,41]$ have explored training models using narrated instructional videos. Specifically, they train models to learn the cross-modal alignment between the text narrations and video segments through contrastive learning [35], where the temporally overlapped video segments and narrations are treated as positive pairs.\n\nHowever, this approach is suboptimal since narrations are noisily aligned with video contents [21]. The noise mainly originates from two aspects. Firstly, some narrations may describe the information that is irrelevant to the task of the video. For instance, as illustrated in Figure 1, narrations highlighted in red boxes discuss the demonstrator's interest in a food or background sounds, which are not related to the task of making a salad. Furthermore, the temporal correspondence between video frames and narrations derived from narrations' timestamps may not be consistently reliable since demonstrators may introduce steps before or after executing them. As illustrated in Figure 1, the demonstrator introduces the procedure \"take a drizzle of olive oil\" before actually performing it.\n\nPrevious methods $[28,34,55]$ propose to mitigate the first problem by leveraging the procedure steps defined by the human-constructed knowledge base wikiHow [25], denoted as wikiHow-steps, to supplement or replace narrations. However, wikiHow-steps may not always align with the actions demonstrated in the videos, as a task can be executed in ways that are not outlined in the",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Previous methods $[28,34,55]$ propose to mitigate the first problem by leveraging the procedure steps defined by the human-constructed knowledge base wikiHow [25], denoted as wikiHow-steps, to supplement or replace narrations.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00d3d1af-353e-4c86-a4ae-ba79e4b82987",
        "questions": "Is the temporal correspondence between video frames and narrations consistently reliable in narrated instructional videos?",
        "answers": "No",
        "context": "Fig. 1: Illustration of different text information in narrated instruction videos. The orange and green bars along the time $(\\mathrm{t})$ axis denote the temporal boundaries of procedure steps and timestamps of narrations, respectively. Sentences highlighted in green indicate task-relevant information, while those in red are task-irrelevant.\ndomains. Instructional videos are in long duration since they aim to provide comprehensive visual guidance and instructions on task execution. Therefore, developing an automated system that can temporally localize procedural steps within instructional videos is crucial to facilitate the accessibility and utility of these educational resources.\n\nPrevious works addressed the task of localizing procedure steps either in a fully supervised $[4,5,26,44]$ or weakly supervised $[7,18,32]$ manners. These methods train models with annotated temporal boundaries or sequence orders of procedure steps. Although these methods have achieved notable performance, they require massive training videos with annotations, which are expensive and labor-intensive to collect. Recent methods $[16,21,34,41]$ have explored training models using narrated instructional videos. Specifically, they train models to learn the cross-modal alignment between the text narrations and video segments through contrastive learning [35], where the temporally overlapped video segments and narrations are treated as positive pairs.\n\nHowever, this approach is suboptimal since narrations are noisily aligned with video contents [21]. The noise mainly originates from two aspects. Firstly, some narrations may describe the information that is irrelevant to the task of the video. For instance, as illustrated in Figure 1, narrations highlighted in red boxes discuss the demonstrator's interest in a food or background sounds, which are not related to the task of making a salad. Furthermore, the temporal correspondence between video frames and narrations derived from narrations' timestamps may not be consistently reliable since demonstrators may introduce steps before or after executing them. As illustrated in Figure 1, the demonstrator introduces the procedure \"take a drizzle of olive oil\" before actually performing it.\n\nPrevious methods $[28,34,55]$ propose to mitigate the first problem by leveraging the procedure steps defined by the human-constructed knowledge base wikiHow [25], denoted as wikiHow-steps, to supplement or replace narrations. However, wikiHow-steps may not always align with the actions demonstrated in the videos, as a task can be executed in ways that are not outlined in the",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Furthermore, the temporal correspondence between video frames and narrations derived from narrations' timestamps may not be consistently reliable since demonstrators may introduce steps before or after executing them.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00e1e415-040b-4601-823a-0864be1b885c",
        "questions": "What is the percentage improvement in performance for narration grounding achieved by the method compared to VINA on the HTM-Align dataset?",
        "answers": "2.8%",
        "context": "4.4 Comparison with State-of-the-Art Approaches\n\n\nResults For Narration and Step Grounding We compare our method with state-of-the-art approaches for narration and step grounding tasks on the HTM-Align and HT-Step datasets, and the results are presented in Table 4. The results for narration grounding on the HTM-Align dataset were obtained by training a model that adds positional embedding to the text input, following previous work [34]. We find that our method significantly outperforms previous approaches, achieving state-of-the-art performance for both narration and step-grounding tasks. Notably, our method improves the performance of VINA, which uses narrations and procedure steps from the wikiHow dataset for training, by $2.8 \\%$ and $5.9 \\%$ for narration and step grounding, respectively. Moreover, VINA [34] additionally uses narrations of videos during test time. In a fair comparison, where narrations are not available during testing, our method outperforms VINA (VINA w/o nar) by $7.7 \\%$. The results further demonstrate the effectiveness of our proposed methods.\n\nResults for Action Step Localization We further compare our model with other works on action step localization using the CrossTask dataset. The results are reported in Table 5. Our method improves the state-of-the-art performance by $3.1 \\%$. The results show the strong transfer capabilities of our learned models to different downstream datasets. In addition, the results indicate that our model can handle text with varying levels of information granularity, from abstract action steps to detailed, enriched procedure steps and various narrations.\n\n5 Conclusions\n\nIn this work, we introduce a novel learning framework designed to train models for localizing procedure steps in noisy, narrated instructional videos. Initially, we utilize LLMs to filter out text information irrelevant to the tasks depicted in the videos and to summarize task-related procedural steps from the narrations. Subsequently, we employ our proposed Multi-Pathway Text-Video Alignment strategy to generate pseudo-alignments between LLM-steps and video segments for training purposes. Extensive experiments conducted across three datasets for three downstream tasks show that our method establishes new state-of-the-art benchmarks. Furthermore, ablation studies confirm that the procedural steps extracted by our framework serve as superior text inputs compared to both narration and procedural steps derived from human-constructed knowledge bases.\n\nAcknowledgements This research has been partially funded by research grants to Dimitris N. Metaxas through NSF: 2310966, 2235405, 2212301, 2003874, and FA9550-23-1-0417 and NIH 2R01HL127661.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Notably, our method improves the performance of VINA, which uses narrations and procedure steps from the wikiHow dataset for training, by $2.8 \\%$ and $5.9 \\%$ for narration and step grounding, respectively.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00e954f6-f574-48f9-bd16-b04f2cf6c298",
        "questions": "Which dataset was used to compare the model's performance on action step localization, and what was the percentage improvement achieved?",
        "answers": "CrossTask dataset, 3.1%",
        "context": "4.4 Comparison with State-of-the-Art Approaches\n\n\nResults For Narration and Step Grounding We compare our method with state-of-the-art approaches for narration and step grounding tasks on the HTM-Align and HT-Step datasets, and the results are presented in Table 4. The results for narration grounding on the HTM-Align dataset were obtained by training a model that adds positional embedding to the text input, following previous work [34]. We find that our method significantly outperforms previous approaches, achieving state-of-the-art performance for both narration and step-grounding tasks. Notably, our method improves the performance of VINA, which uses narrations and procedure steps from the wikiHow dataset for training, by $2.8 \\%$ and $5.9 \\%$ for narration and step grounding, respectively. Moreover, VINA [34] additionally uses narrations of videos during test time. In a fair comparison, where narrations are not available during testing, our method outperforms VINA (VINA w/o nar) by $7.7 \\%$. The results further demonstrate the effectiveness of our proposed methods.\n\nResults for Action Step Localization We further compare our model with other works on action step localization using the CrossTask dataset. The results are reported in Table 5. Our method improves the state-of-the-art performance by $3.1 \\%$. The results show the strong transfer capabilities of our learned models to different downstream datasets. In addition, the results indicate that our model can handle text with varying levels of information granularity, from abstract action steps to detailed, enriched procedure steps and various narrations.\n\n5 Conclusions\n\nIn this work, we introduce a novel learning framework designed to train models for localizing procedure steps in noisy, narrated instructional videos. Initially, we utilize LLMs to filter out text information irrelevant to the tasks depicted in the videos and to summarize task-related procedural steps from the narrations. Subsequently, we employ our proposed Multi-Pathway Text-Video Alignment strategy to generate pseudo-alignments between LLM-steps and video segments for training purposes. Extensive experiments conducted across three datasets for three downstream tasks show that our method establishes new state-of-the-art benchmarks. Furthermore, ablation studies confirm that the procedural steps extracted by our framework serve as superior text inputs compared to both narration and procedural steps derived from human-constructed knowledge bases.\n\nAcknowledgements This research has been partially funded by research grants to Dimitris N. Metaxas through NSF: 2310966, 2235405, 2212301, 2003874, and FA9550-23-1-0417 and NIH 2R01HL127661.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We further compare our model with other works on action step localization using the CrossTask dataset. The results are reported in Table 5. Our method improves the state-of-the-art performance by $3.1 \\%$.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00f24603-8295-4fe0-92c5-53b5d2f74429",
        "questions": "Did the method outperform VINA when narrations were not available during testing, and if so, by what percentage?",
        "answers": "Yes, by 7.7%",
        "context": "4.4 Comparison with State-of-the-Art Approaches\n\n\nResults For Narration and Step Grounding We compare our method with state-of-the-art approaches for narration and step grounding tasks on the HTM-Align and HT-Step datasets, and the results are presented in Table 4. The results for narration grounding on the HTM-Align dataset were obtained by training a model that adds positional embedding to the text input, following previous work [34]. We find that our method significantly outperforms previous approaches, achieving state-of-the-art performance for both narration and step-grounding tasks. Notably, our method improves the performance of VINA, which uses narrations and procedure steps from the wikiHow dataset for training, by $2.8 \\%$ and $5.9 \\%$ for narration and step grounding, respectively. Moreover, VINA [34] additionally uses narrations of videos during test time. In a fair comparison, where narrations are not available during testing, our method outperforms VINA (VINA w/o nar) by $7.7 \\%$. The results further demonstrate the effectiveness of our proposed methods.\n\nResults for Action Step Localization We further compare our model with other works on action step localization using the CrossTask dataset. The results are reported in Table 5. Our method improves the state-of-the-art performance by $3.1 \\%$. The results show the strong transfer capabilities of our learned models to different downstream datasets. In addition, the results indicate that our model can handle text with varying levels of information granularity, from abstract action steps to detailed, enriched procedure steps and various narrations.\n\n5 Conclusions\n\nIn this work, we introduce a novel learning framework designed to train models for localizing procedure steps in noisy, narrated instructional videos. Initially, we utilize LLMs to filter out text information irrelevant to the tasks depicted in the videos and to summarize task-related procedural steps from the narrations. Subsequently, we employ our proposed Multi-Pathway Text-Video Alignment strategy to generate pseudo-alignments between LLM-steps and video segments for training purposes. Extensive experiments conducted across three datasets for three downstream tasks show that our method establishes new state-of-the-art benchmarks. Furthermore, ablation studies confirm that the procedural steps extracted by our framework serve as superior text inputs compared to both narration and procedural steps derived from human-constructed knowledge bases.\n\nAcknowledgements This research has been partially funded by research grants to Dimitris N. Metaxas through NSF: 2310966, 2235405, 2212301, 2003874, and FA9550-23-1-0417 and NIH 2R01HL127661.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "In a fair comparison, where narrations are not available during testing, our method outperforms VINA (VINA w/o nar) by $7.7 \\%$.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00f276c9-41cf-41a1-bdc5-515ab78331b9",
        "questions": "Which publication discusses the adaptation of large language models for clinical text summarization and claims they can outperform human experts?",
        "answers": "Clinical text summarization: Adapting large language models can outperform human experts.",
        "context": "47. Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.B., Aali, A., Bluethgen, C., Pareek, A., Polacin, M., Reis, E.P., Seehofnerova, A., et al.: Clinical text summarization: Adapting large language models can outperform human experts. Research Square (2023)\n48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\n49. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n50. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084 (2021)\n51. Xue, H., Sun, Y., Liu, B., Fu, J., Song, R., Li, H., Luo, J.: Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430 (2022)\n52. Ye, J., Gao, J., Li, Q., Xu, H., Feng, J., Wu, Z., Yu, T., Kong, L.: Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922 (2022)\n53. Zhao, L., Gundavarapu, N.B., Yuan, L., Zhou, H., Yan, S., Sun, J.J., Friedman, L., Qian, R., Weyand, T., Zhao, Y., et al.: Videoprism: A foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217 (2024)\n54. Zhao, Q., Zhang, C., Wang, S., Fu, C., Agarwal, N., Lee, K., Sun, C.: Antgpt: Can large language models help long-term action anticipation from videos? arXiv preprint arXiv:2307.16368 (2023)\n55. Zhou, H., Mart\u00edn-Mart\u00edn, R., Kapadia, M., Savarese, S., Niebles, J.C.: Procedureaware pretraining for instructional video understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10727$10738(2023)$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.B., Aali, A., Bluethgen, C., Pareek, A., Polacin, M., Reis, E.P., Seehofnerova, A., et al.: Clinical text summarization: Adapting large language models can outperform human experts. Research Square (2023)",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "00f520f4-9ef1-4846-9dbc-24fd0a8aa1e4",
        "questions": "In which year was the paper 'Attention is all you need' published in the Advances in Neural Information Processing Systems?",
        "answers": "2017",
        "context": "47. Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.B., Aali, A., Bluethgen, C., Pareek, A., Polacin, M., Reis, E.P., Seehofnerova, A., et al.: Clinical text summarization: Adapting large language models can outperform human experts. Research Square (2023)\n48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\n49. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n50. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084 (2021)\n51. Xue, H., Sun, Y., Liu, B., Fu, J., Song, R., Li, H., Luo, J.: Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430 (2022)\n52. Ye, J., Gao, J., Li, Q., Xu, H., Feng, J., Wu, Z., Yu, T., Kong, L.: Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922 (2022)\n53. Zhao, L., Gundavarapu, N.B., Yuan, L., Zhou, H., Yan, S., Sun, J.J., Friedman, L., Qian, R., Weyand, T., Zhao, Y., et al.: Videoprism: A foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217 (2024)\n54. Zhao, Q., Zhang, C., Wang, S., Fu, C., Agarwal, N., Lee, K., Sun, C.: Antgpt: Can large language models help long-term action anticipation from videos? arXiv preprint arXiv:2307.16368 (2023)\n55. Zhou, H., Mart\u00edn-Mart\u00edn, R., Kapadia, M., Savarese, S., Niebles, J.C.: Procedureaware pretraining for instructional video understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10727$10738(2023)$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "01055b03-085b-4b3e-ab19-3fd25bbdb887",
        "questions": "Is the paper titled 'Videoprism: A foundational visual encoder for video understanding' expected to be published in 2024?",
        "answers": "Yes",
        "context": "47. Van Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J.B., Aali, A., Bluethgen, C., Pareek, A., Polacin, M., Reis, E.P., Seehofnerova, A., et al.: Clinical text summarization: Adapting large language models can outperform human experts. Research Square (2023)\n48. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)\n49. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n50. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084 (2021)\n51. Xue, H., Sun, Y., Liu, B., Fu, J., Song, R., Li, H., Luo, J.: Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430 (2022)\n52. Ye, J., Gao, J., Li, Q., Xu, H., Feng, J., Wu, Z., Yu, T., Kong, L.: Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922 (2022)\n53. Zhao, L., Gundavarapu, N.B., Yuan, L., Zhou, H., Yan, S., Sun, J.J., Friedman, L., Qian, R., Weyand, T., Zhao, Y., et al.: Videoprism: A foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217 (2024)\n54. Zhao, Q., Zhang, C., Wang, S., Fu, C., Agarwal, N., Lee, K., Sun, C.: Antgpt: Can large language models help long-term action anticipation from videos? arXiv preprint arXiv:2307.16368 (2023)\n55. Zhou, H., Mart\u00edn-Mart\u00edn, R., Kapadia, M., Savarese, S., Niebles, J.C.: Procedureaware pretraining for instructional video understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10727$10738(2023)$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Zhao, L., Gundavarapu, N.B., Yuan, L., Zhou, H., Yan, S., Sun, J.J., Friedman, L., Qian, R., Weyand, T., Zhao, Y., et al.: Videoprism: A foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217 (2024)",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "010bd4fe-5789-4404-b84b-f63b4f3ef6b2",
        "questions": "What is the R@1 percentage for the HT-Step dataset when using the S-V-Short pathway?",
        "answers": "37.7",
        "context": "Table 1: Results for the step grounding and action step localization on the HTStep and CrossTask datasets when using different pathways to generate pseudo-labels. \"ZS\" indicates applying the correspondent pre-trained video-text alignment models to perform the downstream tasks in a zero-shot manner. The symbol $\\sqrt{ }$ indicates that the respective pathways are used.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multicolumn{3}{|c|}{Pathway Type} & HT-Step & CrossTask \\\\\n  S-N-V & S-V-Long & S-V-Short & R@1 (\\%) \u4e2a & Avg. R@1(\\%)\u4e2a \\\\\n  & ZS & & 30.3 & 32.3 \\\\\n  & & ZS & 37.3 & 39.7 \\\\\n  $\\checkmark$ & & & 34.2 & 36.4 \\\\\n  & $\\checkmark$ & & 35.0 & 39.1 \\\\\n  & & $\\checkmark$ & 37.7 & 41.6 \\\\\n  $\\checkmark$ & $\\checkmark$ & & 37.9 & 42.6 \\\\\n  $\\checkmark$ & & $\\checkmark$ & 38.0 & 43.7 \\\\\n  & $\\checkmark$ & $\\sqrt{ }$ & 38.9 & 42.5 \\\\\n  $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 41.9 & 47.0 \\\\\n \n\\end{tabular}\nlearning rate follows a cosine decay over 12 epochs. For further details, please refer to the supplementary material.\n\n\n4.3 Ablation Study\n\n\nEffectiveness of Multi-Pathway Text-Video Alignment We evaluate the effectiveness of our proposed MPTVA strategy by comparing the performances of models trained with labels generated through various pathways. The results are shown in Table 1. It is observed that models trained exclusively with S-VShort or S-V-Long labels significantly outperform the pre-trained short-term and long-term video-text alignment models, respectively. The results suggest that the models learn knowledge surpassing the capabilities of the alignment models used for pseudo-label extraction. We can see that the model trained with $\\mathrm{S}-\\mathrm{N}-\\mathrm{V}$ underperforms those trained with S-V-Short or S-V-Long. This may be attributed to the noise present in the timestamps. Moreover, we can observe that using any combination of two pathways achieves better performance improvement than using only one pathway. More importantly, the best improvement is achieved when considering all three pathways. The results demonstrate that each pathway provides complementary information. By fusing these pathways, we can leverage this combined information, resulting in more reliable pseudo-labels.\n\nEffectiveness of LLM-steps To evaluate the effectiveness of LLM-steps, we compare the model trained with different text inputs, including narrations, procedure steps extracted from the wikiHow database following [34], and our LLMsteps. The results are reported in Table 2. It can be observed that the N-TS",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& & $\\checkmark$ & 37.7 & 41.6 \\",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0111aca2-9949-47f1-a83d-53d5bce4588e",
        "questions": "Does the model trained with the S-N-V pathway outperform those trained with S-V-Short or S-V-Long pathways?",
        "answers": "No",
        "context": "Table 1: Results for the step grounding and action step localization on the HTStep and CrossTask datasets when using different pathways to generate pseudo-labels. \"ZS\" indicates applying the correspondent pre-trained video-text alignment models to perform the downstream tasks in a zero-shot manner. The symbol $\\sqrt{ }$ indicates that the respective pathways are used.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multicolumn{3}{|c|}{Pathway Type} & HT-Step & CrossTask \\\\\n  S-N-V & S-V-Long & S-V-Short & R@1 (\\%) \u4e2a & Avg. R@1(\\%)\u4e2a \\\\\n  & ZS & & 30.3 & 32.3 \\\\\n  & & ZS & 37.3 & 39.7 \\\\\n  $\\checkmark$ & & & 34.2 & 36.4 \\\\\n  & $\\checkmark$ & & 35.0 & 39.1 \\\\\n  & & $\\checkmark$ & 37.7 & 41.6 \\\\\n  $\\checkmark$ & $\\checkmark$ & & 37.9 & 42.6 \\\\\n  $\\checkmark$ & & $\\checkmark$ & 38.0 & 43.7 \\\\\n  & $\\checkmark$ & $\\sqrt{ }$ & 38.9 & 42.5 \\\\\n  $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 41.9 & 47.0 \\\\\n \n\\end{tabular}\nlearning rate follows a cosine decay over 12 epochs. For further details, please refer to the supplementary material.\n\n\n4.3 Ablation Study\n\n\nEffectiveness of Multi-Pathway Text-Video Alignment We evaluate the effectiveness of our proposed MPTVA strategy by comparing the performances of models trained with labels generated through various pathways. The results are shown in Table 1. It is observed that models trained exclusively with S-VShort or S-V-Long labels significantly outperform the pre-trained short-term and long-term video-text alignment models, respectively. The results suggest that the models learn knowledge surpassing the capabilities of the alignment models used for pseudo-label extraction. We can see that the model trained with $\\mathrm{S}-\\mathrm{N}-\\mathrm{V}$ underperforms those trained with S-V-Short or S-V-Long. This may be attributed to the noise present in the timestamps. Moreover, we can observe that using any combination of two pathways achieves better performance improvement than using only one pathway. More importantly, the best improvement is achieved when considering all three pathways. The results demonstrate that each pathway provides complementary information. By fusing these pathways, we can leverage this combined information, resulting in more reliable pseudo-labels.\n\nEffectiveness of LLM-steps To evaluate the effectiveness of LLM-steps, we compare the model trained with different text inputs, including narrations, procedure steps extracted from the wikiHow database following [34], and our LLMsteps. The results are reported in Table 2. It can be observed that the N-TS",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We can see that the model trained with $\\mathrm{S}-\\mathrm{N}-\\mathrm{V}$ underperforms those trained with S-V-Short or S-V-Long.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "01364fc5-1a6e-497f-83cd-7bab667b94c9",
        "questions": "What is the average R@1 percentage for the CrossTask dataset when all three pathways (S-N-V, S-V-Long, S-V-Short) are used?",
        "answers": "47.0",
        "context": "Table 1: Results for the step grounding and action step localization on the HTStep and CrossTask datasets when using different pathways to generate pseudo-labels. \"ZS\" indicates applying the correspondent pre-trained video-text alignment models to perform the downstream tasks in a zero-shot manner. The symbol $\\sqrt{ }$ indicates that the respective pathways are used.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multicolumn{3}{|c|}{Pathway Type} & HT-Step & CrossTask \\\\\n  S-N-V & S-V-Long & S-V-Short & R@1 (\\%) \u4e2a & Avg. R@1(\\%)\u4e2a \\\\\n  & ZS & & 30.3 & 32.3 \\\\\n  & & ZS & 37.3 & 39.7 \\\\\n  $\\checkmark$ & & & 34.2 & 36.4 \\\\\n  & $\\checkmark$ & & 35.0 & 39.1 \\\\\n  & & $\\checkmark$ & 37.7 & 41.6 \\\\\n  $\\checkmark$ & $\\checkmark$ & & 37.9 & 42.6 \\\\\n  $\\checkmark$ & & $\\checkmark$ & 38.0 & 43.7 \\\\\n  & $\\checkmark$ & $\\sqrt{ }$ & 38.9 & 42.5 \\\\\n  $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 41.9 & 47.0 \\\\\n \n\\end{tabular}\nlearning rate follows a cosine decay over 12 epochs. For further details, please refer to the supplementary material.\n\n\n4.3 Ablation Study\n\n\nEffectiveness of Multi-Pathway Text-Video Alignment We evaluate the effectiveness of our proposed MPTVA strategy by comparing the performances of models trained with labels generated through various pathways. The results are shown in Table 1. It is observed that models trained exclusively with S-VShort or S-V-Long labels significantly outperform the pre-trained short-term and long-term video-text alignment models, respectively. The results suggest that the models learn knowledge surpassing the capabilities of the alignment models used for pseudo-label extraction. We can see that the model trained with $\\mathrm{S}-\\mathrm{N}-\\mathrm{V}$ underperforms those trained with S-V-Short or S-V-Long. This may be attributed to the noise present in the timestamps. Moreover, we can observe that using any combination of two pathways achieves better performance improvement than using only one pathway. More importantly, the best improvement is achieved when considering all three pathways. The results demonstrate that each pathway provides complementary information. By fusing these pathways, we can leverage this combined information, resulting in more reliable pseudo-labels.\n\nEffectiveness of LLM-steps To evaluate the effectiveness of LLM-steps, we compare the model trained with different text inputs, including narrations, procedure steps extracted from the wikiHow database following [34], and our LLMsteps. The results are reported in Table 2. It can be observed that the N-TS",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 41.9 & 47.0 \\",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "013a3cfa-d2b2-4882-bb2d-32d0a65c2fea",
        "questions": "What is the Recall@1 score for the method 'VINA' on the HTM-Align dataset from the comparison table?",
        "answers": "66.5",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "VINA [34] & 66.5 & 37.4 \\",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0140a718-b50a-4a65-aca3-6771c7ef28e7",
        "questions": "Which method achieved the highest average Recall@1 percentage on the CrossTask Dataset for action step localization?",
        "answers": "Ours",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Ours & \\mathbf{4 7 . 9} \\",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "0156f13c-8098-4b3e-8734-627f1c3726f5",
        "questions": "What is the difference in performance between 'InternVideo-MM-L14' and 'VINA' on the HT-Step dataset?",
        "answers": "0.1",
        "context": "Table 4: Comparison with state-of-the-art methods for the narration grounding and step grounding on the HTM-Align and HT-Step datasets, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Method } & \\multicolumn{2}{c}{ HTM-AlignHT-Step } \\\\\n& R@1 $\\uparrow$ & R@1 $\\uparrow$ \\\\\n  CLIP (ViT-B $/ 32)[38]$ & 23.4 & - \\\\\nMIL-NCE [35] & 34.2 & 30.7 \\\\\nInternVideo-MM-L14 [49] & 40.1 & 37.3 \\\\\nTAN [21] & 49.4 & - \\\\\nTAN* (Joint, S1, PE+LC) [34] & 63 & 31.2 \\\\\nVINA(w/o nar) [34] & - & 35.6 \\\\\nVINA [34] & 66.5 & 37.4 \\\\\n  Ours & $\\mathbf{6 9 . 3}$ & $\\mathbf{4 3 . 3}$ \\\\\n \n\\end{tabular}\n\nTable 5: Comparison with state-of-the-art methods for action step localization on CrossTask Dataset.\n\\begin{tabular}{cc}\n  Method & $\\uparrow$ Avg. R@1 (\\%) \\\\\n  HT100M [36] & 33.6 \\\\\nVideoCLIP [50] & 33.9 \\\\\nMCN [8] & 35.1 \\\\\nDWSA [41] & 35.3 \\\\\nMIL-NCE [35] & 40.5 \\\\\nZhukov [18] & 40.5 \\\\\nVT-TWINS* [24] & 40.7 \\\\\nUniVL [33] & 42.0 \\\\\nVINA [34] & 44.8 \\\\\n  Ours & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\n\nAblation of Filtering Threshold and Window Size for Pseudo-label Generation We report the performance of models trained with pseudo-labels generated with different filtering thresholds $(\\gamma)$ and window sizes $(W)$ in Table 3. We first fix the $W$ as 5 to assess the influence of $\\gamma$. We can see that the best performance is observed when $\\gamma$ is set to 0.65 , and about $15 \\%$ LLM steps are selected for training. Performance decreases as $\\gamma$ decreases from 0.65 , likely due to the introduction of more noisy pseudo-labels. In addition, increasing $\\gamma$ from 0.65 significantly drops the performances, since a large percent of data $(97 \\%)$ is excluded. We then fix $\\gamma$ and evaluate the impact of varying $W$. We can the best performance is achieved when $W$ is 2 . Increasing $W$ beyond this point reduces performance, likely due to the mislabeling of irrelevant frames as matched. Additionally, decreasing $W$ from 2 to 1 results in a slight decrease in performance, as it causes some nearby relevant frames to be annotated as unmatched.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "InternVideo-MM-L14 [49] & 40.1 & 37.3 \\ VINA [34] & 66.5 & 37.4 \\",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "015a50ba-86d3-4bfe-9de4-5628ad100cbf",
        "questions": "What is the valid step ratio when the pseudo-labels are generated using a filter threshold $\\gamma$ of 0.70 and a window size $W$ of 5?",
        "answers": "3 %",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "0.70 & 5 & $3 \\%$ & 35.4 & 41.2",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "015cd1e7-2bc7-4ba4-b0c6-319ca891e09a",
        "questions": "How does the performance of the N-MPTVA model compare to the N-TS model on the HT-Step dataset according to the evaluation metric R@1?",
        "answers": "N-MPTVA: 32.1, N-TS: 30.3",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "N-TS & 30.3 & 32.3 \\ N-MPTVA & 32.1 & 38.0",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "01650350-67db-4350-b368-89013bc3411c",
        "questions": "When using a filter threshold $\\gamma$ of 0.65 and a window size $W$ of 2, what are the R@1 scores achieved on HT-Step and CrossTask datasets?",
        "answers": "HT-Step: 41.9, CrossTask: 47.0",
        "context": "Table 2: Results for step grounding and action step localization on the HT-Step and CrossTask datasets when using different training data. \"N\", \"W\", and \"S\" denote narrations, wikiHow-steps, and LLM-steps, respectively. \"TS\" and \"MPTVA\" denote pseudo-labels generated from timestamps and our MPTVA strategy, respectively.\n\\begin{tabular}{ccc}\n  \\multirow{2}{*}{ Training Data } & HT-Step & CrossTask \\\\\n\\cline { 2 - 3 } & R@1 $\\uparrow$ & Avg. R@1 $\\uparrow$ \\\\\n  N-TS & 30.3 & 32.3 \\\\\nN-MPTVA & 32.1 & 38.0 \\\\\nW-MPTVA & 31.2 & 37.8 \\\\\n  N-MPTVA+W-MPTVA & 35.9 & 40.6 \\\\\n  S-MPTVA & 41.9 & 47.0 \\\\\nN-MPTVA + S-MPTVA & $\\mathbf{4 3 . 3}$ & $\\mathbf{4 7 . 9}$ \\\\\n \n\\end{tabular}\n\nTable 3: Results of the models trained with the pseudo-labels that are generated with different filter thresholds $\\gamma$ and window sizes $W$.\n\\begin{tabular}{|c|c|c|c|c|}\n  \\multirow{2}{*}{$\\gamma$} & \\multirow[t]{2}{*}{W} & \\multirow[t]{2}{*}{\\begin{tabular}{l}\nValid Step \\\\\nRatio\n\\end{tabular}} & HT-Step & CrossTask \\\\\n  & & & R@1 (\\%) & Avg. R@1 \\\\\n  0.60 & 5 & $37 \\%$ & 39.2 & 44.2 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n  0.70 & 5 & $3 \\%$ & 35.4 & 41.2 \\\\\n  0.65 & 1 & $15 \\%$ & 40.3 & 45.7 \\\\\n  0.65 & 2 & $15 \\%$ & 41.9 & 47.0 \\\\\n  0.65 & 5 & $15 \\%$ & 40.6 & 45.2 \\\\\n \n\\end{tabular}\nmodel, which takes narrations as input and employs timestamps for supervision, exhibits the lowest performance. Using the pseudo-labels generated by our proposed MPTVA for narrations (N-MPTVA) leads to improved performance over N-TS, showing that our proposed MPTVA strategy is also effective for noisy narrations. Notably, this model (N-MPTVA) surpasses the one (W-MPTVA) utilizing wikiHow-steps as input. The results indicate that, when using the same supervision (MPTVA), wikiHow-steps are less informative than narrations. It may be attributed to mismatches between instruction articles and videos. Additionally, it could be due to the fact that even the matched instruction articles only cover a small portion of the procedure steps depicted in the videos, considering that a task can be performed in multiple ways. In addition, we observe that using narration and wikiHow articles together, as previous work [34], can enhance model performance. Nonetheless, training the models only with our LLMsteps outperforms this setting by $6 \\%$ and $6.4 \\%$ for step-grounding and action step localization tasks, respectively. These results underscore the effectiveness of LLM-steps in providing highly relevant and clean information for procedure step localization. In addition, we also note that incorporating the information of narration with LLM-steps can further boost the performance for $1.4 \\%$ and $0.9 \\%$.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "0.65 & 2 & $15 \\%$ & 41.9 & 47.0",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "01668796-e975-406b-8043-af980dc44252",
        "questions": "How many videos are included in the HT-Step evaluation dataset?",
        "answers": "600",
        "context": "4 Experiment\n\n\n4.1 Dataset\n\nHTM-370K (Training) Following the previous work [21,34] we train our model on the Food \\& Entertaining subset of Howto100M datasets [36]. It consists of about 370 K videos collected from YouTube.\nHT-Step (Evaluation) This dataset [34] consist of 600 videos selected from the Howto100M dataset [36]. Each video is manually matched to an instructional article of the wikiHow dataset [25]. The temporal boundaries of the procedure steps from the instructional articles are annotated if they are present in the videos. We use the dataset to evaluate the performance of models for procedure step grounding. Following the previous work [34], we report the R@1 metric. R@1 is defined as the ratio of successfully recalled steps to the total number of steps that occur, where a step is considered as successfully recalled if its most relevant video segment falls within the ground truth boundary.\nCrossTask (Evaluation) This dataset [18] has about 4.7K instructional videos, which can be divided into 65 related tasks and 18 primary tasks. Following the previous study [34], we use the dataset to evaluate the model for zero-shot action step localization task. Action step localization is to localize the temporal positions for each occurring action step. Compared to the procedure steps in the HT-Step dataset, the action steps represent more atomic actions, and their text descriptions are more abstract, containing fewer details. The metric is Average Recall@1 (Avg. R@1) [34]. It's computed by first calculating the R@1 metric for each task and then averaging across tasks. We report the results averaging across 20 random sets, each containing 1850 videos from the primary tasks [34]. HTM-Align (Evaluation) This dataset [21] contains 80 videos with annotated temporal alignment between ASR transcriptions and the video contents. It is used to evaluate our model for narration grounding. We report the R@1 metric following the common practice $[10,21]$.\n\n4.2 Implementation Details\n\nFollowing the common practice $[21,34]$, we equally divide each input video into a sequence of 1-second non-overlapped segments and set the frame rate to 16 FPS. The pre-trained S3D video encoder, released by [35], is used as the video backbone $f_{b}$ to extract a feature embedding from each segment. Following [21, 34], $f_{b}$ is kept fixed during training. The text backbone $g_{b}$ is a Bag-of-Words (BoW) model based on Word2Vec embeddings [35]. Following [34], we use the TAN* models, a variance of the TAN model [21] that is pre-trained on long-term instructional videos, as $E_{e}^{L}$ and $E_{e}^{L}$. We also use the text encoder of TAN* as $E_{t}$. In addition, we set $E_{t}^{S}$ and $E_{v}^{S}$ as the pre-trained text and video encoders of the InternVideo-MM-L14 model [49]. We use Llama2-7B [46] to extract LLM-steps from narrations. We set the temperature hyperparameters $\\eta$ and $\\tau$ to 0.07 . The model is trained on Nvidia A100 GPUs. We set the batch size to 32. The AdamW optimizer with an initial learning rate of $2 \\times 10^{-4}$ is employed for training. The",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "HT-Step (Evaluation) This dataset [34] consist of 600 videos selected from the Howto100M dataset [36].",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "017b49cf-249c-4be0-a967-2134dfdd8e2d",
        "questions": "What is the frame rate setting used for each input video when the videos are divided into segments during the experiment?",
        "answers": "16 FPS",
        "context": "4 Experiment\n\n\n4.1 Dataset\n\nHTM-370K (Training) Following the previous work [21,34] we train our model on the Food \\& Entertaining subset of Howto100M datasets [36]. It consists of about 370 K videos collected from YouTube.\nHT-Step (Evaluation) This dataset [34] consist of 600 videos selected from the Howto100M dataset [36]. Each video is manually matched to an instructional article of the wikiHow dataset [25]. The temporal boundaries of the procedure steps from the instructional articles are annotated if they are present in the videos. We use the dataset to evaluate the performance of models for procedure step grounding. Following the previous work [34], we report the R@1 metric. R@1 is defined as the ratio of successfully recalled steps to the total number of steps that occur, where a step is considered as successfully recalled if its most relevant video segment falls within the ground truth boundary.\nCrossTask (Evaluation) This dataset [18] has about 4.7K instructional videos, which can be divided into 65 related tasks and 18 primary tasks. Following the previous study [34], we use the dataset to evaluate the model for zero-shot action step localization task. Action step localization is to localize the temporal positions for each occurring action step. Compared to the procedure steps in the HT-Step dataset, the action steps represent more atomic actions, and their text descriptions are more abstract, containing fewer details. The metric is Average Recall@1 (Avg. R@1) [34]. It's computed by first calculating the R@1 metric for each task and then averaging across tasks. We report the results averaging across 20 random sets, each containing 1850 videos from the primary tasks [34]. HTM-Align (Evaluation) This dataset [21] contains 80 videos with annotated temporal alignment between ASR transcriptions and the video contents. It is used to evaluate our model for narration grounding. We report the R@1 metric following the common practice $[10,21]$.\n\n4.2 Implementation Details\n\nFollowing the common practice $[21,34]$, we equally divide each input video into a sequence of 1-second non-overlapped segments and set the frame rate to 16 FPS. The pre-trained S3D video encoder, released by [35], is used as the video backbone $f_{b}$ to extract a feature embedding from each segment. Following [21, 34], $f_{b}$ is kept fixed during training. The text backbone $g_{b}$ is a Bag-of-Words (BoW) model based on Word2Vec embeddings [35]. Following [34], we use the TAN* models, a variance of the TAN model [21] that is pre-trained on long-term instructional videos, as $E_{e}^{L}$ and $E_{e}^{L}$. We also use the text encoder of TAN* as $E_{t}$. In addition, we set $E_{t}^{S}$ and $E_{v}^{S}$ as the pre-trained text and video encoders of the InternVideo-MM-L14 model [49]. We use Llama2-7B [46] to extract LLM-steps from narrations. We set the temperature hyperparameters $\\eta$ and $\\tau$ to 0.07 . The model is trained on Nvidia A100 GPUs. We set the batch size to 32. The AdamW optimizer with an initial learning rate of $2 \\times 10^{-4}$ is employed for training. The",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Following the common practice $[21,34]$, we equally divide each input video into a sequence of 1-second non-overlapped segments and set the frame rate to 16 FPS.",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.16145v1",
        "ID": "01847803-d840-4ccb-a446-eceecaa0a679",
        "questions": "Which models' text encoder is used for both pre-trained text and video encoders as identified in the document?",
        "answers": "InternVideo-MM-L14",
        "context": "4 Experiment\n\n\n4.1 Dataset\n\nHTM-370K (Training) Following the previous work [21,34] we train our model on the Food \\& Entertaining subset of Howto100M datasets [36]. It consists of about 370 K videos collected from YouTube.\nHT-Step (Evaluation) This dataset [34] consist of 600 videos selected from the Howto100M dataset [36]. Each video is manually matched to an instructional article of the wikiHow dataset [25]. The temporal boundaries of the procedure steps from the instructional articles are annotated if they are present in the videos. We use the dataset to evaluate the performance of models for procedure step grounding. Following the previous work [34], we report the R@1 metric. R@1 is defined as the ratio of successfully recalled steps to the total number of steps that occur, where a step is considered as successfully recalled if its most relevant video segment falls within the ground truth boundary.\nCrossTask (Evaluation) This dataset [18] has about 4.7K instructional videos, which can be divided into 65 related tasks and 18 primary tasks. Following the previous study [34], we use the dataset to evaluate the model for zero-shot action step localization task. Action step localization is to localize the temporal positions for each occurring action step. Compared to the procedure steps in the HT-Step dataset, the action steps represent more atomic actions, and their text descriptions are more abstract, containing fewer details. The metric is Average Recall@1 (Avg. R@1) [34]. It's computed by first calculating the R@1 metric for each task and then averaging across tasks. We report the results averaging across 20 random sets, each containing 1850 videos from the primary tasks [34]. HTM-Align (Evaluation) This dataset [21] contains 80 videos with annotated temporal alignment between ASR transcriptions and the video contents. It is used to evaluate our model for narration grounding. We report the R@1 metric following the common practice $[10,21]$.\n\n4.2 Implementation Details\n\nFollowing the common practice $[21,34]$, we equally divide each input video into a sequence of 1-second non-overlapped segments and set the frame rate to 16 FPS. The pre-trained S3D video encoder, released by [35], is used as the video backbone $f_{b}$ to extract a feature embedding from each segment. Following [21, 34], $f_{b}$ is kept fixed during training. The text backbone $g_{b}$ is a Bag-of-Words (BoW) model based on Word2Vec embeddings [35]. Following [34], we use the TAN* models, a variance of the TAN model [21] that is pre-trained on long-term instructional videos, as $E_{e}^{L}$ and $E_{e}^{L}$. We also use the text encoder of TAN* as $E_{t}$. In addition, we set $E_{t}^{S}$ and $E_{v}^{S}$ as the pre-trained text and video encoders of the InternVideo-MM-L14 model [49]. We use Llama2-7B [46] to extract LLM-steps from narrations. We set the temperature hyperparameters $\\eta$ and $\\tau$ to 0.07 . The model is trained on Nvidia A100 GPUs. We set the batch size to 32. The AdamW optimizer with an initial learning rate of $2 \\times 10^{-4}$ is employed for training. The",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "In addition, we set $E_{t}^{S}$ and $E_{v}^{S}$ as the pre-trained text and video encoders of the InternVideo-MM-L14 model [49].",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "019fa768-7836-41c3-a258-e52ce068ce92",
        "questions": "What optimizer is used for the optimization of parameter vector \u03b2 in the training settings of anchor re-weighting?",
        "answers": "Adam optimizer",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The optimization of parameter vector $\boldsymbol{\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ).",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01a02c0d-1161-4db0-b0d0-f5cb084ada1c",
        "questions": "What is the acceleration ratio of the GPT-J model on the TREC dataset?",
        "answers": "2.2",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT-J & $1.5 \times$ & $2.2 \times$ & $2.9 \times$ & $1.9 \times$ \\",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01a08af9-6bd5-4636-a472-af5aea70aed1",
        "questions": "Does the document suggest that isolating label words within the first 5 layers has a more pronounced effect than isolating non-label words in GPT models?",
        "answers": "Yes",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01a39f80-6333-41ec-8c85-ca141ef02a90",
        "questions": "What is the accuracy achieved by the Anchor Re-weighting method on the SST-2 dataset when using 1-shot per class?",
        "answers": "90.07",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Anchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01cf7f93-5e6c-49e4-9768-599decffd101",
        "questions": "Which method achieved the highest label loyalty score when using the GPT-J model?",
        "answers": "ICL (GPT-J)",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ICL (GPT-J) & 100.00 & 100.00 & 56.82",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01de1868-8827-4859-bd02-5676d187bbae",
        "questions": "Does the Hidden $_{\\text {anchor }}$ method achieve better accuracy than the Text $_{\\text {anchor }}$ method when using the GPT2-XL model?",
        "answers": "Yes",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01dee1a1-9a01-4709-acf8-5508e1d49a09",
        "questions": "What is the effect of different numbers of isolated layers on GPT2XL according to the document?",
        "answers": "Effect of different numbers of isolated layers on GPT2XL",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "(a) Effect of different numbers of isolated layers on GPT2XL",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01e50dde-46ca-4ca5-90da-4776dbf9c505",
        "questions": "What mathematical operation is performed on the attention matrix $A_{l}^{h}$ in the implementation of anchor re-weighting?",
        "answers": "Each $A_{l}^{h}(q, p_{i})$ is multiplied by $\\exp \\left(\beta_{0, l h}^{i}\right)$",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\right)$ by $\\exp \\left(\beta_{0, l h}^{i}\right)$ before proceeding with further computations.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01f3fa0f-c501-4c5b-a7bf-d28e8664a162",
        "questions": "What is the formula for the modified attention matrix $\\hat{A}_{l}^{h}(k, j)$ when $k=q$ and $j=p_{i}$ in the anchor re-weighting implementation?",
        "answers": "$\\hat{A}_{l}^{h}(k, j)=\\exp \\left(\beta_{0, l h}^{i}\right) A_{l}^{h}(k, j)$",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$\\hat{A}_{l}^{h}(k, j)= \begin{cases}\\exp \\left(\beta_{0, l h}^{i}\right) A_{l}^{h}(k, j), & \text { if } k=q, j=p_{i} \\ A_{l}^{h}(k, j), & \text { otherwise }\\end{cases}$",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01f5b716-4b50-4821-a6b6-31e03ae688c0",
        "questions": "What is the value of M chosen for the computation of the prediction confusion matrix in the document?",
        "answers": "10",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\text {pred }}$.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "01fcbf68-8e89-454b-81f8-c72f148106b5",
        "questions": "What is the purpose of using the Confusion $_{i j}$ metric in the document?",
        "answers": "To gauge the true degree of confusion between categories $i$ and $k$ for a given model.",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "To gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0203d17e-40d9-46b5-b5af-e2bdb377281f",
        "questions": "How is the feature $\\hat{\\mathbf{k}}_{i}$ calculated in the document?",
        "answers": "$\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "020f1ec9-c5e4-49b6-b98e-2916ff3603f7",
        "questions": "What is the role of label words in the demonstration examples according to the paper 'Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning'?",
        "answers": "Label words in the demonstration examples function as anchors.",
        "context": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning\n\n\n\nLean Wang ${ ^{\\dagger, \\delta}$, Lei $\\mathrm{Li}^{\\dagger}$, Damai Dai ${ }^{\\dagger}$, Deli Chen ${ }^{\\S}$, Hao Zhou ${ }^{\\S}$, Fandong Meng ${ }^{\\S}$, Jie Zhou ${ }^{\\S}$, Xu Sun ${ }^{\\dagger}$ \\\\ ${ }^{\\dagger}$ National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University \\\\ \u00a7Pattern Recognition Center, WeChat AI, Tencent Inc., China \\\\ \\{lean, daidamai,xusun\\}@pku.edu.cn nlp.lilei@gmail.com \\\\ victorchen@deepseek.com \\{tuxzhou,fandongmeng, withtomzhou\\}@tencent.com\n}\n\n\nIn-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies. ${ }^{1}$\n\n\n1 Introduction\n\nIn-context Learning (ICL) has emerged as a powerful capability alongside the development of scaledup large language models (LLMs) (Brown et al., 2020). By instructing LLMs using few-shot demonstration examples, ICL enables them to perform a wide range of tasks, such as text classification (Min et al., 2022a) and mathematical reasoning (Wei et al., 2022). Since ICL does not require updates to millions or trillions of model parameters and relies on human-understandable natural language instructions (Dong et al., 2023), it has become a promising approach for harnessing the full potentiality of LLMs. Despite its significance, the inner working mechanism of ICL remains an open question, garnering considerable interest from research\n\n\\footnotetext{\n${ }^{1}$ https://github.com/lancopku/\nlabel-words-are-anchors\n}\n\n\nFigure 1: Visualization of the information flow in a GPT model performing ICL. The line depth reflects the significance of the information flow from the right word to the left. The flows involving label words are highlighted. Label words gather information from demonstrations in shallow layers, which is then extracted in deep layers for final prediction.\ncommunities (Xie et al., 2022; Dai et al., 2022; Aky\u00fcrek et al., 2022; Li et al., 2023b).\n\nIn this paper, we find that the label words serve as anchors that aggregate and distribute information in ICL. We first visualize the attention interactive pattern between tokens with a GPT model (Brown et al., 2020) on sentiment analysis (Figure 1). Initial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers. ${ }^{2}$ To draw a clearer picture of this phenomenon, we design two metrics based on saliency\n\n\\footnotetext{\n${ }^{2}$ In this paper, \"shallow\" or \"first\" layers refer to those closer to the input, while \"deep\" or \"last\" layers are closer to the output. Here, \"deep layers\" include those around the midpoint, e.g., layers $25-48$ in a 48-layer GPT2-XL.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02114533-d9d1-4b49-9391-e92479cbdc6c",
        "questions": "Which model is used to visualize the attention interactive pattern between tokens in the study of in-context learning?",
        "answers": "GPT model",
        "context": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning\n\n\n\nLean Wang ${ ^{\\dagger, \\delta}$, Lei $\\mathrm{Li}^{\\dagger}$, Damai Dai ${ }^{\\dagger}$, Deli Chen ${ }^{\\S}$, Hao Zhou ${ }^{\\S}$, Fandong Meng ${ }^{\\S}$, Jie Zhou ${ }^{\\S}$, Xu Sun ${ }^{\\dagger}$ \\\\ ${ }^{\\dagger}$ National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University \\\\ \u00a7Pattern Recognition Center, WeChat AI, Tencent Inc., China \\\\ \\{lean, daidamai,xusun\\}@pku.edu.cn nlp.lilei@gmail.com \\\\ victorchen@deepseek.com \\{tuxzhou,fandongmeng, withtomzhou\\}@tencent.com\n}\n\n\nIn-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies. ${ }^{1}$\n\n\n1 Introduction\n\nIn-context Learning (ICL) has emerged as a powerful capability alongside the development of scaledup large language models (LLMs) (Brown et al., 2020). By instructing LLMs using few-shot demonstration examples, ICL enables them to perform a wide range of tasks, such as text classification (Min et al., 2022a) and mathematical reasoning (Wei et al., 2022). Since ICL does not require updates to millions or trillions of model parameters and relies on human-understandable natural language instructions (Dong et al., 2023), it has become a promising approach for harnessing the full potentiality of LLMs. Despite its significance, the inner working mechanism of ICL remains an open question, garnering considerable interest from research\n\n\\footnotetext{\n${ }^{1}$ https://github.com/lancopku/\nlabel-words-are-anchors\n}\n\n\nFigure 1: Visualization of the information flow in a GPT model performing ICL. The line depth reflects the significance of the information flow from the right word to the left. The flows involving label words are highlighted. Label words gather information from demonstrations in shallow layers, which is then extracted in deep layers for final prediction.\ncommunities (Xie et al., 2022; Dai et al., 2022; Aky\u00fcrek et al., 2022; Li et al., 2023b).\n\nIn this paper, we find that the label words serve as anchors that aggregate and distribute information in ICL. We first visualize the attention interactive pattern between tokens with a GPT model (Brown et al., 2020) on sentiment analysis (Figure 1). Initial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers. ${ }^{2}$ To draw a clearer picture of this phenomenon, we design two metrics based on saliency\n\n\\footnotetext{\n${ }^{2}$ In this paper, \"shallow\" or \"first\" layers refer to those closer to the input, while \"deep\" or \"last\" layers are closer to the output. Here, \"deep layers\" include those around the midpoint, e.g., layers $25-48$ in a 48-layer GPT2-XL.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We first visualize the attention interactive pattern between tokens with a GPT model (Brown et al., 2020) on sentiment analysis (Figure 1).",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02143e2e-79a0-4008-8043-feeedac826fc",
        "questions": "Does the paper 'Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning' propose any methods to improve in-context learning performance?",
        "answers": "Yes",
        "context": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning\n\n\n\nLean Wang ${ ^{\\dagger, \\delta}$, Lei $\\mathrm{Li}^{\\dagger}$, Damai Dai ${ }^{\\dagger}$, Deli Chen ${ }^{\\S}$, Hao Zhou ${ }^{\\S}$, Fandong Meng ${ }^{\\S}$, Jie Zhou ${ }^{\\S}$, Xu Sun ${ }^{\\dagger}$ \\\\ ${ }^{\\dagger}$ National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University \\\\ \u00a7Pattern Recognition Center, WeChat AI, Tencent Inc., China \\\\ \\{lean, daidamai,xusun\\}@pku.edu.cn nlp.lilei@gmail.com \\\\ victorchen@deepseek.com \\{tuxzhou,fandongmeng, withtomzhou\\}@tencent.com\n}\n\n\nIn-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies. ${ }^{1}$\n\n\n1 Introduction\n\nIn-context Learning (ICL) has emerged as a powerful capability alongside the development of scaledup large language models (LLMs) (Brown et al., 2020). By instructing LLMs using few-shot demonstration examples, ICL enables them to perform a wide range of tasks, such as text classification (Min et al., 2022a) and mathematical reasoning (Wei et al., 2022). Since ICL does not require updates to millions or trillions of model parameters and relies on human-understandable natural language instructions (Dong et al., 2023), it has become a promising approach for harnessing the full potentiality of LLMs. Despite its significance, the inner working mechanism of ICL remains an open question, garnering considerable interest from research\n\n\\footnotetext{\n${ }^{1}$ https://github.com/lancopku/\nlabel-words-are-anchors\n}\n\n\nFigure 1: Visualization of the information flow in a GPT model performing ICL. The line depth reflects the significance of the information flow from the right word to the left. The flows involving label words are highlighted. Label words gather information from demonstrations in shallow layers, which is then extracted in deep layers for final prediction.\ncommunities (Xie et al., 2022; Dai et al., 2022; Aky\u00fcrek et al., 2022; Li et al., 2023b).\n\nIn this paper, we find that the label words serve as anchors that aggregate and distribute information in ICL. We first visualize the attention interactive pattern between tokens with a GPT model (Brown et al., 2020) on sentiment analysis (Figure 1). Initial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers. ${ }^{2}$ To draw a clearer picture of this phenomenon, we design two metrics based on saliency\n\n\\footnotetext{\n${ }^{2}$ In this paper, \"shallow\" or \"first\" layers refer to those closer to the input, while \"deep\" or \"last\" layers are closer to the output. Here, \"deep layers\" include those around the midpoint, e.g., layers $25-48$ in a 48-layer GPT2-XL.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02160910-abe5-40ec-9fe1-59cdca54e759",
        "questions": "What metric is used to quantify the correlation between attention distributions and model prediction in the experiments discussed in the document?",
        "answers": "AUC-ROC score",
        "context": "layer as $A_{l} .{ }^{5}$ In deeper layers, we find a strong correlation between the attention distributions on the label words of the target position, represented as $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$, and the model's final prediction, affirming our hypothesis. The experimental setup mirrors that discussed in $\\S 2.2$.\n\n2.3.1 Experiments\n\nWe utilize the AUC-ROC score to quantify the correlation between $A_{l}\\left(q, p_{i}\\right)$ and model prediction, which we denote as $\\mathrm{AUCROC}_{l}$ for the $l$-th layer. We prefer the AUC-ROC metric due to two primary reasons: (1) $A_{l}\\left(q, p_{i}\\right)$ might differ from the probability of the model outputting label $i$ by a constant factor. As Kobayashi et al. (2020) points out, attention should be multiplied by the norm of the key vector to yield 'more interpretable attention'. The AUC-ROC metric can implicitly account for these factors, thus allowing us to uncover the correlation more effectively. (2) The proportion of different labels output by the model may be unbalanced. Using the AUC-ROC metric can help mitigate this issue, reducing disturbances caused by class imbalance.\n\nConsidering the residual mechanism of transformers, we can view each layer's hidden state as the cumulative effect of all prior layer calculations. To quantify the accumulated contribution of the first $l$ layers to model prediction, we introduce $R_{l}$ :\n$$R_{l}=\\frac{\\sum_{i=1}^{l}\\left(\\operatorname{AUCROC}_{i}-0.5\\right)}{\\sum_{i=1}^{N}\\left(\\mathrm{AUCROC}_{i}-0.5\\right)}$$\n\nThis measure tracks the positive contribution above a baseline AUC-ROC threshold of 0.5 . The value of $R_{l}$ signifies the proportional contribution of the first $l$ layers to the model prediction.\n\n2.3.2 Results and Analysis\n\nFigures 5a and 5b delineate correlation metrics for GPT2-XL and GPT-J, averaged across four datasets. The $\\mathrm{AUCROC}_{l}$ for deep layers approaches 0.8 , illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction. Moreover, shallow layers show negligible cumulative contributions $\\left(R_{l}\\right)$, with a significant increase in middle and deep layers. These results signify the crucial role of deep layers for final prediction, validating that the model extracts information from label words in deep layers to form the final prediction.\n\n\\footnotetext{\n${ }^{5}$ Here we sum up the attention matrices of all attention heads in the $l$ th layer for convenience of analysis.\n}\n\n\nFigure 5: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models. The result is averaged over SST-2, TREC, AGNews, and Emoc. AUCROC ${ }_{l}$ reaches 0.8 in deep layers, and $R_{l}$ increases mainly in the middle and later layers.\n\n2.4 Discussion of Our Hypothesis\n\nIn \u00a7 2.2, we have affirmed that the model's shallow layers assemble information from demonstrations via label words to form semantic representations. In $\\S 2.3$, we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the deep layers. Recognizing the crucial function of label words in this process, we have introduced the term \"Anchors\" to denote them. Given the considerable role these \"anchors\" fulfill, we find it intuitive to design ICL improvements based on them, as elaborated in $\\S 3$.\n\n3 Applications of Our Anchor-Based Understanding\n\nWith insights from the validated hypothesis, we propose strategies to boost ICL's accuracy and inference speed. We propose an anchor re-weighting method in $\\S 3.1$ to adjust the demonstrations' contributions and improve accuracy. In \u00a7 3.2, we explore a context compression technique that reduces original demonstrations to anchor hidden states to speed up ICL inference. Besides, in $\\S 3.3$, we utilize anchor distances to perform an analysis to understand",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We utilize the AUC-ROC score to quantify the correlation between $A_{l}\\left(q, p_{i}\right)$ and model prediction, which we denote as $\\mathrm{AUCROC}_{l}$ for the $l$-th layer.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "022031dc-a078-445d-b528-a651e7ce276d",
        "questions": "What is the significance of the value of $R_{l}$ in the context of the document's experiments?",
        "answers": "The value of $R_{l}$ signifies the proportional contribution of the first $l$ layers to the model prediction.",
        "context": "layer as $A_{l} .{ }^{5}$ In deeper layers, we find a strong correlation between the attention distributions on the label words of the target position, represented as $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$, and the model's final prediction, affirming our hypothesis. The experimental setup mirrors that discussed in $\\S 2.2$.\n\n2.3.1 Experiments\n\nWe utilize the AUC-ROC score to quantify the correlation between $A_{l}\\left(q, p_{i}\\right)$ and model prediction, which we denote as $\\mathrm{AUCROC}_{l}$ for the $l$-th layer. We prefer the AUC-ROC metric due to two primary reasons: (1) $A_{l}\\left(q, p_{i}\\right)$ might differ from the probability of the model outputting label $i$ by a constant factor. As Kobayashi et al. (2020) points out, attention should be multiplied by the norm of the key vector to yield 'more interpretable attention'. The AUC-ROC metric can implicitly account for these factors, thus allowing us to uncover the correlation more effectively. (2) The proportion of different labels output by the model may be unbalanced. Using the AUC-ROC metric can help mitigate this issue, reducing disturbances caused by class imbalance.\n\nConsidering the residual mechanism of transformers, we can view each layer's hidden state as the cumulative effect of all prior layer calculations. To quantify the accumulated contribution of the first $l$ layers to model prediction, we introduce $R_{l}$ :\n$$R_{l}=\\frac{\\sum_{i=1}^{l}\\left(\\operatorname{AUCROC}_{i}-0.5\\right)}{\\sum_{i=1}^{N}\\left(\\mathrm{AUCROC}_{i}-0.5\\right)}$$\n\nThis measure tracks the positive contribution above a baseline AUC-ROC threshold of 0.5 . The value of $R_{l}$ signifies the proportional contribution of the first $l$ layers to the model prediction.\n\n2.3.2 Results and Analysis\n\nFigures 5a and 5b delineate correlation metrics for GPT2-XL and GPT-J, averaged across four datasets. The $\\mathrm{AUCROC}_{l}$ for deep layers approaches 0.8 , illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction. Moreover, shallow layers show negligible cumulative contributions $\\left(R_{l}\\right)$, with a significant increase in middle and deep layers. These results signify the crucial role of deep layers for final prediction, validating that the model extracts information from label words in deep layers to form the final prediction.\n\n\\footnotetext{\n${ }^{5}$ Here we sum up the attention matrices of all attention heads in the $l$ th layer for convenience of analysis.\n}\n\n\nFigure 5: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models. The result is averaged over SST-2, TREC, AGNews, and Emoc. AUCROC ${ }_{l}$ reaches 0.8 in deep layers, and $R_{l}$ increases mainly in the middle and later layers.\n\n2.4 Discussion of Our Hypothesis\n\nIn \u00a7 2.2, we have affirmed that the model's shallow layers assemble information from demonstrations via label words to form semantic representations. In $\\S 2.3$, we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the deep layers. Recognizing the crucial function of label words in this process, we have introduced the term \"Anchors\" to denote them. Given the considerable role these \"anchors\" fulfill, we find it intuitive to design ICL improvements based on them, as elaborated in $\\S 3$.\n\n3 Applications of Our Anchor-Based Understanding\n\nWith insights from the validated hypothesis, we propose strategies to boost ICL's accuracy and inference speed. We propose an anchor re-weighting method in $\\S 3.1$ to adjust the demonstrations' contributions and improve accuracy. In \u00a7 3.2, we explore a context compression technique that reduces original demonstrations to anchor hidden states to speed up ICL inference. Besides, in $\\S 3.3$, we utilize anchor distances to perform an analysis to understand",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The value of $R_{l}$ signifies the proportional contribution of the first $l$ layers to the model prediction.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "022fe37d-c8a7-4ea0-86eb-b89d6099ee1f",
        "questions": "In the experiments with GPT2-XL and GPT-J, what is the approximate AUC-ROC score achieved by deep layers, and what does this indicate about the correlation between attention distributions and model prediction?",
        "answers": "0.8, indicating a strong correlation",
        "context": "layer as $A_{l} .{ }^{5}$ In deeper layers, we find a strong correlation between the attention distributions on the label words of the target position, represented as $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$, and the model's final prediction, affirming our hypothesis. The experimental setup mirrors that discussed in $\\S 2.2$.\n\n2.3.1 Experiments\n\nWe utilize the AUC-ROC score to quantify the correlation between $A_{l}\\left(q, p_{i}\\right)$ and model prediction, which we denote as $\\mathrm{AUCROC}_{l}$ for the $l$-th layer. We prefer the AUC-ROC metric due to two primary reasons: (1) $A_{l}\\left(q, p_{i}\\right)$ might differ from the probability of the model outputting label $i$ by a constant factor. As Kobayashi et al. (2020) points out, attention should be multiplied by the norm of the key vector to yield 'more interpretable attention'. The AUC-ROC metric can implicitly account for these factors, thus allowing us to uncover the correlation more effectively. (2) The proportion of different labels output by the model may be unbalanced. Using the AUC-ROC metric can help mitigate this issue, reducing disturbances caused by class imbalance.\n\nConsidering the residual mechanism of transformers, we can view each layer's hidden state as the cumulative effect of all prior layer calculations. To quantify the accumulated contribution of the first $l$ layers to model prediction, we introduce $R_{l}$ :\n$$R_{l}=\\frac{\\sum_{i=1}^{l}\\left(\\operatorname{AUCROC}_{i}-0.5\\right)}{\\sum_{i=1}^{N}\\left(\\mathrm{AUCROC}_{i}-0.5\\right)}$$\n\nThis measure tracks the positive contribution above a baseline AUC-ROC threshold of 0.5 . The value of $R_{l}$ signifies the proportional contribution of the first $l$ layers to the model prediction.\n\n2.3.2 Results and Analysis\n\nFigures 5a and 5b delineate correlation metrics for GPT2-XL and GPT-J, averaged across four datasets. The $\\mathrm{AUCROC}_{l}$ for deep layers approaches 0.8 , illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction. Moreover, shallow layers show negligible cumulative contributions $\\left(R_{l}\\right)$, with a significant increase in middle and deep layers. These results signify the crucial role of deep layers for final prediction, validating that the model extracts information from label words in deep layers to form the final prediction.\n\n\\footnotetext{\n${ }^{5}$ Here we sum up the attention matrices of all attention heads in the $l$ th layer for convenience of analysis.\n}\n\n\nFigure 5: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models. The result is averaged over SST-2, TREC, AGNews, and Emoc. AUCROC ${ }_{l}$ reaches 0.8 in deep layers, and $R_{l}$ increases mainly in the middle and later layers.\n\n2.4 Discussion of Our Hypothesis\n\nIn \u00a7 2.2, we have affirmed that the model's shallow layers assemble information from demonstrations via label words to form semantic representations. In $\\S 2.3$, we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the deep layers. Recognizing the crucial function of label words in this process, we have introduced the term \"Anchors\" to denote them. Given the considerable role these \"anchors\" fulfill, we find it intuitive to design ICL improvements based on them, as elaborated in $\\S 3$.\n\n3 Applications of Our Anchor-Based Understanding\n\nWith insights from the validated hypothesis, we propose strategies to boost ICL's accuracy and inference speed. We propose an anchor re-weighting method in $\\S 3.1$ to adjust the demonstrations' contributions and improve accuracy. In \u00a7 3.2, we explore a context compression technique that reduces original demonstrations to anchor hidden states to speed up ICL inference. Besides, in $\\S 3.3$, we utilize anchor distances to perform an analysis to understand",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The $\\mathrm{AUCROC}_{l}$ for deep layers approaches 0.8 , illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02300212-e7a7-45ae-bd09-91eb04d36829",
        "questions": "What is the proposed hypothesis about label words in in-context learning according to the paper?",
        "answers": "Label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow.",
        "context": "tion strategies (Ye et al., 2023; Li et al., 2023a) and calibration techniques (Zhao et al., 2021; Min et al., 2022a) could bring clear boosts to the ICL performance. The second stream investigates the inner working mechanism of ICL through different conceptual lenses, such as making an analogy of ICL to gradient descent (von Oswald et al., 2022; Dai et al., 2022) and viewing the process of ICL as a Bayesian inference (Xie et al., 2022).\n\nIn this paper, we provide a novel perspective by examining the information flow in language models to gain an understanding of ICL. Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the effectiveness, efficiency, and interpretability of ICL.\n\n5 Conclusion\n\nIn this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow. Experimental results with attention manipulation and analysis of predictions correlation consolidate the hypothesis holds well in GPT2XL and GPT-J models. Inspired by the new understanding perspective, we propose three practical applications. First, an anchor re-weighting method is proposed to improve ICL accuracy. Second, we explore a demonstration compression technique to accelerate ICL inference. Lastly, we showcase an analysis framework to diagnose ICL errors on a real-world dataset. These promising applications again verify the hypothesis and open up new directions for future investigations on ICL.\n\nLimitations\n\nOur study, while providing valuable insights into in-context learning (ICL), has several limitations. Firstly, our research scope was limited to classification tasks and did not delve into the realm of generative tasks. Additionally, our hypothesis was only examined within conventional ICL paradigms, leaving other ICL paradigms such as the chain of thought prompting (CoT) (Wei et al., 2022) unexplored. Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 billion parameters. Further research that replicates our study using larger-scale models would be beneficial in corroborating our findings and refining the hypotheses set forth in our investigation.\n\nAcknowledgement\n\nWe thank all reviewers for their thoughtful and insightful suggestions. This work is supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author.\n\nReferences\n\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. ArXiv preprint, abs/2211.15661.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. SemEval-2019 task 3: EmoContext contextual emotion detection in text. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 39-48, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. ArXiv preprint, abs/2212.10559.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023. A survey for in-context learning. ArXiv preprint, abs/2301.00234.\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight:",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02352cfc-43d3-41cd-8443-6c48cfbb5634",
        "questions": "What is one of the limitations of the study on in-context learning mentioned in the document?",
        "answers": "The research scope was limited to classification tasks and did not delve into the realm of generative tasks.",
        "context": "tion strategies (Ye et al., 2023; Li et al., 2023a) and calibration techniques (Zhao et al., 2021; Min et al., 2022a) could bring clear boosts to the ICL performance. The second stream investigates the inner working mechanism of ICL through different conceptual lenses, such as making an analogy of ICL to gradient descent (von Oswald et al., 2022; Dai et al., 2022) and viewing the process of ICL as a Bayesian inference (Xie et al., 2022).\n\nIn this paper, we provide a novel perspective by examining the information flow in language models to gain an understanding of ICL. Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the effectiveness, efficiency, and interpretability of ICL.\n\n5 Conclusion\n\nIn this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow. Experimental results with attention manipulation and analysis of predictions correlation consolidate the hypothesis holds well in GPT2XL and GPT-J models. Inspired by the new understanding perspective, we propose three practical applications. First, an anchor re-weighting method is proposed to improve ICL accuracy. Second, we explore a demonstration compression technique to accelerate ICL inference. Lastly, we showcase an analysis framework to diagnose ICL errors on a real-world dataset. These promising applications again verify the hypothesis and open up new directions for future investigations on ICL.\n\nLimitations\n\nOur study, while providing valuable insights into in-context learning (ICL), has several limitations. Firstly, our research scope was limited to classification tasks and did not delve into the realm of generative tasks. Additionally, our hypothesis was only examined within conventional ICL paradigms, leaving other ICL paradigms such as the chain of thought prompting (CoT) (Wei et al., 2022) unexplored. Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 billion parameters. Further research that replicates our study using larger-scale models would be beneficial in corroborating our findings and refining the hypotheses set forth in our investigation.\n\nAcknowledgement\n\nWe thank all reviewers for their thoughtful and insightful suggestions. This work is supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author.\n\nReferences\n\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. ArXiv preprint, abs/2211.15661.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. SemEval-2019 task 3: EmoContext contextual emotion detection in text. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 39-48, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. ArXiv preprint, abs/2212.10559.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023. A survey for in-context learning. ArXiv preprint, abs/2301.00234.\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight:",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Firstly, our research scope was limited to classification tasks and did not delve into the realm of generative tasks.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "023f3852-0af8-4933-9f99-697f70ef6d2b",
        "questions": "How many parameters were the models investigated in the study on in-context learning limited to, due to hardware constraints?",
        "answers": "6 billion",
        "context": "tion strategies (Ye et al., 2023; Li et al., 2023a) and calibration techniques (Zhao et al., 2021; Min et al., 2022a) could bring clear boosts to the ICL performance. The second stream investigates the inner working mechanism of ICL through different conceptual lenses, such as making an analogy of ICL to gradient descent (von Oswald et al., 2022; Dai et al., 2022) and viewing the process of ICL as a Bayesian inference (Xie et al., 2022).\n\nIn this paper, we provide a novel perspective by examining the information flow in language models to gain an understanding of ICL. Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the effectiveness, efficiency, and interpretability of ICL.\n\n5 Conclusion\n\nIn this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow. Experimental results with attention manipulation and analysis of predictions correlation consolidate the hypothesis holds well in GPT2XL and GPT-J models. Inspired by the new understanding perspective, we propose three practical applications. First, an anchor re-weighting method is proposed to improve ICL accuracy. Second, we explore a demonstration compression technique to accelerate ICL inference. Lastly, we showcase an analysis framework to diagnose ICL errors on a real-world dataset. These promising applications again verify the hypothesis and open up new directions for future investigations on ICL.\n\nLimitations\n\nOur study, while providing valuable insights into in-context learning (ICL), has several limitations. Firstly, our research scope was limited to classification tasks and did not delve into the realm of generative tasks. Additionally, our hypothesis was only examined within conventional ICL paradigms, leaving other ICL paradigms such as the chain of thought prompting (CoT) (Wei et al., 2022) unexplored. Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 billion parameters. Further research that replicates our study using larger-scale models would be beneficial in corroborating our findings and refining the hypotheses set forth in our investigation.\n\nAcknowledgement\n\nWe thank all reviewers for their thoughtful and insightful suggestions. This work is supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author.\n\nReferences\n\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. ArXiv preprint, abs/2211.15661.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In $A d$ vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. 2019. SemEval-2019 task 3: EmoContext contextual emotion detection in text. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 39-48, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. ArXiv preprint, abs/2212.10559.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023. A survey for in-context learning. ArXiv preprint, abs/2301.00234.\n\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, ChinYew Lin, and Deepak Ravichandran. 2001. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight:",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 billion parameters.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "024d4ab1-0f79-4c69-9380-d67510935b1e",
        "questions": "Which model shows a more pronounced acceleration effect when using the Hidden anchor method, GPT2-XL or GPT-J?",
        "answers": "GPT-J",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0250457f-c4d9-4d9e-9a97-a7d2392cb140",
        "questions": "What is the acceleration ratio of the Hidden anchor method for the GPT-J model on the AGNews dataset?",
        "answers": "2.9 times",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "\begin{tabular}{c|cccc} \\hline Model & SST-2 & TREC & AGNews & EmoC \\ \\hline GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$ \\ GPT-J & $1.5 \times$ & $2.2 \times$ & $2.9 \times$ & $1.9 \times$ \\ \\hline \\end{tabular}",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02540d49-db1f-4aad-ac19-80beac065dcf",
        "questions": "Does the proposed approximation metric, Confusion $_{i j}^{\text {pred }}$, identify the most confusing case in the TREC dataset as Description-Entity?",
        "answers": "Yes",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Figure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (Entity-Abbreviation, Description-Abbreviation).",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02567d2b-898c-47ae-ba29-667531a09fb4",
        "questions": "What is the impact of isolating label words within the first 5 layers on the model's behavior according to the document?",
        "answers": "Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.",
        "context": "Figure 3: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ in different layers on SST-2 and AGNews. Results of other datasets can be found in Appendix B. Initially, $S_{w p}$ occupies a significant proportion, but it gradually decays over layers, while $S_{p q}$ becomes the dominant one.\npart to label tokens, which is facilitated by the transformer's attention mechanism. By manipulating the attention layer in the model to block this flow and examining the model behavior change, we validate the existence of the information aggregation process and its contribution to the final prediction.\n\nExperimental Settings We retain the same test sample size of 1000 inputs as $\\S 2.1$. We use the same demonstration for a single random seed. To further validate our findings on larger models, we incorporate GPT-J (6B) (Wang and Komatsuzaki, 2021) in experiments, which exceeds GPT2-XL in model size and capacity.\n\nImplementation Details To block the information flow to label words, we isolate label words by manipulating the attention matrix $A$. Specifically, we set $A_{l}(p, i)(i<p)$ to 0 in the attention matrix $A_{l}$ of the $l$-th layer, where $p$ represents label\n\n\nFigure 4: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.\nwords and $i$ represents preceding words. Consequently, in the $l$-th layer, label words cannot access information from the prior demonstration text.\n\nMetrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens: (1) Label Loyalty: measures the consistency of output labels with and without isolation. (2) Word Loyalty: employs the Jaccard similarity to compare the top-5 predicted words with and without isolation, capturing more subtle model output alterations (See Appendix C for details). Low loyalty indicates a profound impact of isolation on model predictions.\n\nResults and Analysis Figure 4 illustrates a notable influence on the model's behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used. This observation underlines the fundamental importance of shallow-layer information aggregation via label words in ICL. It also emphasizes the superiority of label words over non-label words. Further tests with variable numbers of layers reaffirm these findings (Appendix D). Moreover, similar results were obtained when testing ICL with semantically unrelated labels (refer to Appendix F.2).\n\n2.3 Deep Layers: Information Extraction\n\nWe proceed to validate the latter part of our hypothesis that the model extracts information from label words to form the final prediction. We denote the sum of the attention matrices in the $l$-th",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Figure 4: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02581491-4b4e-427d-910a-7f56ee305bfc",
        "questions": "What metric is used to measure the consistency of output labels with and without isolation in the document?",
        "answers": "Label Loyalty",
        "context": "Figure 3: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ in different layers on SST-2 and AGNews. Results of other datasets can be found in Appendix B. Initially, $S_{w p}$ occupies a significant proportion, but it gradually decays over layers, while $S_{p q}$ becomes the dominant one.\npart to label tokens, which is facilitated by the transformer's attention mechanism. By manipulating the attention layer in the model to block this flow and examining the model behavior change, we validate the existence of the information aggregation process and its contribution to the final prediction.\n\nExperimental Settings We retain the same test sample size of 1000 inputs as $\\S 2.1$. We use the same demonstration for a single random seed. To further validate our findings on larger models, we incorporate GPT-J (6B) (Wang and Komatsuzaki, 2021) in experiments, which exceeds GPT2-XL in model size and capacity.\n\nImplementation Details To block the information flow to label words, we isolate label words by manipulating the attention matrix $A$. Specifically, we set $A_{l}(p, i)(i<p)$ to 0 in the attention matrix $A_{l}$ of the $l$-th layer, where $p$ represents label\n\n\nFigure 4: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.\nwords and $i$ represents preceding words. Consequently, in the $l$-th layer, label words cannot access information from the prior demonstration text.\n\nMetrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens: (1) Label Loyalty: measures the consistency of output labels with and without isolation. (2) Word Loyalty: employs the Jaccard similarity to compare the top-5 predicted words with and without isolation, capturing more subtle model output alterations (See Appendix C for details). Low loyalty indicates a profound impact of isolation on model predictions.\n\nResults and Analysis Figure 4 illustrates a notable influence on the model's behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used. This observation underlines the fundamental importance of shallow-layer information aggregation via label words in ICL. It also emphasizes the superiority of label words over non-label words. Further tests with variable numbers of layers reaffirm these findings (Appendix D). Moreover, similar results were obtained when testing ICL with semantically unrelated labels (refer to Appendix F.2).\n\n2.3 Deep Layers: Information Extraction\n\nWe proceed to validate the latter part of our hypothesis that the model extracts information from label words to form the final prediction. We denote the sum of the attention matrices in the $l$-th",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Metrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens: (1) Label Loyalty: measures the consistency of output labels with and without isolation.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "025b7c5f-1284-49d7-8d4e-2753ef26a5b7",
        "questions": "Does the document suggest that the influence of isolating label words becomes inconsequential within the last 5 layers?",
        "answers": "Yes",
        "context": "Figure 3: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ in different layers on SST-2 and AGNews. Results of other datasets can be found in Appendix B. Initially, $S_{w p}$ occupies a significant proportion, but it gradually decays over layers, while $S_{p q}$ becomes the dominant one.\npart to label tokens, which is facilitated by the transformer's attention mechanism. By manipulating the attention layer in the model to block this flow and examining the model behavior change, we validate the existence of the information aggregation process and its contribution to the final prediction.\n\nExperimental Settings We retain the same test sample size of 1000 inputs as $\\S 2.1$. We use the same demonstration for a single random seed. To further validate our findings on larger models, we incorporate GPT-J (6B) (Wang and Komatsuzaki, 2021) in experiments, which exceeds GPT2-XL in model size and capacity.\n\nImplementation Details To block the information flow to label words, we isolate label words by manipulating the attention matrix $A$. Specifically, we set $A_{l}(p, i)(i<p)$ to 0 in the attention matrix $A_{l}$ of the $l$-th layer, where $p$ represents label\n\n\nFigure 4: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.\nwords and $i$ represents preceding words. Consequently, in the $l$-th layer, label words cannot access information from the prior demonstration text.\n\nMetrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens: (1) Label Loyalty: measures the consistency of output labels with and without isolation. (2) Word Loyalty: employs the Jaccard similarity to compare the top-5 predicted words with and without isolation, capturing more subtle model output alterations (See Appendix C for details). Low loyalty indicates a profound impact of isolation on model predictions.\n\nResults and Analysis Figure 4 illustrates a notable influence on the model's behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used. This observation underlines the fundamental importance of shallow-layer information aggregation via label words in ICL. It also emphasizes the superiority of label words over non-label words. Further tests with variable numbers of layers reaffirm these findings (Appendix D). Moreover, similar results were obtained when testing ICL with semantically unrelated labels (refer to Appendix F.2).\n\n2.3 Deep Layers: Information Extraction\n\nWe proceed to validate the latter part of our hypothesis that the model extracts information from label words to form the final prediction. We denote the sum of the attention matrices in the $l$-th",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Results and Analysis Figure 4 illustrates a notable influence on the model's behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "025f1f28-e4d9-48f5-9aa0-fe016a66817e",
        "questions": "What is the primary model chosen for investigation in the study due to its moderate model size and decent ICL performance?",
        "answers": "GPT2-XL",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022).",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0262a5aa-edec-41ef-9762-762620379089",
        "questions": "How many examples are sampled from the test set for evaluation in the experiments conducted with GPT2-XL?",
        "answers": "1000",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "026e7549-fd37-429e-86be-49572e1a901c",
        "questions": "Does the study propose that label words function as anchors in the ICL information flow?",
        "answers": "Yes",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "027752fe-61f6-4c6e-a89c-0559bf4fa4a3",
        "questions": "What is the accuracy achieved by LLaMA-33B on SST-2 when using semantically-unrelated labels with eight shots per class?",
        "answers": "83.0%",
        "context": "\\begin{tabular}{ccc}\n  Isolation Layer & Output Label & $V_{5}$ (sorted by probability) \\\\\n  First 5 layers & World & \"In\",\" The\",\" Google\",\"<lendoftextl>\",\" A\" \\\\\nNo isolation & World & \" World\",\" Technology\",\" Politics\",\" Israel\",\" Human\" \\\\\n \n\\end{tabular}\n\nTable 5: Results on a test sample with the label \"World\" from AGNews.\n\n\nFigure 8: The chart demonstrates variations in label loyalty and word loyalty, dependent on whether label or non-label words are isolated in various layers. 'First' refers to the first several layers, while 'Last' to the last ones. Deep-colored lines represent label word isolation, whereas light colors denote non-label words. Remarkably, isolating label words in the shallow layers significantly influences the outcome, regardless of whether this is compared to isolation in deep layers or to nonlabel word isolation.\nthe important role of information aggregation via label words in the shallow layers.\n\nE Details for the Calculation of AUCROC $_{l$}\n\nSuppose the positions of the label words in the input $x$ are $p_{1}, \\ldots, p_{C}$ (without loss of generality, we suppose $p_{i}$ corresponds to the $i$ th class), the targeted position is $q$, the sum of the attention ma-\ntrices of all attention heads at the layer is $A_{l}$. We postulate that there's a strong correlation between the attention distributions on the label words of the target position $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ and the model's final prediction. We use the AUCROC score to quantify this correlation. We re$\\operatorname{gard}\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ as a classifier's prediction for the model output label (that is, $A_{l}\\left(q, p_{i}\\right)$ is equivalent to the probability of model outputting label $i$ ), and compute the AUC-ROC value of this prediction relative to the actual model output. We denote this as $\\mathrm{AUCROC}_{l}$. For the case with more demonstrations (Appendix F.1), we simply sum up all $A_{l}(q, p)$ of the same class.\n\nF Additional Experimental Results\n\nF. 1 Results with More Demonstrations\n\nWe implement our experimental analysis utilizing two demonstrations per class, resulting in a total of $4,12,8$, and 8 demonstrations respectively for SST-2, TREC, AGNews, and EmoC. Our findings, as depicted in Figure 9, Figure 10, and Figure 11, exhibit a high degree of similarity to the results obtained from experiments that employ one demonstration per class.\n\nF. 2 Results for In-Context Learning with semantically-unrelated labels\n\nThe applicability of our analytical conclusions to ICL variants, such as the semantically unrelated label ICL (Wei et al., 2023), is an intriguing subject. Given that both GPT2-XL and GPT-J-6B perform at levels akin to random guessing in this ICL setting, we chose LLaMA-33B (Touvron et al., 2023) and SST-2 for our experiment. We substituted labels with 'A'/'B', and adhered to a similar experimental setup as in sections $\\S 2.2$ and $\\S 2.3$. However, we applied eight shots per class to facilitate the model in achieving an accuracy of $83.0 \\%$ on SST-2. The outcomes align with those derived in $\\S 2.2$ and $\\S 2.3$. Figure 12 shows the more pronounced impact of isolating labels in the shallow layers compared to their isolation in the deep layers or the isolation of non-label tokens. Figure 13 con-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "However, we applied eight shots per class to facilitate the model in achieving an accuracy of $83.0 \\%$ on SST-2.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "027c013f-3801-4399-ae6a-8e2f9c19ed34",
        "questions": "In the context of AGNews, what are the first five words sorted by probability when no isolation is applied?",
        "answers": "\" World\", \" Technology\", \" Politics\", \" Israel\", \" Human\"",
        "context": "\\begin{tabular}{ccc}\n  Isolation Layer & Output Label & $V_{5}$ (sorted by probability) \\\\\n  First 5 layers & World & \"In\",\" The\",\" Google\",\"<lendoftextl>\",\" A\" \\\\\nNo isolation & World & \" World\",\" Technology\",\" Politics\",\" Israel\",\" Human\" \\\\\n \n\\end{tabular}\n\nTable 5: Results on a test sample with the label \"World\" from AGNews.\n\n\nFigure 8: The chart demonstrates variations in label loyalty and word loyalty, dependent on whether label or non-label words are isolated in various layers. 'First' refers to the first several layers, while 'Last' to the last ones. Deep-colored lines represent label word isolation, whereas light colors denote non-label words. Remarkably, isolating label words in the shallow layers significantly influences the outcome, regardless of whether this is compared to isolation in deep layers or to nonlabel word isolation.\nthe important role of information aggregation via label words in the shallow layers.\n\nE Details for the Calculation of AUCROC $_{l$}\n\nSuppose the positions of the label words in the input $x$ are $p_{1}, \\ldots, p_{C}$ (without loss of generality, we suppose $p_{i}$ corresponds to the $i$ th class), the targeted position is $q$, the sum of the attention ma-\ntrices of all attention heads at the layer is $A_{l}$. We postulate that there's a strong correlation between the attention distributions on the label words of the target position $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ and the model's final prediction. We use the AUCROC score to quantify this correlation. We re$\\operatorname{gard}\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ as a classifier's prediction for the model output label (that is, $A_{l}\\left(q, p_{i}\\right)$ is equivalent to the probability of model outputting label $i$ ), and compute the AUC-ROC value of this prediction relative to the actual model output. We denote this as $\\mathrm{AUCROC}_{l}$. For the case with more demonstrations (Appendix F.1), we simply sum up all $A_{l}(q, p)$ of the same class.\n\nF Additional Experimental Results\n\nF. 1 Results with More Demonstrations\n\nWe implement our experimental analysis utilizing two demonstrations per class, resulting in a total of $4,12,8$, and 8 demonstrations respectively for SST-2, TREC, AGNews, and EmoC. Our findings, as depicted in Figure 9, Figure 10, and Figure 11, exhibit a high degree of similarity to the results obtained from experiments that employ one demonstration per class.\n\nF. 2 Results for In-Context Learning with semantically-unrelated labels\n\nThe applicability of our analytical conclusions to ICL variants, such as the semantically unrelated label ICL (Wei et al., 2023), is an intriguing subject. Given that both GPT2-XL and GPT-J-6B perform at levels akin to random guessing in this ICL setting, we chose LLaMA-33B (Touvron et al., 2023) and SST-2 for our experiment. We substituted labels with 'A'/'B', and adhered to a similar experimental setup as in sections $\\S 2.2$ and $\\S 2.3$. However, we applied eight shots per class to facilitate the model in achieving an accuracy of $83.0 \\%$ on SST-2. The outcomes align with those derived in $\\S 2.2$ and $\\S 2.3$. Figure 12 shows the more pronounced impact of isolating labels in the shallow layers compared to their isolation in the deep layers or the isolation of non-label tokens. Figure 13 con-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "No isolation & World & \" World\",\" Technology\",\" Politics\",\" Israel\",\" Human\"",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "027dc8dd-d8bb-40a9-9fcf-f5af80fc9ce2",
        "questions": "What is the correlation between attention distributions on label words and the model's final prediction quantified by in the document?",
        "answers": "AUCROC score",
        "context": "\\begin{tabular}{ccc}\n  Isolation Layer & Output Label & $V_{5}$ (sorted by probability) \\\\\n  First 5 layers & World & \"In\",\" The\",\" Google\",\"<lendoftextl>\",\" A\" \\\\\nNo isolation & World & \" World\",\" Technology\",\" Politics\",\" Israel\",\" Human\" \\\\\n \n\\end{tabular}\n\nTable 5: Results on a test sample with the label \"World\" from AGNews.\n\n\nFigure 8: The chart demonstrates variations in label loyalty and word loyalty, dependent on whether label or non-label words are isolated in various layers. 'First' refers to the first several layers, while 'Last' to the last ones. Deep-colored lines represent label word isolation, whereas light colors denote non-label words. Remarkably, isolating label words in the shallow layers significantly influences the outcome, regardless of whether this is compared to isolation in deep layers or to nonlabel word isolation.\nthe important role of information aggregation via label words in the shallow layers.\n\nE Details for the Calculation of AUCROC $_{l$}\n\nSuppose the positions of the label words in the input $x$ are $p_{1}, \\ldots, p_{C}$ (without loss of generality, we suppose $p_{i}$ corresponds to the $i$ th class), the targeted position is $q$, the sum of the attention ma-\ntrices of all attention heads at the layer is $A_{l}$. We postulate that there's a strong correlation between the attention distributions on the label words of the target position $\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ and the model's final prediction. We use the AUCROC score to quantify this correlation. We re$\\operatorname{gard}\\left(A_{l}\\left(q, p_{1}\\right), \\ldots, A_{l}\\left(q, p_{C}\\right)\\right)$ as a classifier's prediction for the model output label (that is, $A_{l}\\left(q, p_{i}\\right)$ is equivalent to the probability of model outputting label $i$ ), and compute the AUC-ROC value of this prediction relative to the actual model output. We denote this as $\\mathrm{AUCROC}_{l}$. For the case with more demonstrations (Appendix F.1), we simply sum up all $A_{l}(q, p)$ of the same class.\n\nF Additional Experimental Results\n\nF. 1 Results with More Demonstrations\n\nWe implement our experimental analysis utilizing two demonstrations per class, resulting in a total of $4,12,8$, and 8 demonstrations respectively for SST-2, TREC, AGNews, and EmoC. Our findings, as depicted in Figure 9, Figure 10, and Figure 11, exhibit a high degree of similarity to the results obtained from experiments that employ one demonstration per class.\n\nF. 2 Results for In-Context Learning with semantically-unrelated labels\n\nThe applicability of our analytical conclusions to ICL variants, such as the semantically unrelated label ICL (Wei et al., 2023), is an intriguing subject. Given that both GPT2-XL and GPT-J-6B perform at levels akin to random guessing in this ICL setting, we chose LLaMA-33B (Touvron et al., 2023) and SST-2 for our experiment. We substituted labels with 'A'/'B', and adhered to a similar experimental setup as in sections $\\S 2.2$ and $\\S 2.3$. However, we applied eight shots per class to facilitate the model in achieving an accuracy of $83.0 \\%$ on SST-2. The outcomes align with those derived in $\\S 2.2$ and $\\S 2.3$. Figure 12 shows the more pronounced impact of isolating labels in the shallow layers compared to their isolation in the deep layers or the isolation of non-label tokens. Figure 13 con-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We use the AUCROC score to quantify this correlation.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "028990e6-2dd2-48eb-bd57-7ceaa623c278",
        "questions": "What is the name of the approach proposed to improve ICL's accuracy by reweighting label anchors?",
        "answers": "Anchor Re-weighting",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Based on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "028b4601-c928-4b58-8b24-de88a308406d",
        "questions": "Which datasets showed significant performance enhancement with the proposed anchor reweighting method?",
        "answers": "SST-2 and EmoC",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "As Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "029f8bac-07ee-45ad-a63f-da295877696a",
        "questions": "How many extra samples per class are chosen to form the auxiliary training set for the Anchor Re-weighting experiments?",
        "answers": "Four",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02a8c33b-0af1-44ed-8a44-36bc8224a1c8",
        "questions": "Which conference did the paper by Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer on noisy channel language model prompting for few-shot text classification appear in?",
        "answers": "The 60th Annual Meeting of the Association for Computational Linguistics",
        "context": "Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057-7075, Online. Association for Computational Linguistics.\n\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a. Unified demonstration retriever for incontext learning. ArXiv preprint, abs/2305.04320.\n\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\n\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b. Transformers as algorithms: Generalization and stability in in-context learning.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.\n\nPaul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In $A d$ vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 14014-14024.\n\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316-5330, Dublin, Ireland. Association for Computational Linguistics.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.\n\nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2022. Transformers learn in-context by gradient descent. ArXiv preprint, abs/2212.07677.\n\nBen Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903.\n\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. $A r X i v$, abs/2303.03846.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\n\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. ArXiv preprint, abs/2302.05698.\n\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 24222437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649-657.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316-5330, Dublin, Ireland. Association for Computational Linguistics.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02aa4b88-4451-41f9-a853-40f381abda59",
        "questions": "What is the title of the paper authored by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample in 2023?",
        "answers": "Llama: Open and efficient foundation language models",
        "context": "Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057-7075, Online. Association for Computational Linguistics.\n\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a. Unified demonstration retriever for incontext learning. ArXiv preprint, abs/2305.04320.\n\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\n\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b. Transformers as algorithms: Generalization and stability in in-context learning.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.\n\nPaul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In $A d$ vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 14014-14024.\n\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316-5330, Dublin, Ireland. Association for Computational Linguistics.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.\n\nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2022. Transformers learn in-context by gradient descent. ArXiv preprint, abs/2212.07677.\n\nBen Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903.\n\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. $A r X i v$, abs/2303.03846.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\n\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. ArXiv preprint, abs/2302.05698.\n\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 24222437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649-657.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02ae7872-9c3e-4bc8-b715-cf06aaae38c4",
        "questions": "Did the paper by Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma on in-context learning as implicit Bayesian inference appear in the International Conference on Learning Representations in 2022?",
        "answers": "Yes",
        "context": "Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057-7075, Online. Association for Computational Linguistics.\n\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a. Unified demonstration retriever for incontext learning. ArXiv preprint, abs/2305.04320.\n\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\n\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b. Transformers as algorithms: Generalization and stability in in-context learning.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.\n\nPaul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In $A d$ vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 14014-14024.\n\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022a. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316-5330, Dublin, Ireland. Association for Computational Linguistics.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.\n\nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2022. Transformers learn in-context by gradient descent. ArXiv preprint, abs/2212.07677.\n\nBen Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903.\n\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. $A r X i v$, abs/2303.03846.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\n\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. 2023. Compositional exemplars for in-context learning. ArXiv preprint, abs/2302.05698.\n\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-truth labels matter: A deeper look into input-label demonstrations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 24222437, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649-657.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02bc47b9-753d-447a-ad5a-c28cf8c6c01f",
        "questions": "What are the label words used for the SST-2 task in the demonstration templates?",
        "answers": "Positive, Negative",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "SST-2 & \\begin{tabular}{l} Review: <S1> \\\\ Sentiment: <L> \\\\ Review: $<$ S> \\\\ Sentiment: \\end{tabular} & Positive, Negative",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02c336a0-64b3-491b-9929-12440dc513b6",
        "questions": "Which models were used in the paper 'Calibrate before use: Improving few-shot performance of language models'?",
        "answers": "GPT2-XL (1.5B) and GPT-J (6B)",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02c3a5fd-79f5-478f-816c-03df3e333680",
        "questions": "Does isolating shallow layers have a significant impact on the model according to the study on the impact of the numbers of isolated layers?",
        "answers": "Yes",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02cbc304-90d0-4d0f-8fe2-45b6cc4e0460",
        "questions": "What is the average accuracy boost achieved by the anchor re-weighting method compared to standard ICL baselines?",
        "answers": "16.7%",
        "context": "Figure 2: Illustration of our hypothesis. In shallow layers, label words gather information from demonstrations to form semantic representations for deeper processing, while deep layers extract and utilize this information from label words to formulate the final prediction.\nscores to portray the information flow in ICL and further propose the following hypothesis:\n\nInformation Flow with Labels as Anchors\n$\\mathcal{H}_{1}$ : In shallow layers, label words gather the information of demonstrations to form semantic representations for deeper layers.\n$\\mathcal{H}_{2}$ : In deep layers, the model extracts the information from label words to form the final prediction.\n\nTwo experiments are designed to validate the hypothesis using GPT2-XL (Radford et al., 2019) and GPT-J (Wang and Komatsuzaki, 2021) across several text classification benchmarks. (1) By blocking the information aggregation path to label words in certain layers, we find that such isolation in shallow layers significantly impairs model performance. This indicates that label words collect useful information during forward propagation in shallow layers. (2) We investigate the relationship between the attention distributions on the label words of the target position and the model's final prediction. Our results illustrate a strong positive correlation, where a candidate label's probability increases with more attention weight on its corresponding label token. In summary, these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets.\n\nDrawing on insights from the information flow perspective, we explore three approaches to enhance ICL's effectiveness, efficiency, and interpretability. (1) An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a $16.7 \\%$ average accuracy boost compared to standard ICL baselines. (2) For quicker ICL inference, inputs are compressed\ninto pre-calculated anchor representations since model predictions primarily rely on label word activations. Testing shows a $1.8 \\times$ speedup in inference with only a minimal performance trade-off. (3) An error analysis of ICL on GPT2-XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors, implying that errors might result from similar anchor representations. These promising applications further validate our hypothesis and shed light on future ICL studies for better transparency of LLMs.\n\n2 Label Words are Anchors\n\nThis section confirms the intuitive findings using two saliency score-based metrics as discussed in \u00a7 2.1. The quantitative results lead to a proposed hypothesis for the ICL working mechanism: $\\mathcal{H}_{1}:$ In shallow layers, label words aggregate information from demonstration examples to form semantic representations for later computations. $\\mathcal{H}_{2}$ : In deep layers, the model makes predictions by extracting information from label words. The validation for these hypotheses is presented in $\\S 2.2$ and $\\S 2.3$, respectively.\n\n2.1 Hypothesis Motivated by Saliency Scores\n\nThis section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model. The saliency technique (Simonyan et al., 2013), a common interpretation tool, is employed for highlighting critical token interactions. Following common practice, we use the Taylor expansion (Michel et al., 2019) to calculate the saliency score for each element of the attention matrix:\n$$I_{l}=\\left|\\sum_{h} A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a $16.7 \\%$ average accuracy boost compared to standard ICL baselines.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02cce923-4f11-4e20-af45-d25c4468f3b1",
        "questions": "Which models were used to validate the hypothesis regarding information flow with labels as anchors in the document?",
        "answers": "GPT2-XL and GPT-J",
        "context": "Figure 2: Illustration of our hypothesis. In shallow layers, label words gather information from demonstrations to form semantic representations for deeper processing, while deep layers extract and utilize this information from label words to formulate the final prediction.\nscores to portray the information flow in ICL and further propose the following hypothesis:\n\nInformation Flow with Labels as Anchors\n$\\mathcal{H}_{1}$ : In shallow layers, label words gather the information of demonstrations to form semantic representations for deeper layers.\n$\\mathcal{H}_{2}$ : In deep layers, the model extracts the information from label words to form the final prediction.\n\nTwo experiments are designed to validate the hypothesis using GPT2-XL (Radford et al., 2019) and GPT-J (Wang and Komatsuzaki, 2021) across several text classification benchmarks. (1) By blocking the information aggregation path to label words in certain layers, we find that such isolation in shallow layers significantly impairs model performance. This indicates that label words collect useful information during forward propagation in shallow layers. (2) We investigate the relationship between the attention distributions on the label words of the target position and the model's final prediction. Our results illustrate a strong positive correlation, where a candidate label's probability increases with more attention weight on its corresponding label token. In summary, these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets.\n\nDrawing on insights from the information flow perspective, we explore three approaches to enhance ICL's effectiveness, efficiency, and interpretability. (1) An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a $16.7 \\%$ average accuracy boost compared to standard ICL baselines. (2) For quicker ICL inference, inputs are compressed\ninto pre-calculated anchor representations since model predictions primarily rely on label word activations. Testing shows a $1.8 \\times$ speedup in inference with only a minimal performance trade-off. (3) An error analysis of ICL on GPT2-XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors, implying that errors might result from similar anchor representations. These promising applications further validate our hypothesis and shed light on future ICL studies for better transparency of LLMs.\n\n2 Label Words are Anchors\n\nThis section confirms the intuitive findings using two saliency score-based metrics as discussed in \u00a7 2.1. The quantitative results lead to a proposed hypothesis for the ICL working mechanism: $\\mathcal{H}_{1}:$ In shallow layers, label words aggregate information from demonstration examples to form semantic representations for later computations. $\\mathcal{H}_{2}$ : In deep layers, the model makes predictions by extracting information from label words. The validation for these hypotheses is presented in $\\S 2.2$ and $\\S 2.3$, respectively.\n\n2.1 Hypothesis Motivated by Saliency Scores\n\nThis section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model. The saliency technique (Simonyan et al., 2013), a common interpretation tool, is employed for highlighting critical token interactions. Following common practice, we use the Taylor expansion (Michel et al., 2019) to calculate the saliency score for each element of the attention matrix:\n$$I_{l}=\\left|\\sum_{h} A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Two experiments are designed to validate the hypothesis using GPT2-XL (Radford et al., 2019) and GPT-J (Wang and Komatsuzaki, 2021) across several text classification benchmarks.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02ce965d-352c-48a4-82b2-a2ae2b950753",
        "questions": "What is the formula used to calculate the saliency score for each element of the attention matrix in the document?",
        "answers": "$I_{l}=\\left|\\sum_{h} A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$",
        "context": "Figure 2: Illustration of our hypothesis. In shallow layers, label words gather information from demonstrations to form semantic representations for deeper processing, while deep layers extract and utilize this information from label words to formulate the final prediction.\nscores to portray the information flow in ICL and further propose the following hypothesis:\n\nInformation Flow with Labels as Anchors\n$\\mathcal{H}_{1}$ : In shallow layers, label words gather the information of demonstrations to form semantic representations for deeper layers.\n$\\mathcal{H}_{2}$ : In deep layers, the model extracts the information from label words to form the final prediction.\n\nTwo experiments are designed to validate the hypothesis using GPT2-XL (Radford et al., 2019) and GPT-J (Wang and Komatsuzaki, 2021) across several text classification benchmarks. (1) By blocking the information aggregation path to label words in certain layers, we find that such isolation in shallow layers significantly impairs model performance. This indicates that label words collect useful information during forward propagation in shallow layers. (2) We investigate the relationship between the attention distributions on the label words of the target position and the model's final prediction. Our results illustrate a strong positive correlation, where a candidate label's probability increases with more attention weight on its corresponding label token. In summary, these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets.\n\nDrawing on insights from the information flow perspective, we explore three approaches to enhance ICL's effectiveness, efficiency, and interpretability. (1) An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a $16.7 \\%$ average accuracy boost compared to standard ICL baselines. (2) For quicker ICL inference, inputs are compressed\ninto pre-calculated anchor representations since model predictions primarily rely on label word activations. Testing shows a $1.8 \\times$ speedup in inference with only a minimal performance trade-off. (3) An error analysis of ICL on GPT2-XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors, implying that errors might result from similar anchor representations. These promising applications further validate our hypothesis and shed light on future ICL studies for better transparency of LLMs.\n\n2 Label Words are Anchors\n\nThis section confirms the intuitive findings using two saliency score-based metrics as discussed in \u00a7 2.1. The quantitative results lead to a proposed hypothesis for the ICL working mechanism: $\\mathcal{H}_{1}:$ In shallow layers, label words aggregate information from demonstration examples to form semantic representations for later computations. $\\mathcal{H}_{2}$ : In deep layers, the model makes predictions by extracting information from label words. The validation for these hypotheses is presented in $\\S 2.2$ and $\\S 2.3$, respectively.\n\n2.1 Hypothesis Motivated by Saliency Scores\n\nThis section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model. The saliency technique (Simonyan et al., 2013), a common interpretation tool, is employed for highlighting critical token interactions. Following common practice, we use the Taylor expansion (Michel et al., 2019) to calculate the saliency score for each element of the attention matrix:\n$$I_{l}=\\left|\\sum_{h} A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Following common practice, we use the Taylor expansion (Michel et al., 2019) to calculate the saliency score for each element of the attention matrix: $$I_{l}=\\left|\\sum_{h} A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$$",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02e1030c-791d-469f-913e-8c9792a75c02",
        "questions": "Which model was chosen as the primary model for investigating based on its moderate model size and decent ICL performance?",
        "answers": "GPT2-XL",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022).",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "02f6ef8e-f8a8-470e-80fc-4c999150b640",
        "questions": "For what purposes are the datasets, including Stanford Sentiment Treebank Binary (SST-2), TREC, AGNews, and EmoContext, used in the provided document?",
        "answers": "Sentiment analysis, question type classification, topic classification, emotion classification.",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification.",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0305ba49-8a8c-450c-8290-f61f1616b10d",
        "questions": "What quantitative metric helps assess the mean significance of information flow from the text part to label words, and how is it defined?",
        "answers": "S_{w p}, defined as S_{w p} = \\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|}; C_{w p} = \\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Verbatim Definition",
        "evidence_source": "table",
        "evidence_context": "$S_{w p}$, the mean significance of information flow from the text part to label words: $$\\begin{aligned} S_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\ C_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\} \\end{aligned}$$",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0330dbc8-ea7a-4b57-beb4-0c32ade8c244",
        "questions": "What is the acceleration ratio of the GPT2-XL model on the AGNews dataset using the Hidden Anchor method?",
        "answers": "2.5 \u00d7",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0339d23b-763b-49ea-b9be-64e1b08d0f95",
        "questions": "Compare the acceleration ratios of the GPT2-XL and GPT-J models on the EmoC dataset using the Hidden Anchor method. Which model shows a greater acceleration?",
        "answers": "GPT-J with 1.9 \u00d7 shows greater acceleration than GPT2-XL with 1.4 \u00d7.",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$ \\ GPT-J & $1.5 \times$ & $2.2 \times$ & $2.9 \times$ & $1.9 \times$",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "033f692f-ad9d-41e7-8685-e760530d9dac",
        "questions": "What is the total acceleration ratio difference between GPT-J and GPT2-XL across all datasets (SST-2, TREC, AGNews, EmoC) using the Hidden Anchor method?",
        "answers": "2.8 \u00d7",
        "context": "\\begin{tabular}{c|cccc}\n  Model & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n \n\\end{tabular}\n\nTable 3: Acceleration ratios of the Hidden ${ }_{\\text {anchor }}$ method.\na more elaborated analysis of the speed-up ratios. Besides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL\uff0c demonstrating its great potential to apply to larger language models.\n\n3.3 Anchor Distances for Error Diagnosis\n\nLastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.\n\n3.3.1 Method\n\nOur previous analysis in $\\S 2.3$ shows a strong correlation between the model output and $A\\left(q, p_{i}\\right)$, which is determined by $\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T}$ as per Eq. 7. Should the key vectors $\\mathbf{k}$ for label words $p_{i}$ and $p_{k}$ be similar, $A\\left(q, p_{i}\\right)$ and $A\\left(q, p_{k}\\right)$ will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors $\\mathbf{q}_{q}$, we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in $\\mathbf{q}_{q}$, denoted as $\\hat{\\mathbf{k}}$ (see Appendix J for details). We anticipate that the distances between these ks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of $0-1$, with 0 indicating the highest degree of category confusion:\n$$\\text { Confusion }_{i j}^{\\text {pred }}=\\frac{\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{i}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{j}}}}\\right\\|}{\\max _{s \\neq t}\\left\\|\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{s}}}}-\\hat{\\mathbf{k}_{\\mathbf{p}_{\\mathbf{t}}}}\\right\\|}$$\n\n3.3.2 Experiments\n\nWe utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\n\nWe calculate the actual model confusion score, Confusion ${ }_{i j}$, between category $i$ and category $k$ using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion $_{i j}^{\\text {pred }}$, and the actual confusion score, Confusion $_{i j}$, via heatmaps.\n\n\nFigure 6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.\n\n3.3.3 Results\n\nFigure 6 shows that the proposed approximation metric, Confusion ${ }_{i j}^{\\text {pred }}$, can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (EntityAbbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.\n\n4 Related Work\n\nThe existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022; Wei et al., 2022), and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$ \\ GPT-J & $1.5 \times$ & $2.2 \times$ & $2.9 \times$ & $1.9 \times$",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03438da8-aaec-44a2-bb01-84c481d4e46d",
        "questions": "What is the accuracy for Vanilla In-Context Learning on the AGNews dataset when using 1-shot per class?",
        "answers": "73.32",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0344324f-982f-40a9-80d6-216a07bc5630",
        "questions": "Compare the label loyalty between Text anchor and Hidden anchor methods using the GPT-J model, and identify which method has higher label loyalty.",
        "answers": "Hidden anchor",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Text $_{\text {anchor }}$ & 53.45 & 43.85 & 40.83 \nHidden $_{\text {anchor }}$ & 89.06 & 75.04 & 55.59",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "034be832-f8c7-450d-b036-bf2cee165adb",
        "questions": "What is the difference in accuracy between the Hidden random and Hidden random-top methods when averaged over SST-2, TREC, AGNews, and EmoC?",
        "answers": "2.38",
        "context": "\\begin{tabular}{c|cccc|c}\n  Method & SST-2 & TREC & AGNews & EmoC & Average \\\\\n  Vanilla In-Context Learning ( 1-shot per class ) & 61.28 & 57.56 & 73.32 & 15.44 & 51.90 \\\\\nVanilla In-Context Learning ( 5-shot per class ) & 64.75 & 60.40 & 52.52 & 9.80 & 46.87 \\\\\nAnchor Re-weighting (1-shot per class) & $\\mathbf{9 0 . 0 7}$ & $\\mathbf{6 0 . 9 2}$ & $\\mathbf{8 1 . 9 4}$ & $\\mathbf{4 1 . 6 4}$ & $\\mathbf{6 8 . 6 4}$ \\\\\n \n\\end{tabular}\n\nTable 1: The effect after adding parameter $\\beta_{0}^{i}$. For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks.\nfrom the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states $\\boldsymbol{H}=\\left\\{\\left\\{\\boldsymbol{h}_{l}^{i}\\right\\}_{i=1}^{C}\\right\\}_{l=1}^{N}\\left(\\boldsymbol{h}_{l}^{i}\\right.$ is the $l$-th layer's hidden state of the $i$-th label word in the demonstration). By concatenating $\\boldsymbol{h}_{l}^{1}, \\ldots, \\boldsymbol{h}_{l}^{C}$ at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\n\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. ${ }^{6}$ This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, ${ }^{7}$ as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden ${ }_{\\text {anchor }}$.\n\n3.2.2 Experiments\n\nWe follow the same experimental settings as \u00a7 2.2. We compare our Hidden ${ }_{\\text {anchor }}$ input compression method with two equally efficient baselines.\nText $_{\\text {anchor }}$ : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer.\nHidden $_{\\text {random }}$ : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden ${ }_{\\text {anchor }}$ ).\nHidden $_{\\text {random-top }}$ : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden $_{\\text {random }}$ and report the one with the highest label loyalty.\n\nThe Text ${ }_{\\text {anchor }}$ method is included to demonstrate that the effectiveness of Hidden ${ }_{\\text {anchor }}$ is attributed to the aggregation of information in label\n\n\\footnotetext{\n${ }^{6}$ Omitting formatting significantly reduces accuracy, as the model will favor common tokens like \"the\" over label words, indicating confusion about the expected output type.\n${ }^{7}$ Here, \"formatting\" refers to elements like \"Review:\" and \"Sentiment:\" in Figure 2.\n}\n\\begin{tabular}{|c|c|c|c|}\n  Method & Label Loyalty & Word Loyalty & Acc. \\\\\n  ICL (GPT2-XL) & 100.00 & 100.00 & 51.90 \\\\\n  Text $_{\\text {anchor }}$ & 51.05 & 36.65 & 38.77 \\\\\n  Hidden $_{\\text {random }}$ & 48.96 & 5.59 & 39.96 \\\\\n  Hidden $_{\\text {random-top }}$ & 57.52 & 4.49 & 41.72 \\\\\n  Hidden $_{\\text {anchor }}$ & 79.47 & 62.17 & 45.04 \\\\\n  ICL (GPT-J) & 100.00 & 100.00 & 56.82 \\\\\n  Text $_{\\text {anchor }}$ & 53.45 & 43.85 & 40.83 \\\\\n  Hidden $_{\\text {random }}$ & 49.03 & 2.16 & 31.51 \\\\\n  Hidden $_{\\text {random-top }}$ & 71.10 & 11.36 & 52.34 \\\\\n  Hidden $_{\\text {anchor }}$ & 89.06 & 75.04 & 55.59 \\\\\n \n\\end{tabular}\n\nTable 2: Results of different compression methods on GPT2-XL and GPT-J (averaged over SST-2, TREC, AGNews, and EmoC). Acc. denotes accuracy. The best results are shown in bold. Our method achieves the best compression performance.\nwords, rather than the mere text of label words. If we find that Hidden ${ }_{\\text {anchor }}$ surpasses Text ${ }_{\\text {anchor }}$ in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden ${ }_{\\text {random }}$ method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states.\n\nWe assess all compression methods using the label loyalty and word loyalty introduced in $\\S 2.2$, in addition to classification accuracy.\n\n3.2.3 Results\n\nWe can see from Table 2 that the proposed compression method Hidden ${ }_{\\text {anchor }}$ achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from $1.1 \\times$ to $2.9 \\times$, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Hidden $_{\text {random }}$ & 48.96 & 5.59 & 39.96 \nHidden $_{\text {random-top }}$ & 57.52 & 4.49 & 41.72",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "035d6ede-a3c9-4e99-b131-aac79f4b6e25",
        "questions": "What is the acceleration ratio of GPT2-XL on the TREC dataset?",
        "answers": "1.5 \u00d7",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "035da456-5dbb-4a03-83c2-b7da05dcb25e",
        "questions": "Comparing GPT2-XL and GPT-J, which model shows a higher acceleration ratio on the SST-2 dataset and what are their respective acceleration ratios?",
        "answers": "GPT-J shows a higher acceleration ratio on the SST-2 dataset with an acceleration ratio of 1.5 \u00d7, while GPT2-XL has an acceleration ratio of 1.1 \u00d7.",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT2-XL & $1.1 \times$ & $1.5 \times$ & $2.5 \times$ & $1.4 \times$ \\ GPT-J & $1.5 \times$ & $2.2 \times$ & $2.9 \times$ & $1.9 \times$",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0361194e-41a4-4908-aa86-40178be6aeb4",
        "questions": "What is the correlation observed between the acceleration ratio and the ratio of the total demonstration length (L_{demo}) to the length of the text predicted (L_{x})? Give an example from the AGNews dataset to support your answer.",
        "answers": "From Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( L_{demo} ) to the length of the text predicted (L_{x}). It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio. For instance, the AGNews dataset, which has the longest L_{demo}, presents the highest acceleration ratio among the datasets analyzed.",
        "context": "Figure 11: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer in GPT models when more demonstrations are employed.\n\n\nFigure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\n\nH Training Settings of Anchor Re-weighting\n\nFor each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in $\\S 2.2$. The optimization of parame-\n\n\nFigure 13: $\\mathrm{AUCROC}_{l}$ and $R_{l}$ of each layer of LLaMA33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.\nter vector $\\boldsymbol{\\beta}$ is carried out using gradient descent, specifically with the Adam optimizer (Kingma and $\\mathrm{Ba}, 2015$ ). The learning rate is set at 0.01 , with $\\beta_{1}=0.9$ and $\\beta_{2}=0.999$. Due to memory constraints, we use a batch size of 1 . This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.\n\nI The Factor of $L_{\\text {demo }$ and $L_{\\mathrm{x}}$}\n\\begin{tabular}{c|cccc}\n  & SST-2 & TREC & AGNews & EmoC \\\\\n  GPT2-XL & $1.1 \\times$ & $1.5 \\times$ & $2.5 \\times$ & $1.4 \\times$ \\\\\nGPT-J & $1.5 \\times$ & $2.2 \\times$ & $2.9 \\times$ & $1.9 \\times$ \\\\\n $L_{\\text {demo }}$ & 18 & 61 & 151 & 53 \\\\\n$L_{\\mathbf{x}}$ & 19 & 7 & 37 & 12 \\\\\n \n\\end{tabular}\n\nTable 6: Acceleration ratios, $L_{\\text {demo }}$ and $L_{\\mathbf{x}}$.\n\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\n\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden ${ }_{\\text {anchor }}$ method in contexts involving longer demonstration lengths.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Table 6: Acceleration ratios, $L_{\text {demo }}$ and $L_{\\mathbf{x}}$.\nFrom Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length ( $L_{\text {demo }}$ ) to the length of the text predicted $\\left(L_{\\mathbf{x}}\right)$. It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest $L_{\text {demo }}$, presents the highest acceleration ratio among the datasets analyzed.",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0361524c-2cc0-4859-aa65-24ab713b7e05",
        "questions": "What are the label words used for the sentiment analysis task SST-2 in the demonstration templates?",
        "answers": "Positive, Negative",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "SST-2 & \\begin{tabular}{l} Review: <S1> \\\\ Sentiment: <L> \\\\ Review: <S> \\\\ Sentiment: \\end{tabular} & Positive, Negative",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03622744-2eb5-47f3-b459-4714a263e403",
        "questions": "In the demonstration template for the AGNews task, what categories are used as label words?",
        "answers": "World, Sports Business, Technology",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "AGNews & \\begin{tabular}{l} Article: <S1> \\\\ Answer: <L> \\\\ Article: <S> \\\\ Answer: \\end{tabular} & World, Sports Business, Technology",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03691338-50f4-443c-bc19-500f2cd8c979",
        "questions": "Can you list the types of answer categories that are used as label words for the TREC task according to the demonstration templates?",
        "answers": "Abbreviation, Entity Description, Person Location, Number",
        "context": "Table 4: Demonstration templates and label words. Here $<$ S1> represents the demonstration, $<S>$ represents the input to be predicted, and $<\\mathrm{L}>$ represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.\n\\begin{tabular}{|c|c|c|}\n  Task & Template & Label Words \\\\\n  SST-2 & \\begin{tabular}{l}\nReview: <S1> \\\\\nSentiment: <L> \\\\\nReview: $<$ S> \\\\\nSentiment:\n\\end{tabular} & Positive, Negative \\\\\n  TREC & \\begin{tabular}{l}\nQuestion: <S1> \\\\\nAnswer Type: <L> \\\\\nQuestion: <S> \\\\\nAnswer Type:\n\\end{tabular} & Abbreviation, Entity Description, Person Location, Number \\\\\n  AGNews & \\begin{tabular}{l}\nArticle: <S1> \\\\\nAnswer: <L> \\\\\nArticle: <S> \\\\\nAnswer:\n\\end{tabular} & World, Sports Business, Technology \\\\\n  EmoC & \\begin{tabular}{l}\nDialogue: <S1> \\\\\nEmotion: <L> \\\\\nDialogue: <S> \\\\\nEmotion:\n\\end{tabular} & \\begin{tabular}{l}\nOthers, Happy \\\\\nSad, Angry\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR.\n\nAppendix\n\nA Experimental Settings\n\nFor models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\n\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, EmoContext (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.\n\nB Results of $S_{w p, S_{p q}$, and $S_{w w}$ on TREC and $\\operatorname{EmoC}$}\n\nFigure 7 illustrates the relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, $S_{w p}$ (the information flow from the text part to label words)\n\n\nFigure 7: Relative size of $S_{w p}, S_{p q}$, and $S_{w w}$ on TREC and EmoC, which is similar to that on SST-2 and AGNews.\nis prominent, while $S_{p q}$ (the information flow from label words to targeted positions) is less significant. However, in deeper layers, $S_{p q}$ dominates. Importantly, $S_{w p}$ and $S_{p q}$ generally exceed $S_{w w}$, indicating that interactions involving label words are predominant.\n\nC Reason for Using Word Loyalty Besides Label Loyalty\n\nLabel loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.\n\nD Isolating Different Numbers of Layers\n\nWe study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "TREC & \\begin{tabular}{l} Question: <S1> \\\\ Answer Type: <L> \\\\ Question: <S> \\\\ Answer Type: \\end{tabular} & Abbreviation, Entity Description, Person Location, Number",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "036d10dd-a39a-48b3-840f-d34e2e004989",
        "questions": "What is the expression to calculate the mean significance of information flow from label words to the target position, denoted as $S_{p q}$?",
        "answers": "$S_{p q} = \\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|}$",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$S_{p q}, the mean significance of information flow from label words to the target position: $$\\begin{aligned} S_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "037d4e96-8504-4d9b-89a2-d95447914603",
        "questions": "What set of indices is used to define $C_{w w}$ when calculating the mean significance of information flow amongst all words, denoted as $S_{w w}$?",
        "answers": "C_{w w} = \\{(i, j): j<i\\}-C_{w p}-C_{p q}",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ : $$\\begin{aligned} S_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\ C_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "0383178c-6e09-4651-b8e0-882b2079645c",
        "questions": "How is $S_{w p}$, the mean significance of information flow from the text part to label words, mathematically calculated?",
        "answers": "$S_{w p} = \\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|}$",
        "context": "Here, $A_{h, l}$ is the value of the attention matrix of the $h$-th attention head in the $l$-th layer, $x$ is the input, and $\\mathcal{L}(x)$ is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix $I_{l}$ for the $l$-th layer. ${ }^{3} \\quad I_{l}(i, j)$ represents the significance of the information flow from the $j$-th word to the $i$-th word for ICL. By observing $I_{l}$, we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\n\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on $I_{l}$. Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2 , denoted as $p_{1}, \\ldots, p_{C}$, where $C$ represents the total number of label words; ${ }^{4}$ (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as $q$; and (iii) the text part, i.e., the tokens before label words in the demonstration.\n\nThe definitions of the three quantitative metrics follow below.\n$S_{w p}$, the mean significance of information flow from the text part to label words:\n$$\\begin{aligned}\nS_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|} \\\\\nC_{w p} & =\\left\\{\\left(p_{k}, j\\right): k \\in[1, C], j<p_{k}\\right\\}\n\\end{aligned}$$\n$S_{p q}$, the mean significance of information flow from label words to the target position:\n$$\\begin{aligned}\nS_{p q} & =\\frac{\\sum_{(i, j) \\in C_{p q}} I_{l}(i, j)}{\\left|C_{p q}\\right|} \\\\\nC_{p q} & =\\left\\{\\left(q, p_{k}\\right): k \\in[1, C]\\right\\}\n\\end{aligned}$$\n$S_{w w}$, the mean significance of the information flow amongst all words, excluding influences represented by $S_{w p}$ and $S_{p q}$ :\n$$\\begin{aligned}\nS_{w w} & =\\frac{\\sum_{(i, j) \\in C_{w w}} I_{l}(i, j)}{\\left|C_{w w}\\right|} \\\\\nC_{w w} & =\\{(i, j): j<i\\}-C_{w p}-C_{p q}\n\\end{aligned}$$\n$S_{w p}, S_{p q}$, and $S_{w w}$ help assess different information flows in the model. $S_{w p}$ indicates the intensity of information aggregation onto label words. A\n\n\\footnotetext{\n${ }^{3}$ Another choice is to use $I_{l}=\\sum_{h}\\left|A_{h, l} \\odot \\frac{\\partial \\mathcal{L}(x)}{\\partial A_{h, l}}\\right|$, which raises quite similar results.\n${ }^{4}$ In this study, the term 'label words' is approximately equal to 'label tokens'. The only deviation is the 'Abbreviation' in the TREC dataset, where we use the first subword in experiments, following Zhao et al. (2021).\n}\nhigh $S_{p q}$ demonstrates a strong information extraction from label words for final decision-making. $S_{w w}$ assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by $S_{w p}$ and $S_{p q}$.\n\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5 B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002; Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F. 1 for details). Results reflect averages from five random seeds.\n\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, $S_{p q}$, the significance of the information flow from label words to targeted positions, is low, while $S_{w p}$, the information flow from the text part to label words is high; (2) in deep layers, $S_{p q}$, the importance of information flow from label words to the targeted position becomes the dominant one. Notably, $S_{p q}$ and $S_{w p}$ usually surpass $S_{w w}$, suggesting that interactions involving label words outweigh others.\n\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.\n\n2.2 Shallow Layers: Information Aggregation\n\nIn this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$S_{w p}$, the mean significance of information flow from the text part to label words: $$\\begin{aligned} S_{w p} & =\\frac{\\sum_{(i, j) \\in C_{w p}} I_{l}(i, j)}{\\left|C_{w p}\\right|}",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03889405-0e8c-416f-bf76-f7c581f61e9a",
        "questions": "What is the approximated form of the logistic regression model in the proposed approach for improving ICL accuracy?",
        "answers": "log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "This approximates a logistic regression model where: $$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03927935-8d5f-42c0-83f0-ba7756b8002a",
        "questions": "How is the attention weight adjusted in the anchor re-weighting method described in the document?",
        "answers": "The attention weight is adjusted as follows: \\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Inspired by the similarity between ICL and logistic regression, we've incorporated a learnable \\beta_{0}^{i} into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ : $$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "039d60ba-9c99-4735-95a6-52a708fce7af",
        "questions": "What expression is used to find the re-weighting vector \\boldsymbol{\\beta}^{\\star} during the training process?",
        "answers": "\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)",
        "context": "the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.\n\n3.1 Anchor Re-weighting\n\nBased on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n\n3.1.1 Method\n\u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution $\\left(A\\left(q, p_{1}\\right), \\ldots, A\\left(q, p_{C}\\right)\\right)$ on label words $p_{1}, \\ldots, p_{C}$ of the target position $q$ in deep layers. We can view the attention module as a classifier $\\boldsymbol{f}$,\n$$\\begin{aligned}\n& \\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x) \\\\\n\\approx & A\\left(q, p_{i}\\right) \\\\\n= & \\frac{\\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{p_{i}}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{N} \\exp \\left(\\mathbf{q}_{q} \\mathbf{k}_{j}^{T} / \\sqrt{d}\\right)}\n\\end{aligned}$$\n\nBy setting $\\mathbf{q}_{q} / \\sqrt{d}=\\hat{\\mathbf{x}}$ and $\\mathbf{k}_{p_{i}}-\\mathbf{k}_{p_{C}}=\\boldsymbol{\\beta}_{i}$, we deduce:\n$$\\log \\frac{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=i \\mid X=x)}{\\operatorname{Pr}_{\\boldsymbol{f}}(Y=C \\mid X=x)}=\\boldsymbol{\\beta}_{i}^{T} \\hat{\\mathbf{x}}$$\n\nThis approximates a logistic regression model where:\n$$\\log \\frac{\\operatorname{Pr}_{f}(Y=i \\mid X=x)}{\\operatorname{Pr}_{f}(Y=C \\mid X=x)}=\\beta_{0}^{i}+\\boldsymbol{\\beta}_{i}^{T} \\mathbf{x}$$\n\nIn this equation, $\\beta_{0}^{i}$ and $\\boldsymbol{\\beta}_{i}^{T}$ are parameters that can be learned, while x is the input feature.\n\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable $\\beta_{0}^{i}$ into Eq. (7), which is equivalent to adjusting the attention weights $A\\left(q, p_{i}\\right)$ :\n$$\\hat{A}\\left(q, p_{i}\\right)=\\exp \\left(\\beta_{0}^{i}\\right) A\\left(q, p_{i}\\right)$$\n\nEach $\\beta_{0}^{i}$ is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\n\nTo train the re-weighting vector $\\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}$, we utilize an auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. Here, we perform ICL with normal demonstrations and optimize $\\boldsymbol{\\beta}$ with respect to the classification loss $\\mathcal{L}$ on $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$ :\n$$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$\n\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it\nas Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.\n\n3.1.2 Experiments\n\nWe choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set $\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$. The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters $\\left\\{\\beta_{0}^{i}\\right\\}$ are trained using gradient descent. More details can be found in Appendix H.\n\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of $\\boldsymbol{\\beta}$ is included as demonstrations (5-shot per class) for a fair comparison.\n\n3.1.3 Results\n\nAs Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector $\\boldsymbol{\\beta}$ to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.\n\n3.2 Anchor-Only Context Compression\n\nWe further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.\n\n3.2.1 Method\n\nIn $\\S 2.3$, we find that the model output heavily relies on the label words, which collect information",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "To train the re-weighting vector \\boldsymbol{\\beta}=\\left\\{\\beta_{0}^{i}\\right\\}, we utilize an auxiliary training set \\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right). Here, we perform ICL with normal demonstrations and optimize \\boldsymbol{\\beta} with respect to the classification loss \\mathcal{L} on \\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right) : $$\\boldsymbol{\\beta}^{\\star}=\\arg \\min _{\\boldsymbol{\\beta}} \\mathcal{L}\\left(\\boldsymbol{X}_{\\text {train }}, \\boldsymbol{Y}_{\\text {train }}\\right)$$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03a5fab2-a04d-4ed4-be56-ed9f507de9cf",
        "questions": "What is the expression for the attention matrix $A_{l}^{h}$ before any re-weighting is applied in the anchor re-weighting implementation?",
        "answers": "A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right)",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right)$",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03a7590a-b25c-49a3-adbc-59fe47c2e637",
        "questions": "In the implementation of anchor re-weighting, what condition leads to multiplying $A_{l}^{h}(k, j)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$?",
        "answers": "if k=q, j=p_{i}",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "equation",
        "evidence_context": "$\\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\ A_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}$",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03a89879-36a9-4d74-80de-59f550f0fc2b",
        "questions": "What is the definition of the modified attention function $\\operatorname{Attention}_{l}^{h}(Q, K, V)$ after applying anchor re-weighting?",
        "answers": "\\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V",
        "context": "(a) Results on the SST-2 dataset\n\n(b) Results on the TREC dataset\n\n(c) Results on the AGNews dataset\n\n(d) Results on the EmoC dataset\n\nFigure 9: Relative sizes of $S_{w p}, S_{p q}$, and $S_{w w}$ when more demonstrations are employed.\n\n(a) Effect of different numbers of isolated layers on GPT2XL\n\n(b) Effect of different numbers of isolated layers on GPT-J\n\nFigure 10: Variations in label loyalty and word loyalty when more demonstrations are employed.\nfirmed that the model leverages information from anchors in the deeper layers to perform classification.\n\nG Implementation of Anchor Re-weighting\n\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix $A_{l}^{h}$ of the $h$ th head in the $l$ th layer, we multiply each $A_{l}^{h}\\left(q, p_{i}\\right)$ by $\\exp \\left(\\beta_{0, l h}^{i}\\right)$ before proceeding with further computations. This means that for each attention head, we introduce the following modifications:\n$$\\begin{aligned}\n& \\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V, \\\\\n& A_{l}^{h}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) \\\\\n& \\hat{A}_{l}^{h}(k, j)= \\begin{cases}\\exp \\left(\\beta_{0, l h}^{i}\\right) A_{l}^{h}(k, j), & \\text { if } k=q, j=p_{i} \\\\\nA_{l}^{h}(k, j), & \\text { otherwise }\\end{cases}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "\\operatorname{Attention}_{l}^{h}(Q, K, V)=\\hat{A}_{l}^{h} V",
        "evidence_page_no": 12,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03aa88ef-0678-43d5-9805-3e4845aff4b8",
        "questions": "How is the first direction vector $\\mathbf{v}_1$ for the centralized query vectors determined?",
        "answers": "$\\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}$",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$\\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}$",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03aca477-4254-4fad-abb7-4d12b350b84b",
        "questions": "What is the formula for calculating each feature $\\hat{k}_i$ while projecting the key vector onto direction vectors?",
        "answers": "$\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.14160v4",
        "ID": "03b5cdd7-b367-4684-ad76-fa6341761cbe",
        "questions": "What condition must each subsequent direction vector $\\mathbf{v}_i$ fulfill, apart from maximizing variance?",
        "answers": "Orthogonality with all previous directions: $\\mathbf{v}_{i}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}$",
        "context": "J Calculation of $\\hat{k$}\n\nFor the sampled sequence $x_{1}, \\ldots, x_{T}$ to be predicted, we denote the query vectors of the target positions as $\\mathbf{q}_{1}, \\ldots, \\mathbf{q}_{T}$. We then compute the matrix $\\hat{\\mathbf{Q}}=\\left(\\mathbf{q}_{1}-\\overline{\\mathbf{q}}, \\ldots, \\mathbf{q}_{T}-\\overline{\\mathbf{q}}\\right)$ by subtracting the mean vector, $\\overline{\\mathbf{q}}$, from each query vector. Subsequently, we determine the $M$ directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$, that correspond to the M largest variation directions for the centralized query vectors $\\hat{\\mathbf{q}}_{1}, \\ldots, \\hat{\\mathbf{q}}_{T}$. The $i^{t h}$ direction, $\\mathbf{v}_{i}$, is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{i-1}$. This process can be formalized as follows:\n$$\\begin{aligned}\n& \\mathbf{v}_{1}=\\underset{\\|\\mathbf{v}\\|=1}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\mathbf{v}_{2}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\} \\\\\n& \\ldots \\\\\n& \\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}\n\\end{aligned}$$\n\nWe define $\\sigma_{i}$ as the square root of the variance of the projection of $\\hat{\\mathbf{Q}}$ onto the $i^{t h}$ direction, i.e., $\\sqrt{\\operatorname{Var}\\left\\{\\mathbf{v}_{i}^{\\top} \\hat{\\mathbf{Q}}\\right\\}}$.\n\nTo derive features $\\hat{k}$ s, we project the key vector $\\mathbf{k}$ onto the directions $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}$ and scale the projections by the corresponding standard deviations $\\sigma_{1}, \\ldots, \\sigma_{M}$. Each feature, $\\hat{\\mathbf{k}}_{i}$, is thus calculated as $\\sigma_{i} \\mathbf{v}_{i}^{T} \\mathbf{k}$.\n\nWe further examine the influence of $M$ on the prediction confusion matrix, Confusion $i j^{\\text {pred }}$, as depicted in Figure 14. Given the similarity in outcomes for various $M$, we settle on a value of $M=10$ for computation of Confusion $i j^{\\text {pred }}$.\n\nK Calculation of Confusion ${ _{i j}$}\n\nTo gauge the true degree of confusion between categories $i$ and $k$ for a given model, we suggest utilizing the Confusion ${ }_{i j}$ metric:\n\nFirst, we procure all test samples $x_{t}$ bearing true labels $i$ or $k$. We then obtain the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ yielded by the model for categories $i$ and $k$, respectively, on these samples. These probabilities are normalized to a total of 1 . Essentially, we derive a classifier $f$ that delivers the probabilities $p_{i}^{t}$ and $p_{j}^{t}$ for the categories $i$ and $k$ respectively, on the test samples $x_{t}$. By calculating the Area Under\nthe Receiver Operating Characteristic Curve (AUCROC) value of this classifier $f$, we get the degree of confusion between category $i$ and $k$, termed as Confusion $_{i j}$.\n\nThe computed Confusion $i j$ is a value that never exceeds 1. The closer Confusion $i j$ approximates 1, the less pronounced the confusion, and vice versa.\n\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ${ }_{i j}$, we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.\n\nL Reproducibility\n\nIn the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of $42,43,44,45$, and 46 . We invite readers to delve into the code for additional implementation details that may arouse their interest.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$\\mathbf{v}_{M}=\\underset{\\|\\mathbf{v}\\|=1, \\mathbf{v} \\perp \\mathbf{v}_{1}, \\ldots, \\mathbf{v} \\perp \\mathbf{v}_{M-1}}{\\arg \\max } \\operatorname{Var}\\left\\{\\mathbf{v}^{\\top} \\hat{\\mathbf{Q}}\\right\\}$",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03bfd4bd-07bc-466a-b996-cda3dbc960f6",
        "questions": "Which organization supported the research mentioned in the document with the grant number NSF OAC 2112606?",
        "answers": "NSF",
        "context": "$F$ that shows most errors made by our system are due to NER and OpenIE, which could benefit from direct fine-tuning. Given that the rest of the errors are graph search errors, also in Appendix F, we note that several avenues for improvements over simple PPR exist, such as allowing relations to guide graph traversal directly. Finally, and perhaps most importantly, HippoRAG's scalability still calls for further validation. Although we show that Llama-3 could obtain similar performance to closed-source models and thus reduce costs considerably, we are yet to empirically prove the efficiency and efficacy of our synthetic hippocampal index as its size grows way beyond current benchmarks.\n\n\nAcknowledgments\n\n\nThe authors would like to thank colleagues from the OSU NLP group and Percy Liang for their thoughtful comments. This research was supported in part by NSF OAC 2112606, NIH R01LM014199, ARL W911NF2220144, and Cisco. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\n\nReferences\n[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.\n[2] B. AlKhamissi, M. Li, A. Celikyilmaz, M. T. Diab, and M. Ghazvininejad. A review on language models as knowledge bases. ArXiv, abs/2204.06031, 2022. URL https://arxiv. org/abs/2204.06031.\n[3] G. Angeli, M. J. Johnson Premkumar, and C. D. Manning. Leveraging linguistic structure for open domain information extraction. In C. Zong and M. Strube, editors, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1034. URL https://aclanthology.org/P15-1034.\n[4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07, page 2670-2676, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.\n[5] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In A. Korhonen, D. Traum, and L. M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1470. URL https://aclanthology.org/ P19-1470.\n[6] B. Chen and A. L. Bertozzi. AutoKG: Efficient Automated Knowledge Graph Generation for Language Models. In 2023 IEEE International Conference on Big Data (BigData), pages 3117-3126, Los Alamitos, CA, USA, dec 2023. IEEE Computer Society. doi: 10.1109/BigData59044.2023.10386454. URL https://doi.ieeecomputersociety.org/ 10.1109/BigData59044.2023.10386454.\n[7] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. CoRR, abs/2310.05029, 2023. doi: 10. 48550/ARXIV.2310.05029. URL https://doi.org/10.48550/arXiv. 2310.05029.\n[8] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, H. Zhang, and D. Yu. Dense x retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023. URL https://arxiv.org/abs/2312.06648.\n[9] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "This research was supported in part by NSF OAC 2112606, NIH R01LM014199, ARL W911NF2220144, and Cisco.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03c54c71-fca0-4536-a169-df82b9bdee2d",
        "questions": "What is the main issue with HippoRAG's scalability according to the document?",
        "answers": "HippoRAG's scalability still calls for further validation.",
        "context": "$F$ that shows most errors made by our system are due to NER and OpenIE, which could benefit from direct fine-tuning. Given that the rest of the errors are graph search errors, also in Appendix F, we note that several avenues for improvements over simple PPR exist, such as allowing relations to guide graph traversal directly. Finally, and perhaps most importantly, HippoRAG's scalability still calls for further validation. Although we show that Llama-3 could obtain similar performance to closed-source models and thus reduce costs considerably, we are yet to empirically prove the efficiency and efficacy of our synthetic hippocampal index as its size grows way beyond current benchmarks.\n\n\nAcknowledgments\n\n\nThe authors would like to thank colleagues from the OSU NLP group and Percy Liang for their thoughtful comments. This research was supported in part by NSF OAC 2112606, NIH R01LM014199, ARL W911NF2220144, and Cisco. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\n\nReferences\n[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.\n[2] B. AlKhamissi, M. Li, A. Celikyilmaz, M. T. Diab, and M. Ghazvininejad. A review on language models as knowledge bases. ArXiv, abs/2204.06031, 2022. URL https://arxiv. org/abs/2204.06031.\n[3] G. Angeli, M. J. Johnson Premkumar, and C. D. Manning. Leveraging linguistic structure for open domain information extraction. In C. Zong and M. Strube, editors, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1034. URL https://aclanthology.org/P15-1034.\n[4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07, page 2670-2676, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.\n[5] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In A. Korhonen, D. Traum, and L. M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1470. URL https://aclanthology.org/ P19-1470.\n[6] B. Chen and A. L. Bertozzi. AutoKG: Efficient Automated Knowledge Graph Generation for Language Models. In 2023 IEEE International Conference on Big Data (BigData), pages 3117-3126, Los Alamitos, CA, USA, dec 2023. IEEE Computer Society. doi: 10.1109/BigData59044.2023.10386454. URL https://doi.ieeecomputersociety.org/ 10.1109/BigData59044.2023.10386454.\n[7] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. CoRR, abs/2310.05029, 2023. doi: 10. 48550/ARXIV.2310.05029. URL https://doi.org/10.48550/arXiv. 2310.05029.\n[8] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, H. Zhang, and D. Yu. Dense x retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023. URL https://arxiv.org/abs/2312.06648.\n[9] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Finally, and perhaps most importantly, HippoRAG's scalability still calls for further validation.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03c6a61a-537c-42e8-9697-9b20517e0427",
        "questions": "Does the document suggest that Llama-3 could potentially reduce costs compared to closed-source models?",
        "answers": "Yes",
        "context": "$F$ that shows most errors made by our system are due to NER and OpenIE, which could benefit from direct fine-tuning. Given that the rest of the errors are graph search errors, also in Appendix F, we note that several avenues for improvements over simple PPR exist, such as allowing relations to guide graph traversal directly. Finally, and perhaps most importantly, HippoRAG's scalability still calls for further validation. Although we show that Llama-3 could obtain similar performance to closed-source models and thus reduce costs considerably, we are yet to empirically prove the efficiency and efficacy of our synthetic hippocampal index as its size grows way beyond current benchmarks.\n\n\nAcknowledgments\n\n\nThe authors would like to thank colleagues from the OSU NLP group and Percy Liang for their thoughtful comments. This research was supported in part by NSF OAC 2112606, NIH R01LM014199, ARL W911NF2220144, and Cisco. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\n\nReferences\n[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.\n[2] B. AlKhamissi, M. Li, A. Celikyilmaz, M. T. Diab, and M. Ghazvininejad. A review on language models as knowledge bases. ArXiv, abs/2204.06031, 2022. URL https://arxiv. org/abs/2204.06031.\n[3] G. Angeli, M. J. Johnson Premkumar, and C. D. Manning. Leveraging linguistic structure for open domain information extraction. In C. Zong and M. Strube, editors, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1034. URL https://aclanthology.org/P15-1034.\n[4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI'07, page 2670-2676, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.\n[5] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi. COMET: Commonsense transformers for automatic knowledge graph construction. In A. Korhonen, D. Traum, and L. M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1470. URL https://aclanthology.org/ P19-1470.\n[6] B. Chen and A. L. Bertozzi. AutoKG: Efficient Automated Knowledge Graph Generation for Language Models. In 2023 IEEE International Conference on Big Data (BigData), pages 3117-3126, Los Alamitos, CA, USA, dec 2023. IEEE Computer Society. doi: 10.1109/BigData59044.2023.10386454. URL https://doi.ieeecomputersociety.org/ 10.1109/BigData59044.2023.10386454.\n[7] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. CoRR, abs/2310.05029, 2023. doi: 10. 48550/ARXIV.2310.05029. URL https://doi.org/10.48550/arXiv. 2310.05029.\n[8] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, H. Zhang, and D. Yu. Dense x retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023. URL https://arxiv.org/abs/2312.06648.\n[9] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, and J. Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv:2309.12307, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Although we show that Llama-3 could obtain similar performance to closed-source models and thus reduce costs considerably, we are yet to empirically prove the efficiency and efficacy of our synthetic hippocampal index as its size grows way beyond current benchmarks.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03ca7fc9-eabb-4e02-b222-c94934301ab4",
        "questions": "What is the number of unique nodes in the 2WikiMultiHopQA dataset used in the experimental setup?",
        "answers": "42,694",
        "context": "After the query nodes $R_{q}$ are found, we run the PPR algorithm over the hippocampal index, i.e., a KG with $|N|$ nodes and $|E|+\\left|E^{\\prime}\\right|$ edges (triple-based and synonymy-based), using a personalized probability distribution $\\vec{n}$ defined over $N$, in which each query node has equal probability and all other nodes have a probability of zero. This allows probability mass to be distributed to nodes that are primarily in the (joint) neighborhood of the query nodes, such as Professor Thomas, and contribute to eventual retrieval. After running the PPR algorithm, we obtain an updated probability distribution $\\overrightarrow{n^{\\prime}}$ over $N$. Finally, in order to obtain passage scores, we multiply $\\overrightarrow{n^{\\prime}}$ with the previously defined $\\mathbf{P}$ matrix to obtain $\\vec{p}$, a ranking score for each passage, which we use for retrieval.\n\nNode Specificity. We introduce node specificity as a neurobiologically plausible way to further improve retrieval. It is well known that global signals for word importance, like inverse document frequency (IDF), can improve information retrieval. However, in order for our brain to leverage IDF for retrieval, the number of total \"passages\" encoded would need to be aggregated with all node activations before memory retrieval is complete. While simple for normal computers, this process would require activating connections between an aggregator neuron and all nodes in the hippocampal index every time retrieval occurs, likely introducing prohibitive computational overhead. Given these constraints, we propose node specificity as an alternative IDF signal which requires only local signals and is thus more neurobiologically plausible. We define the node specificity of node $i$ as $s_{i}=\\left|P_{i}\\right|^{-1}$, where $P_{i}$ is the set of passages in $P$ from which node $i$ was extracted, information that is already available at each node. Node specificity is used in retrieval by multiplying each query node probability $\\vec{n}$ with $s_{i}$ before PPR; this allows us to modulate each of their neighborhood's probability as well as their own. We illustrate node specificity in Figure 2 through relative symbol size: the Stanford logo grows larger than the Alzheimer's symbol since it appears in fewer documents.\n\n\n3 Experimental Setup\n\n\n3.1 Datasets\n\nWe evaluate our method's retrieval capabilities primarily on two challenging multi-hop QA benchmarks, MuSiQue (answerable) [60] and 2WikiMultiHopQA [25]. For completeness, we also include the HotpotQA [70] dataset even though it has been found to be a much weaker test for multi-hop reasoning due to many spurious signals [60], as we also show in Appendix B. To limit the experimental cost, we extract 1,000 questions from each validation set as done in previous work [48, 61]. In order to create a more realistic retrieval setting, we follow IRCoT [61] and collect all candidate passages (including supporting and distractor passages) from our selected questions and form a retrieval corpus for each dataset. The details of these datasets are shown in Table 1.\n\nTable 1: Retrieval corpora and extracted KG statistics for each of our 1,000 question dev sets.\n\\begin{tabular}{lrrr}\n  & MuSiQue & 2Wiki & HotpotQA \\\\\n  \\# of Passages $(P)$ & 11,656 & 6,119 & 9,221 \\\\\n\\# of Unique Nodes $(N)$ & 91,729 & 42,694 & 82,157 \\\\\n\\# of Unique Edges $(E)$ & 21,714 & 7,867 & 17,523 \\\\\n\\# of Unique Triples & 107,448 & 50,671 & 98,709 \\\\\n\\# of Contriever Synonym Edges $\\left(E^{\\prime}\\right)$ & 145,990 & 146,020 & 159,112 \\\\\n\\# of ColBERTv2 Synonym Edges $\\left(E^{\\prime}\\right)$ & 191,636 & 82,526 & 171,856 \\\\\n \n\\end{tabular}\n\n3.2 Baselines\n\nWe compare against several strong and widely used retrieval methods: BM25 [52], Contriever [27], GTR [41] and ColBERTv2 [53]. Additionally, we compare against two recent LLM-augmented baselines: Propositionizer [8], which rewrites passages into propositions, and RAPTOR [54], which constructs summary nodes to ease retrieval from long documents. In addition to the single-step retrieval methods above, we also include the multi-step retrieval method IRCoT [61] as a baseline.\n\n\\footnotetext{\n${ }^{4}$ Many details around the hippocampal memory indexing theory are omitted from this study for simplicity. We encourage interested reader to follow the references in $\\S 2.1$ for more.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "# of Unique Nodes $(N)$ & 91,729 & 42,694 & 82,157 \\",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03cc8bf9-6862-403f-b55d-aee71f2ba3e9",
        "questions": "What is the purpose of node specificity in the retrieval process described in the document?",
        "answers": "Node specificity is used in retrieval by multiplying each query node probability with $s_{i}$ before PPR; this allows us to modulate each of their neighborhood's probability as well as their own.",
        "context": "After the query nodes $R_{q}$ are found, we run the PPR algorithm over the hippocampal index, i.e., a KG with $|N|$ nodes and $|E|+\\left|E^{\\prime}\\right|$ edges (triple-based and synonymy-based), using a personalized probability distribution $\\vec{n}$ defined over $N$, in which each query node has equal probability and all other nodes have a probability of zero. This allows probability mass to be distributed to nodes that are primarily in the (joint) neighborhood of the query nodes, such as Professor Thomas, and contribute to eventual retrieval. After running the PPR algorithm, we obtain an updated probability distribution $\\overrightarrow{n^{\\prime}}$ over $N$. Finally, in order to obtain passage scores, we multiply $\\overrightarrow{n^{\\prime}}$ with the previously defined $\\mathbf{P}$ matrix to obtain $\\vec{p}$, a ranking score for each passage, which we use for retrieval.\n\nNode Specificity. We introduce node specificity as a neurobiologically plausible way to further improve retrieval. It is well known that global signals for word importance, like inverse document frequency (IDF), can improve information retrieval. However, in order for our brain to leverage IDF for retrieval, the number of total \"passages\" encoded would need to be aggregated with all node activations before memory retrieval is complete. While simple for normal computers, this process would require activating connections between an aggregator neuron and all nodes in the hippocampal index every time retrieval occurs, likely introducing prohibitive computational overhead. Given these constraints, we propose node specificity as an alternative IDF signal which requires only local signals and is thus more neurobiologically plausible. We define the node specificity of node $i$ as $s_{i}=\\left|P_{i}\\right|^{-1}$, where $P_{i}$ is the set of passages in $P$ from which node $i$ was extracted, information that is already available at each node. Node specificity is used in retrieval by multiplying each query node probability $\\vec{n}$ with $s_{i}$ before PPR; this allows us to modulate each of their neighborhood's probability as well as their own. We illustrate node specificity in Figure 2 through relative symbol size: the Stanford logo grows larger than the Alzheimer's symbol since it appears in fewer documents.\n\n\n3 Experimental Setup\n\n\n3.1 Datasets\n\nWe evaluate our method's retrieval capabilities primarily on two challenging multi-hop QA benchmarks, MuSiQue (answerable) [60] and 2WikiMultiHopQA [25]. For completeness, we also include the HotpotQA [70] dataset even though it has been found to be a much weaker test for multi-hop reasoning due to many spurious signals [60], as we also show in Appendix B. To limit the experimental cost, we extract 1,000 questions from each validation set as done in previous work [48, 61]. In order to create a more realistic retrieval setting, we follow IRCoT [61] and collect all candidate passages (including supporting and distractor passages) from our selected questions and form a retrieval corpus for each dataset. The details of these datasets are shown in Table 1.\n\nTable 1: Retrieval corpora and extracted KG statistics for each of our 1,000 question dev sets.\n\\begin{tabular}{lrrr}\n  & MuSiQue & 2Wiki & HotpotQA \\\\\n  \\# of Passages $(P)$ & 11,656 & 6,119 & 9,221 \\\\\n\\# of Unique Nodes $(N)$ & 91,729 & 42,694 & 82,157 \\\\\n\\# of Unique Edges $(E)$ & 21,714 & 7,867 & 17,523 \\\\\n\\# of Unique Triples & 107,448 & 50,671 & 98,709 \\\\\n\\# of Contriever Synonym Edges $\\left(E^{\\prime}\\right)$ & 145,990 & 146,020 & 159,112 \\\\\n\\# of ColBERTv2 Synonym Edges $\\left(E^{\\prime}\\right)$ & 191,636 & 82,526 & 171,856 \\\\\n \n\\end{tabular}\n\n3.2 Baselines\n\nWe compare against several strong and widely used retrieval methods: BM25 [52], Contriever [27], GTR [41] and ColBERTv2 [53]. Additionally, we compare against two recent LLM-augmented baselines: Propositionizer [8], which rewrites passages into propositions, and RAPTOR [54], which constructs summary nodes to ease retrieval from long documents. In addition to the single-step retrieval methods above, we also include the multi-step retrieval method IRCoT [61] as a baseline.\n\n\\footnotetext{\n${ }^{4}$ Many details around the hippocampal memory indexing theory are omitted from this study for simplicity. We encourage interested reader to follow the references in $\\S 2.1$ for more.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Node specificity is used in retrieval by multiplying each query node probability $\u000bec{n}$ with $s_{i}$ before PPR; this allows us to modulate each of their neighborhood's probability as well as their own.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03cd3231-7948-44ae-beb6-533abca281f1",
        "questions": "How many unique triples are there in the MuSiQue dataset according to the experimental setup?",
        "answers": "107,448",
        "context": "After the query nodes $R_{q}$ are found, we run the PPR algorithm over the hippocampal index, i.e., a KG with $|N|$ nodes and $|E|+\\left|E^{\\prime}\\right|$ edges (triple-based and synonymy-based), using a personalized probability distribution $\\vec{n}$ defined over $N$, in which each query node has equal probability and all other nodes have a probability of zero. This allows probability mass to be distributed to nodes that are primarily in the (joint) neighborhood of the query nodes, such as Professor Thomas, and contribute to eventual retrieval. After running the PPR algorithm, we obtain an updated probability distribution $\\overrightarrow{n^{\\prime}}$ over $N$. Finally, in order to obtain passage scores, we multiply $\\overrightarrow{n^{\\prime}}$ with the previously defined $\\mathbf{P}$ matrix to obtain $\\vec{p}$, a ranking score for each passage, which we use for retrieval.\n\nNode Specificity. We introduce node specificity as a neurobiologically plausible way to further improve retrieval. It is well known that global signals for word importance, like inverse document frequency (IDF), can improve information retrieval. However, in order for our brain to leverage IDF for retrieval, the number of total \"passages\" encoded would need to be aggregated with all node activations before memory retrieval is complete. While simple for normal computers, this process would require activating connections between an aggregator neuron and all nodes in the hippocampal index every time retrieval occurs, likely introducing prohibitive computational overhead. Given these constraints, we propose node specificity as an alternative IDF signal which requires only local signals and is thus more neurobiologically plausible. We define the node specificity of node $i$ as $s_{i}=\\left|P_{i}\\right|^{-1}$, where $P_{i}$ is the set of passages in $P$ from which node $i$ was extracted, information that is already available at each node. Node specificity is used in retrieval by multiplying each query node probability $\\vec{n}$ with $s_{i}$ before PPR; this allows us to modulate each of their neighborhood's probability as well as their own. We illustrate node specificity in Figure 2 through relative symbol size: the Stanford logo grows larger than the Alzheimer's symbol since it appears in fewer documents.\n\n\n3 Experimental Setup\n\n\n3.1 Datasets\n\nWe evaluate our method's retrieval capabilities primarily on two challenging multi-hop QA benchmarks, MuSiQue (answerable) [60] and 2WikiMultiHopQA [25]. For completeness, we also include the HotpotQA [70] dataset even though it has been found to be a much weaker test for multi-hop reasoning due to many spurious signals [60], as we also show in Appendix B. To limit the experimental cost, we extract 1,000 questions from each validation set as done in previous work [48, 61]. In order to create a more realistic retrieval setting, we follow IRCoT [61] and collect all candidate passages (including supporting and distractor passages) from our selected questions and form a retrieval corpus for each dataset. The details of these datasets are shown in Table 1.\n\nTable 1: Retrieval corpora and extracted KG statistics for each of our 1,000 question dev sets.\n\\begin{tabular}{lrrr}\n  & MuSiQue & 2Wiki & HotpotQA \\\\\n  \\# of Passages $(P)$ & 11,656 & 6,119 & 9,221 \\\\\n\\# of Unique Nodes $(N)$ & 91,729 & 42,694 & 82,157 \\\\\n\\# of Unique Edges $(E)$ & 21,714 & 7,867 & 17,523 \\\\\n\\# of Unique Triples & 107,448 & 50,671 & 98,709 \\\\\n\\# of Contriever Synonym Edges $\\left(E^{\\prime}\\right)$ & 145,990 & 146,020 & 159,112 \\\\\n\\# of ColBERTv2 Synonym Edges $\\left(E^{\\prime}\\right)$ & 191,636 & 82,526 & 171,856 \\\\\n \n\\end{tabular}\n\n3.2 Baselines\n\nWe compare against several strong and widely used retrieval methods: BM25 [52], Contriever [27], GTR [41] and ColBERTv2 [53]. Additionally, we compare against two recent LLM-augmented baselines: Propositionizer [8], which rewrites passages into propositions, and RAPTOR [54], which constructs summary nodes to ease retrieval from long documents. In addition to the single-step retrieval methods above, we also include the multi-step retrieval method IRCoT [61] as a baseline.\n\n\\footnotetext{\n${ }^{4}$ Many details around the hippocampal memory indexing theory are omitted from this study for simplicity. We encourage interested reader to follow the references in $\\S 2.1$ for more.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "# of Unique Triples & 107,448 & 50,671 & 98,709 \\",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03d03575-54bb-48bd-8e8b-1b517a0cd3b5",
        "questions": "What is one of the main advantages of HippoRAG compared to iterative retrieval methods like IRCoT in terms of cost and time efficiency?",
        "answers": "HippoRAG's main advantages against iterative retrieval methods is the dramatic online retrieval efficiency gains brought on by its single-step multi-hop retrieval ability in terms of both cost and time.",
        "context": "promising, more work needs to be done to solve this context-context tradeoff since simple ensembling does lower performance in some cases, especially for the 2WikiMultiHopQA dataset.\n\nTable 13: Multi-step retrieval performance. Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in significant improvements on all three datasets.\n\\begin{tabular}{llllllll|lll}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  IRCoT & Contriever & 39.1 & 52.2 & 51.6 & 63.8 & 65.9 & 81.6 & 52.2 & 65.9 \\\\\n& ColBERTv2 & 41.7 & 53.7 & 64.1 & 74.4 & $\\underline{67.9}$ & 82.0 & 57.9 & 70.0 \\\\\n  IRCoT + HippoRAG & Contriever & 43.9 & 56.6 & 75.3 & $\\underline{93.4}$ & 65.8 & 82.3 & 61.7 & 77.4 \\\\\n& ColBERTv2 & $\\mathbf{4 5 . 3}$ & $\\underline{57.6}$ & $\\mathbf{7 5 . 8}$ & $\\mathbf{9 3 . 9}$ & 67.0 & 83.0 & $\\mathbf{6 2 . 7}$ & $\\underline{78.2}$ \\\\\n  IRCoT + HippoRAG w/ & Contriever & $\\underline{44.4}$ & $\\mathbf{5 8 . 5}$ & 75.3 & 91.5 & 66.9 & $\\underline{85.0}$ & $\\underline{62.2}$ & $\\mathbf{7 8 . 3}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & 40.2 & 53.4 & 74.5 & 91.2 & $\\mathbf{6 8 . 2}$ & $\\mathbf{8 5 . 3}$ & 61.0 & 76.6 \\\\\n \n\\end{tabular}\n\n\nF. 3 OpenIE Limitations\n\n\nOpenIE is a critical step in extracting structured knowledge from unstructured text. Nonetheless, its shortcomings can result in gaps in knowledge that may impair retrieval and QA capabilities. As shown in Table 14, GPT-3.5 Turbo overlooks the crucial song title \"Don't Let Me Wait Too Long\" during the OpenIE process. This title represents the most significant element within the passage. A probable reason is that the model is insensitive to such a long entity. Besides, the model does not accurately capture the beginning and ending years of the war, which are essential for the query. This is an example of how models routinely ignore temporal properties. Overall, these failures highlight the need to improve the extraction of critical information.\n\nTable 14: Open information extraction error examples on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & Passage & Missed Triples \\\\\n  What company is the label responsible for \"Don't Let Me Wait Too Long\" a part of? & \"Don't Let Me Wait Too Long\" was sequenced on side one of the LP, between the ballads \"The Light That Has Lighted the World\" and \"Who Can See It\" ... & (Don't Let Me Wait Too Long, sequenced on, side one of the LP) \\\\\n  When did the president of the Confederate States of America end his fight in the MexicanAmerican war? & Jefferson Davis fought in the Mexi-can-American War (1846-1848), as the colonel of a volunteer regiment ... & \\begin{tabular}{l}\n(Mexican-American \\\\\nWar, starts, 1846), \\\\\n(Mexican-American \\\\\nWar, ends, 1848)\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nG Cost and Efficiency Comparison\n\nOne of HippoRAG's main advantages against iterative retrieval methods is the dramatic online retrieval efficiency gains brought on by its single-step multi-hop retrieval ability in terms of both cost and time. Specifically, as seen in Table 15, retrieval costs for IRCoT are 10 to 30 times higher than HippoRAG since it only requires extracting relevant named entities from the query instead of processing all of the retrieved documents. In systems with extremely high usage, a cost difference of an order of magnitude such as this one could be extremely important. The difference with IRCoT in terms of latency is also substantial, although more challenging to measure exactly. Also as seen in Table 15, HippoRAG can be 6 to 13 times faster than IRCoT, depending on the number of retrieval rounds that need to be executed (2-4 in our experiments). ${ }^{6}$\n\n\\footnotetext{\n${ }^{6}$ We use a single thread to query the OpenAI API for online retrieval in both IRCoT and HippoRAG. Since IRCoT is an iterative process and each of the iterations must be done sequentially, these speed comparisons are appropriate.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "One of HippoRAG's main advantages against iterative retrieval methods is the dramatic online retrieval efficiency gains brought on by its single-step multi-hop retrieval ability in terms of both cost and time.",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03d820d3-a3aa-4c06-b050-c7df142fa639",
        "questions": "How many times higher are the retrieval costs for IRCoT compared to HippoRAG?",
        "answers": "10 to 30 times higher",
        "context": "promising, more work needs to be done to solve this context-context tradeoff since simple ensembling does lower performance in some cases, especially for the 2WikiMultiHopQA dataset.\n\nTable 13: Multi-step retrieval performance. Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in significant improvements on all three datasets.\n\\begin{tabular}{llllllll|lll}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  IRCoT & Contriever & 39.1 & 52.2 & 51.6 & 63.8 & 65.9 & 81.6 & 52.2 & 65.9 \\\\\n& ColBERTv2 & 41.7 & 53.7 & 64.1 & 74.4 & $\\underline{67.9}$ & 82.0 & 57.9 & 70.0 \\\\\n  IRCoT + HippoRAG & Contriever & 43.9 & 56.6 & 75.3 & $\\underline{93.4}$ & 65.8 & 82.3 & 61.7 & 77.4 \\\\\n& ColBERTv2 & $\\mathbf{4 5 . 3}$ & $\\underline{57.6}$ & $\\mathbf{7 5 . 8}$ & $\\mathbf{9 3 . 9}$ & 67.0 & 83.0 & $\\mathbf{6 2 . 7}$ & $\\underline{78.2}$ \\\\\n  IRCoT + HippoRAG w/ & Contriever & $\\underline{44.4}$ & $\\mathbf{5 8 . 5}$ & 75.3 & 91.5 & 66.9 & $\\underline{85.0}$ & $\\underline{62.2}$ & $\\mathbf{7 8 . 3}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & 40.2 & 53.4 & 74.5 & 91.2 & $\\mathbf{6 8 . 2}$ & $\\mathbf{8 5 . 3}$ & 61.0 & 76.6 \\\\\n \n\\end{tabular}\n\n\nF. 3 OpenIE Limitations\n\n\nOpenIE is a critical step in extracting structured knowledge from unstructured text. Nonetheless, its shortcomings can result in gaps in knowledge that may impair retrieval and QA capabilities. As shown in Table 14, GPT-3.5 Turbo overlooks the crucial song title \"Don't Let Me Wait Too Long\" during the OpenIE process. This title represents the most significant element within the passage. A probable reason is that the model is insensitive to such a long entity. Besides, the model does not accurately capture the beginning and ending years of the war, which are essential for the query. This is an example of how models routinely ignore temporal properties. Overall, these failures highlight the need to improve the extraction of critical information.\n\nTable 14: Open information extraction error examples on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & Passage & Missed Triples \\\\\n  What company is the label responsible for \"Don't Let Me Wait Too Long\" a part of? & \"Don't Let Me Wait Too Long\" was sequenced on side one of the LP, between the ballads \"The Light That Has Lighted the World\" and \"Who Can See It\" ... & (Don't Let Me Wait Too Long, sequenced on, side one of the LP) \\\\\n  When did the president of the Confederate States of America end his fight in the MexicanAmerican war? & Jefferson Davis fought in the Mexi-can-American War (1846-1848), as the colonel of a volunteer regiment ... & \\begin{tabular}{l}\n(Mexican-American \\\\\nWar, starts, 1846), \\\\\n(Mexican-American \\\\\nWar, ends, 1848)\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nG Cost and Efficiency Comparison\n\nOne of HippoRAG's main advantages against iterative retrieval methods is the dramatic online retrieval efficiency gains brought on by its single-step multi-hop retrieval ability in terms of both cost and time. Specifically, as seen in Table 15, retrieval costs for IRCoT are 10 to 30 times higher than HippoRAG since it only requires extracting relevant named entities from the query instead of processing all of the retrieved documents. In systems with extremely high usage, a cost difference of an order of magnitude such as this one could be extremely important. The difference with IRCoT in terms of latency is also substantial, although more challenging to measure exactly. Also as seen in Table 15, HippoRAG can be 6 to 13 times faster than IRCoT, depending on the number of retrieval rounds that need to be executed (2-4 in our experiments). ${ }^{6}$\n\n\\footnotetext{\n${ }^{6}$ We use a single thread to query the OpenAI API for online retrieval in both IRCoT and HippoRAG. Since IRCoT is an iterative process and each of the iterations must be done sequentially, these speed comparisons are appropriate.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Specifically, as seen in Table 15, retrieval costs for IRCoT are 10 to 30 times higher than HippoRAG since it only requires extracting relevant named entities from the query instead of processing all of the retrieved documents.",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03e645b9-a24e-45ed-9651-823f1f0e68c0",
        "questions": "Does the OpenIE process in GPT-3.5 Turbo accurately capture the beginning and ending years of the Mexican-American War?",
        "answers": "No",
        "context": "promising, more work needs to be done to solve this context-context tradeoff since simple ensembling does lower performance in some cases, especially for the 2WikiMultiHopQA dataset.\n\nTable 13: Multi-step retrieval performance. Combining HippoRAG with standard multi-step retrieval methods like IRCoT results in significant improvements on all three datasets.\n\\begin{tabular}{llllllll|lll}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  IRCoT & Contriever & 39.1 & 52.2 & 51.6 & 63.8 & 65.9 & 81.6 & 52.2 & 65.9 \\\\\n& ColBERTv2 & 41.7 & 53.7 & 64.1 & 74.4 & $\\underline{67.9}$ & 82.0 & 57.9 & 70.0 \\\\\n  IRCoT + HippoRAG & Contriever & 43.9 & 56.6 & 75.3 & $\\underline{93.4}$ & 65.8 & 82.3 & 61.7 & 77.4 \\\\\n& ColBERTv2 & $\\mathbf{4 5 . 3}$ & $\\underline{57.6}$ & $\\mathbf{7 5 . 8}$ & $\\mathbf{9 3 . 9}$ & 67.0 & 83.0 & $\\mathbf{6 2 . 7}$ & $\\underline{78.2}$ \\\\\n  IRCoT + HippoRAG w/ & Contriever & $\\underline{44.4}$ & $\\mathbf{5 8 . 5}$ & 75.3 & 91.5 & 66.9 & $\\underline{85.0}$ & $\\underline{62.2}$ & $\\mathbf{7 8 . 3}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & 40.2 & 53.4 & 74.5 & 91.2 & $\\mathbf{6 8 . 2}$ & $\\mathbf{8 5 . 3}$ & 61.0 & 76.6 \\\\\n \n\\end{tabular}\n\n\nF. 3 OpenIE Limitations\n\n\nOpenIE is a critical step in extracting structured knowledge from unstructured text. Nonetheless, its shortcomings can result in gaps in knowledge that may impair retrieval and QA capabilities. As shown in Table 14, GPT-3.5 Turbo overlooks the crucial song title \"Don't Let Me Wait Too Long\" during the OpenIE process. This title represents the most significant element within the passage. A probable reason is that the model is insensitive to such a long entity. Besides, the model does not accurately capture the beginning and ending years of the war, which are essential for the query. This is an example of how models routinely ignore temporal properties. Overall, these failures highlight the need to improve the extraction of critical information.\n\nTable 14: Open information extraction error examples on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & Passage & Missed Triples \\\\\n  What company is the label responsible for \"Don't Let Me Wait Too Long\" a part of? & \"Don't Let Me Wait Too Long\" was sequenced on side one of the LP, between the ballads \"The Light That Has Lighted the World\" and \"Who Can See It\" ... & (Don't Let Me Wait Too Long, sequenced on, side one of the LP) \\\\\n  When did the president of the Confederate States of America end his fight in the MexicanAmerican war? & Jefferson Davis fought in the Mexi-can-American War (1846-1848), as the colonel of a volunteer regiment ... & \\begin{tabular}{l}\n(Mexican-American \\\\\nWar, starts, 1846), \\\\\n(Mexican-American \\\\\nWar, ends, 1848)\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nG Cost and Efficiency Comparison\n\nOne of HippoRAG's main advantages against iterative retrieval methods is the dramatic online retrieval efficiency gains brought on by its single-step multi-hop retrieval ability in terms of both cost and time. Specifically, as seen in Table 15, retrieval costs for IRCoT are 10 to 30 times higher than HippoRAG since it only requires extracting relevant named entities from the query instead of processing all of the retrieved documents. In systems with extremely high usage, a cost difference of an order of magnitude such as this one could be extremely important. The difference with IRCoT in terms of latency is also substantial, although more challenging to measure exactly. Also as seen in Table 15, HippoRAG can be 6 to 13 times faster than IRCoT, depending on the number of retrieval rounds that need to be executed (2-4 in our experiments). ${ }^{6}$\n\n\\footnotetext{\n${ }^{6}$ We use a single thread to query the OpenAI API for online retrieval in both IRCoT and HippoRAG. Since IRCoT is an iterative process and each of the iterations must be done sequentially, these speed comparisons are appropriate.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Besides, the model does not accurately capture the beginning and ending years of the war, which are essential for the query.",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03ee36c3-2b3f-419f-bba9-fbb0d8c6827b",
        "questions": "What algorithm is used in the HippoRAG methodology to enable context-based retrieval?",
        "answers": "Personalized PageRank algorithm",
        "context": "Figure 2: Detailed HippoRAG Methodology. We model the three components of human long-term memory to mimic its pattern separation and completion functions. For offline indexing (Middle), we use an LLM to process passages into open KG triples, which are then added to our artificial hippocampal index, while our synthetic parahippocampal regions (PHR) detect synonymy. In the example above, triples involving Professor Thomas are extracted and integrated into the KG. For online retrieval (Bottom), our LLM neocortex extracts named entities from a query while our parahippocampal retrieval encoders link them to our hippocampal index. We then leverage the Personalized PageRank algorithm to enable context-based retrieval and extract Professor Thomas. ${ }^{4}$\n(PPR) algorithm [23], a version of PageRank that distributes probability across a graph only through a set of user-defined source nodes. This constraint allows us to bias the PPR output only towards the set of query nodes, just as the hippocampus extracts associated signals from specific partial cues. ${ }^{2}$ Finally, as is done when the hippocampal signal is sent upstream, we aggregate the output PPR node probability over the previously indexed passages and use that to rank them for retrieval.\n\n\n2.3 Detailed Methodology\n\n\nOffline Indexing. Our indexing process involves processing a set of passages $P$ using an instructiontuned LLM $L$ and a retrieval encoder $M$. As seen in Figure 2 we first use $L$ to extract a set of noun phrase nodes $N$ and relation edges $E$ from each passage in $P$ via OpenIE. This process is done via 1 -shot prompting of the LLM with the prompts shown in Appendix I. Specifically, we first extract a set of named entities from each passage. We then add the named entities to the OpenIE prompt to extract the final triples, which also contain concepts (noun phrases) beyond named entities. We find that this two-step prompt configuration leads to an appropriate balance between generality and bias towards named entities. Finally, we use $M$ to add the extra set of synonymy relations $E^{\\prime}$ discussed above when the cosine similarity between two entity representations in $N$ is above a threshold $\\tau$. As stated above, this introduces more edges to our hippocampal index and allows for more effective pattern completion. This indexing process defines a $|N| \\times|P|$ matrix $\\mathbf{P}$, which contains the number of times each noun phrase in the KG appears in each original passage.\nOnline Retrieval. During the retrieval process, we prompt $L$ using a 1 -shot prompt to extract a set of named entities from a query $q$, our previously defined query named entities $C_{q}=\\left\\{c_{1}, \\ldots, c_{n}\\right\\}$ (Stanford and Alzheimer's in our Figure 2 example). These named entities $C_{q}$ from the query are then encoded by the same retrieval encoder $M$. Then, the previously defined query nodes are chosen as the set of nodes in $N$ with the highest cosine similarity to the query named entities $C_{q}$. More formally, query nodes are defined as $R_{q}=\\left\\{r_{1}, \\ldots, r_{n}\\right\\}$ such that $r_{i}=e_{k}$ where $k=$ $\\arg \\max _{j}$ cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$, represented as the Stanford logo and the Alzheimer's purple ribbon symbol in Figure 2\n\n\\footnotetext{\n${ }^{2}$ Intriguingly, some work in cognitive science has also found a correlation between human word recall and the output of the PageRank algorithm [19].\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We then leverage the Personalized PageRank algorithm to enable context-based retrieval and extract Professor Thomas.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03f7ba05-cf4b-4315-a325-4d812bdfb727",
        "questions": "In the HippoRAG methodology, what is the purpose of using a retrieval encoder during offline indexing?",
        "answers": "To add the extra set of synonymy relations when the cosine similarity between two entity representations is above a threshold.",
        "context": "Figure 2: Detailed HippoRAG Methodology. We model the three components of human long-term memory to mimic its pattern separation and completion functions. For offline indexing (Middle), we use an LLM to process passages into open KG triples, which are then added to our artificial hippocampal index, while our synthetic parahippocampal regions (PHR) detect synonymy. In the example above, triples involving Professor Thomas are extracted and integrated into the KG. For online retrieval (Bottom), our LLM neocortex extracts named entities from a query while our parahippocampal retrieval encoders link them to our hippocampal index. We then leverage the Personalized PageRank algorithm to enable context-based retrieval and extract Professor Thomas. ${ }^{4}$\n(PPR) algorithm [23], a version of PageRank that distributes probability across a graph only through a set of user-defined source nodes. This constraint allows us to bias the PPR output only towards the set of query nodes, just as the hippocampus extracts associated signals from specific partial cues. ${ }^{2}$ Finally, as is done when the hippocampal signal is sent upstream, we aggregate the output PPR node probability over the previously indexed passages and use that to rank them for retrieval.\n\n\n2.3 Detailed Methodology\n\n\nOffline Indexing. Our indexing process involves processing a set of passages $P$ using an instructiontuned LLM $L$ and a retrieval encoder $M$. As seen in Figure 2 we first use $L$ to extract a set of noun phrase nodes $N$ and relation edges $E$ from each passage in $P$ via OpenIE. This process is done via 1 -shot prompting of the LLM with the prompts shown in Appendix I. Specifically, we first extract a set of named entities from each passage. We then add the named entities to the OpenIE prompt to extract the final triples, which also contain concepts (noun phrases) beyond named entities. We find that this two-step prompt configuration leads to an appropriate balance between generality and bias towards named entities. Finally, we use $M$ to add the extra set of synonymy relations $E^{\\prime}$ discussed above when the cosine similarity between two entity representations in $N$ is above a threshold $\\tau$. As stated above, this introduces more edges to our hippocampal index and allows for more effective pattern completion. This indexing process defines a $|N| \\times|P|$ matrix $\\mathbf{P}$, which contains the number of times each noun phrase in the KG appears in each original passage.\nOnline Retrieval. During the retrieval process, we prompt $L$ using a 1 -shot prompt to extract a set of named entities from a query $q$, our previously defined query named entities $C_{q}=\\left\\{c_{1}, \\ldots, c_{n}\\right\\}$ (Stanford and Alzheimer's in our Figure 2 example). These named entities $C_{q}$ from the query are then encoded by the same retrieval encoder $M$. Then, the previously defined query nodes are chosen as the set of nodes in $N$ with the highest cosine similarity to the query named entities $C_{q}$. More formally, query nodes are defined as $R_{q}=\\left\\{r_{1}, \\ldots, r_{n}\\right\\}$ such that $r_{i}=e_{k}$ where $k=$ $\\arg \\max _{j}$ cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$, represented as the Stanford logo and the Alzheimer's purple ribbon symbol in Figure 2\n\n\\footnotetext{\n${ }^{2}$ Intriguingly, some work in cognitive science has also found a correlation between human word recall and the output of the PageRank algorithm [19].\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Finally, we use $M$ to add the extra set of synonymy relations $E^{\\prime}$ discussed above when the cosine similarity between two entity representations in $N$ is above a threshold $\tau$.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03f9c3a3-5fb1-40dd-b4fe-f75045709961",
        "questions": "What is the matrix defined during the offline indexing process in the HippoRAG methodology, and what does it represent?",
        "answers": "|N| \u00d7 |P| matrix, which contains the number of times each noun phrase in the KG appears in each original passage.",
        "context": "Figure 2: Detailed HippoRAG Methodology. We model the three components of human long-term memory to mimic its pattern separation and completion functions. For offline indexing (Middle), we use an LLM to process passages into open KG triples, which are then added to our artificial hippocampal index, while our synthetic parahippocampal regions (PHR) detect synonymy. In the example above, triples involving Professor Thomas are extracted and integrated into the KG. For online retrieval (Bottom), our LLM neocortex extracts named entities from a query while our parahippocampal retrieval encoders link them to our hippocampal index. We then leverage the Personalized PageRank algorithm to enable context-based retrieval and extract Professor Thomas. ${ }^{4}$\n(PPR) algorithm [23], a version of PageRank that distributes probability across a graph only through a set of user-defined source nodes. This constraint allows us to bias the PPR output only towards the set of query nodes, just as the hippocampus extracts associated signals from specific partial cues. ${ }^{2}$ Finally, as is done when the hippocampal signal is sent upstream, we aggregate the output PPR node probability over the previously indexed passages and use that to rank them for retrieval.\n\n\n2.3 Detailed Methodology\n\n\nOffline Indexing. Our indexing process involves processing a set of passages $P$ using an instructiontuned LLM $L$ and a retrieval encoder $M$. As seen in Figure 2 we first use $L$ to extract a set of noun phrase nodes $N$ and relation edges $E$ from each passage in $P$ via OpenIE. This process is done via 1 -shot prompting of the LLM with the prompts shown in Appendix I. Specifically, we first extract a set of named entities from each passage. We then add the named entities to the OpenIE prompt to extract the final triples, which also contain concepts (noun phrases) beyond named entities. We find that this two-step prompt configuration leads to an appropriate balance between generality and bias towards named entities. Finally, we use $M$ to add the extra set of synonymy relations $E^{\\prime}$ discussed above when the cosine similarity between two entity representations in $N$ is above a threshold $\\tau$. As stated above, this introduces more edges to our hippocampal index and allows for more effective pattern completion. This indexing process defines a $|N| \\times|P|$ matrix $\\mathbf{P}$, which contains the number of times each noun phrase in the KG appears in each original passage.\nOnline Retrieval. During the retrieval process, we prompt $L$ using a 1 -shot prompt to extract a set of named entities from a query $q$, our previously defined query named entities $C_{q}=\\left\\{c_{1}, \\ldots, c_{n}\\right\\}$ (Stanford and Alzheimer's in our Figure 2 example). These named entities $C_{q}$ from the query are then encoded by the same retrieval encoder $M$. Then, the previously defined query nodes are chosen as the set of nodes in $N$ with the highest cosine similarity to the query named entities $C_{q}$. More formally, query nodes are defined as $R_{q}=\\left\\{r_{1}, \\ldots, r_{n}\\right\\}$ such that $r_{i}=e_{k}$ where $k=$ $\\arg \\max _{j}$ cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$, represented as the Stanford logo and the Alzheimer's purple ribbon symbol in Figure 2\n\n\\footnotetext{\n${ }^{2}$ Intriguingly, some work in cognitive science has also found a correlation between human word recall and the output of the PageRank algorithm [19].\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "This indexing process defines a $|N| \times|P|$ matrix $\\mathbf{P}$, which contains the number of times each noun phrase in the KG appears in each original passage.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03fa6c59-a9e8-4538-880b-3ff9f4b7d61d",
        "questions": "In which district was the Portuguese footballer Alhandra born?",
        "answers": "Lisbon",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "03fd60b3-afbd-47df-8612-154953a08862",
        "questions": "What is the population of Vila Franca de Xira as of 2011?",
        "answers": "136,886",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Vila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0409a997-8408-46d9-a8dc-d8840b0e52d5",
        "questions": "Is Vila Franca de Xira located on both banks of the Tagus River?",
        "answers": "Yes",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Vila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "040c4d58-954a-4019-a093-e30f1e1eedd9",
        "questions": "What is the primary advantage of using RAG methods as a long-term memory system over parametric memory in LLMs?",
        "answers": "RAG methods offer a simple way to update knowledge over time.",
        "context": "easily. As seen in Table 6, both ColBERTv2 and IRCoT fail to extract the necessary passages since they cannot access these associations. On the other hand, HippoRAG leverages its web of associations in its hippocampal index and graph search algorithm to determine that Professor Thomas is relevant to this query and retrieves his passages appropriately. More examples of these path-finding multi-hop questions can be found in our case study in Appendix E.\n\n\n6 Related Work\n\n\nParametric Long-Term Memory. It is well-accepted, even among skeptical researchers, that the parameters of modern LLMs encode a remarkable amount of world knowledge [2, 10, 17, 21, 24, 31, 47, 62], which can be leveraged by an LLM in flexible and robust ways [64, 65, 74]. Nevertheless, our ability to update this vast knowledge store, an essential part of any long-term memory system, is still surprisingly limited. Although many techniques to update LLMs exist, such as standard fine-tuning, model unlearning and model editing [12,37,38,39,40,76], it is clear that no methodology has emerged as a robust solution for continual learning in LLMs [20, 35, 78].\nRAG as Long-Term Memory. On the other hand, using RAG methods as a long-term memory system offers a simple way to update knowledge over time [28, 33, 50, 56]. More sophisticated RAG methods, which perform multiple steps of retrieval and generation from an LLM, are even able to integrate information across new or updated knowledge elements[30, 49, 55, 61, 69, 71, 73], another crucial aspect of long-term memory systems. As discussed above, however, this type of online information integration is unable to solve the more complex knowledge integration tasks that we illustrate with our path-finding multi-hop QA examples.\n\nSome other methods, such as RAPTOR [54], MemWalker [7] and GraphRAG [14], integrate information during the offline indexing phase similarly to HippoRAG and might be able to handle these more complex tasks. However, these methods integrate information by summarizing knowledge elements, which means that the summarization process must be repeated any time new data is added. In contrast, HippoRAG can continuously integrate new knowledge by simply adding edges to its KG.\nLong Context as Long-Term Memory. Context lengths for both open and closed source LLMs have increased dramatically in the past year $[9,13,16,46,51]$. This scaling trend seems to indicate that future LLMs could perform long-term memory storage within massive context windows. However, the viability of this future remains largely uncertain given the many engineering hurdles involved and the apparent limitations of long-context LLMs, even within current context lengths [32, 34, 77].\n\nLLMs \\& KGs. Combining the strengths of language models and knowledge graphs has been an active research direction for many years, both for augmenting LLMs with a KG in different ways [36, 63, 66] or augmenting KGs by either distilling knowledge from an LLM's parametric knowledge $[5,67]$ or using them to parse text directly $[6,22,75]$. In an exceptionally comprehensive survey, Pan et al. [43] present a roadmap for this research direction and highlight the importance of work which synergizes these two important technologies [29, 57, 72, 80]. Like these works, HippoRAG is a strong and principled example of the synergy we must strike between these two technologies, combining the power of LLMs for knowledge graph construction with the strengths of structured knowledge and graph search for improved augmentation of an LLM's capacities.\n\n7 Conclusions \\& Limitations\n\nOur proposed neurobiologically principled methodology, although simple, already shows promise for overcoming the inherent limitations of standard RAG systems while retaining their advantages over parametric memory. HippoRAG's knowledge integration capabilities, demonstrated by its strong results on path-following multi-hop QA and promise on path-finding multi-hop QA, as well as its dramatic efficiency improvements and continuously updating nature, makes it a powerful middleground framework between standard RAG methods and parametric memory and offers a compelling solution for long-term memory in LLMs.\n\nNevertheless, several limitations can be addressed in future work to enable HippoRAG to achieve this goal better. First, we note that all components of HippoRAG are currently used off-the-shelf without any extra training. There is therefore much room to improve our method's practical viability by performing specific component fine-tuning. This is evident in the error analysis discussed in Appendix",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "RAG as Long-Term Memory. On the other hand, using RAG methods as a long-term memory system offers a simple way to update knowledge over time [28, 33, 50, 56].",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "042506df-e159-4eab-ab52-f0583a7fe1b3",
        "questions": "Which method is mentioned as being able to continuously integrate new knowledge by adding edges to its knowledge graph?",
        "answers": "HippoRAG",
        "context": "easily. As seen in Table 6, both ColBERTv2 and IRCoT fail to extract the necessary passages since they cannot access these associations. On the other hand, HippoRAG leverages its web of associations in its hippocampal index and graph search algorithm to determine that Professor Thomas is relevant to this query and retrieves his passages appropriately. More examples of these path-finding multi-hop questions can be found in our case study in Appendix E.\n\n\n6 Related Work\n\n\nParametric Long-Term Memory. It is well-accepted, even among skeptical researchers, that the parameters of modern LLMs encode a remarkable amount of world knowledge [2, 10, 17, 21, 24, 31, 47, 62], which can be leveraged by an LLM in flexible and robust ways [64, 65, 74]. Nevertheless, our ability to update this vast knowledge store, an essential part of any long-term memory system, is still surprisingly limited. Although many techniques to update LLMs exist, such as standard fine-tuning, model unlearning and model editing [12,37,38,39,40,76], it is clear that no methodology has emerged as a robust solution for continual learning in LLMs [20, 35, 78].\nRAG as Long-Term Memory. On the other hand, using RAG methods as a long-term memory system offers a simple way to update knowledge over time [28, 33, 50, 56]. More sophisticated RAG methods, which perform multiple steps of retrieval and generation from an LLM, are even able to integrate information across new or updated knowledge elements[30, 49, 55, 61, 69, 71, 73], another crucial aspect of long-term memory systems. As discussed above, however, this type of online information integration is unable to solve the more complex knowledge integration tasks that we illustrate with our path-finding multi-hop QA examples.\n\nSome other methods, such as RAPTOR [54], MemWalker [7] and GraphRAG [14], integrate information during the offline indexing phase similarly to HippoRAG and might be able to handle these more complex tasks. However, these methods integrate information by summarizing knowledge elements, which means that the summarization process must be repeated any time new data is added. In contrast, HippoRAG can continuously integrate new knowledge by simply adding edges to its KG.\nLong Context as Long-Term Memory. Context lengths for both open and closed source LLMs have increased dramatically in the past year $[9,13,16,46,51]$. This scaling trend seems to indicate that future LLMs could perform long-term memory storage within massive context windows. However, the viability of this future remains largely uncertain given the many engineering hurdles involved and the apparent limitations of long-context LLMs, even within current context lengths [32, 34, 77].\n\nLLMs \\& KGs. Combining the strengths of language models and knowledge graphs has been an active research direction for many years, both for augmenting LLMs with a KG in different ways [36, 63, 66] or augmenting KGs by either distilling knowledge from an LLM's parametric knowledge $[5,67]$ or using them to parse text directly $[6,22,75]$. In an exceptionally comprehensive survey, Pan et al. [43] present a roadmap for this research direction and highlight the importance of work which synergizes these two important technologies [29, 57, 72, 80]. Like these works, HippoRAG is a strong and principled example of the synergy we must strike between these two technologies, combining the power of LLMs for knowledge graph construction with the strengths of structured knowledge and graph search for improved augmentation of an LLM's capacities.\n\n7 Conclusions \\& Limitations\n\nOur proposed neurobiologically principled methodology, although simple, already shows promise for overcoming the inherent limitations of standard RAG systems while retaining their advantages over parametric memory. HippoRAG's knowledge integration capabilities, demonstrated by its strong results on path-following multi-hop QA and promise on path-finding multi-hop QA, as well as its dramatic efficiency improvements and continuously updating nature, makes it a powerful middleground framework between standard RAG methods and parametric memory and offers a compelling solution for long-term memory in LLMs.\n\nNevertheless, several limitations can be addressed in future work to enable HippoRAG to achieve this goal better. First, we note that all components of HippoRAG are currently used off-the-shelf without any extra training. There is therefore much room to improve our method's practical viability by performing specific component fine-tuning. This is evident in the error analysis discussed in Appendix",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In contrast, HippoRAG can continuously integrate new knowledge by simply adding edges to its KG.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04397e4a-3aff-4da3-9c91-536ba383dfb5",
        "questions": "Does the document suggest that future LLMs could perform long-term memory storage within massive context windows?",
        "answers": "Yes",
        "context": "easily. As seen in Table 6, both ColBERTv2 and IRCoT fail to extract the necessary passages since they cannot access these associations. On the other hand, HippoRAG leverages its web of associations in its hippocampal index and graph search algorithm to determine that Professor Thomas is relevant to this query and retrieves his passages appropriately. More examples of these path-finding multi-hop questions can be found in our case study in Appendix E.\n\n\n6 Related Work\n\n\nParametric Long-Term Memory. It is well-accepted, even among skeptical researchers, that the parameters of modern LLMs encode a remarkable amount of world knowledge [2, 10, 17, 21, 24, 31, 47, 62], which can be leveraged by an LLM in flexible and robust ways [64, 65, 74]. Nevertheless, our ability to update this vast knowledge store, an essential part of any long-term memory system, is still surprisingly limited. Although many techniques to update LLMs exist, such as standard fine-tuning, model unlearning and model editing [12,37,38,39,40,76], it is clear that no methodology has emerged as a robust solution for continual learning in LLMs [20, 35, 78].\nRAG as Long-Term Memory. On the other hand, using RAG methods as a long-term memory system offers a simple way to update knowledge over time [28, 33, 50, 56]. More sophisticated RAG methods, which perform multiple steps of retrieval and generation from an LLM, are even able to integrate information across new or updated knowledge elements[30, 49, 55, 61, 69, 71, 73], another crucial aspect of long-term memory systems. As discussed above, however, this type of online information integration is unable to solve the more complex knowledge integration tasks that we illustrate with our path-finding multi-hop QA examples.\n\nSome other methods, such as RAPTOR [54], MemWalker [7] and GraphRAG [14], integrate information during the offline indexing phase similarly to HippoRAG and might be able to handle these more complex tasks. However, these methods integrate information by summarizing knowledge elements, which means that the summarization process must be repeated any time new data is added. In contrast, HippoRAG can continuously integrate new knowledge by simply adding edges to its KG.\nLong Context as Long-Term Memory. Context lengths for both open and closed source LLMs have increased dramatically in the past year $[9,13,16,46,51]$. This scaling trend seems to indicate that future LLMs could perform long-term memory storage within massive context windows. However, the viability of this future remains largely uncertain given the many engineering hurdles involved and the apparent limitations of long-context LLMs, even within current context lengths [32, 34, 77].\n\nLLMs \\& KGs. Combining the strengths of language models and knowledge graphs has been an active research direction for many years, both for augmenting LLMs with a KG in different ways [36, 63, 66] or augmenting KGs by either distilling knowledge from an LLM's parametric knowledge $[5,67]$ or using them to parse text directly $[6,22,75]$. In an exceptionally comprehensive survey, Pan et al. [43] present a roadmap for this research direction and highlight the importance of work which synergizes these two important technologies [29, 57, 72, 80]. Like these works, HippoRAG is a strong and principled example of the synergy we must strike between these two technologies, combining the power of LLMs for knowledge graph construction with the strengths of structured knowledge and graph search for improved augmentation of an LLM's capacities.\n\n7 Conclusions \\& Limitations\n\nOur proposed neurobiologically principled methodology, although simple, already shows promise for overcoming the inherent limitations of standard RAG systems while retaining their advantages over parametric memory. HippoRAG's knowledge integration capabilities, demonstrated by its strong results on path-following multi-hop QA and promise on path-finding multi-hop QA, as well as its dramatic efficiency improvements and continuously updating nature, makes it a powerful middleground framework between standard RAG methods and parametric memory and offers a compelling solution for long-term memory in LLMs.\n\nNevertheless, several limitations can be addressed in future work to enable HippoRAG to achieve this goal better. First, we note that all components of HippoRAG are currently used off-the-shelf without any extra training. There is therefore much room to improve our method's practical viability by performing specific component fine-tuning. This is evident in the error analysis discussed in Appendix",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "This scaling trend seems to indicate that future LLMs could perform long-term memory storage within massive context windows.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0439a866-bd97-4c72-8739-c31cdb99e18d",
        "questions": "In which district is Vila Franca de Xira located?",
        "answers": "Lisbon",
        "context": "Question \\& Answer\n\n\nQuestion In which district was Alhandra born?\nAnswer Lisbon\n\nSupporting Passages\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886, in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\nDistractor Passages (Excerpts)\n\n1. Chirakkalkulam\n\nChirakkalkulam is a small residential area near Kannur town of Kannur District, Kerala state, South India. Chirakkalkulam is located between Thayatheru and Kannur City. Chirakkalkulam's significance arises from the birth of the historic Arakkal Kingdom.\n2. Frank T. and Polly Lewis House\n\nThe Frank T. and Polly Lewis House is located in Lodi, Wisconsin, United States. It was added to the National Register of Historic Places in 2009. The house is located within the Portage Street Historic District.\n\n3. Birth certificate\n\nIn the U.S., the issuance of birth certificates is a function of the Vital Records Office of the states, capital district, territories and former territories ...\n\nFigure 3: HippoRAG Pipeline Example (Question and Annotations). (Top) We provide an example question and its answer. (Middle \\& Bottom) The supporting and distractor passages for this question. Two supporting passages are needed to solve this question. The excerpts of the distractor passages are related to the \"district\" mentioned in the question.\n\nA HippoRAG Pipeline Example\n\nTo better demonstrate how our HippoRAG pipeline works, we use the path-following example from the MuSiQue dataset shown in Table 6. We use HippoRAG's indexing and retrieval processes to follow this question and a subset of the associated corpus. The question, its answer, and its supporting and distractor passages are as shown in Figure 3. The indexing stage is shown in Figure 4, showing both the OpenIE procedure as well as the relevant subgraph of our KG. Finally, we illustrate the retrieval stage in Figure 5, including query NER, query node retrieval, how the PPR algorithm changes node probabilities, and how the top retrieval results are calculated.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Vila Franca de Xira is a municipality in the Lisbon District in Portugal.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "043d17e5-2f0e-4937-b4bb-2a5c61cc6889",
        "questions": "What is the population of Vila Franca de Xira as of 2011?",
        "answers": "136,886",
        "context": "Question \\& Answer\n\n\nQuestion In which district was Alhandra born?\nAnswer Lisbon\n\nSupporting Passages\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886, in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\nDistractor Passages (Excerpts)\n\n1. Chirakkalkulam\n\nChirakkalkulam is a small residential area near Kannur town of Kannur District, Kerala state, South India. Chirakkalkulam is located between Thayatheru and Kannur City. Chirakkalkulam's significance arises from the birth of the historic Arakkal Kingdom.\n2. Frank T. and Polly Lewis House\n\nThe Frank T. and Polly Lewis House is located in Lodi, Wisconsin, United States. It was added to the National Register of Historic Places in 2009. The house is located within the Portage Street Historic District.\n\n3. Birth certificate\n\nIn the U.S., the issuance of birth certificates is a function of the Vital Records Office of the states, capital district, territories and former territories ...\n\nFigure 3: HippoRAG Pipeline Example (Question and Annotations). (Top) We provide an example question and its answer. (Middle \\& Bottom) The supporting and distractor passages for this question. Two supporting passages are needed to solve this question. The excerpts of the distractor passages are related to the \"district\" mentioned in the question.\n\nA HippoRAG Pipeline Example\n\nTo better demonstrate how our HippoRAG pipeline works, we use the path-following example from the MuSiQue dataset shown in Table 6. We use HippoRAG's indexing and retrieval processes to follow this question and a subset of the associated corpus. The question, its answer, and its supporting and distractor passages are as shown in Figure 3. The indexing stage is shown in Figure 4, showing both the OpenIE procedure as well as the relevant subgraph of our KG. Finally, we illustrate the retrieval stage in Figure 5, including query NER, query node retrieval, how the PPR algorithm changes node probabilities, and how the top retrieval results are calculated.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The population in 2011 was 136,886, in an area of $318.19 \\mathrm{~km}^{2}$.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04408df3-2004-45b8-a0d8-336027812704",
        "questions": "Is Vila Franca de Xira situated on both banks of the Tagus River?",
        "answers": "Yes",
        "context": "Question \\& Answer\n\n\nQuestion In which district was Alhandra born?\nAnswer Lisbon\n\nSupporting Passages\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886, in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\nDistractor Passages (Excerpts)\n\n1. Chirakkalkulam\n\nChirakkalkulam is a small residential area near Kannur town of Kannur District, Kerala state, South India. Chirakkalkulam is located between Thayatheru and Kannur City. Chirakkalkulam's significance arises from the birth of the historic Arakkal Kingdom.\n2. Frank T. and Polly Lewis House\n\nThe Frank T. and Polly Lewis House is located in Lodi, Wisconsin, United States. It was added to the National Register of Historic Places in 2009. The house is located within the Portage Street Historic District.\n\n3. Birth certificate\n\nIn the U.S., the issuance of birth certificates is a function of the Vital Records Office of the states, capital district, territories and former territories ...\n\nFigure 3: HippoRAG Pipeline Example (Question and Annotations). (Top) We provide an example question and its answer. (Middle \\& Bottom) The supporting and distractor passages for this question. Two supporting passages are needed to solve this question. The excerpts of the distractor passages are related to the \"district\" mentioned in the question.\n\nA HippoRAG Pipeline Example\n\nTo better demonstrate how our HippoRAG pipeline works, we use the path-following example from the MuSiQue dataset shown in Table 6. We use HippoRAG's indexing and retrieval processes to follow this question and a subset of the associated corpus. The question, its answer, and its supporting and distractor passages are as shown in Figure 3. The indexing stage is shown in Figure 4, showing both the OpenIE procedure as well as the relevant subgraph of our KG. Finally, we illustrate the retrieval stage in Figure 5, including query NER, query node retrieval, how the PPR algorithm changes node probabilities, and how the top retrieval results are calculated.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "044ed322-8c98-4c49-b540-33b718ac3907",
        "questions": "What is the percentage of errors accounted for by ignoring contextual cues in the method's design?",
        "answers": "48%",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Unfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F.",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "045a1c04-f0ab-4022-b67f-1513d73d4e0c",
        "questions": "Which model performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines according to Table 12?",
        "answers": "HippoRAG",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Table 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "045abe0c-9f93-432a-9175-198fef88a6fd",
        "questions": "What is the R@5 performance of HippoRAG with ColBERTv2 on the 2Wiki dataset as shown in Table 12?",
        "answers": "89.1",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& ColBERTv2 & 40.9 & 51.9 & 70.7 & \\underline{89.1} & 60.5 & 77.7 & 57.4 & 72.9 \\",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0461d287-dde9-45d2-8dcb-9b62f9a847f5",
        "questions": "What is the name of India's first private FM radio station that started on 3 July 2001?",
        "answers": "Radio City",
        "context": "Passage NER (Indexing)\n\n\nInstruction:\n\nYour task is to extract named entities from the given paragraph.\nRespond with a JSON list of entities.\n\nOne-Shot Demonstration:\n```\nParagraph:\nRadio City\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English\nand regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music\nportal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related\nfeatures.\n``\n{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]}\n```\nInput:\nParagraph:\nPASSAGE TO INDEX\n\nFigure 7: Prompt for passage NER during indexing.\nInstruction:\nYou're a very effective entity extraction system. Please extract all named entities that are important for\nsolving the questions below. Place the named entities in JSON format.\nOne-Shot Demonstration:\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\n\\{\"named_entities\": [\"First for Women\", \"Arthur's Magazine\"]\\}\nInput:\nQuestion: QUERY TO INDEX\n\nFigure 8: Prompt for query NER during retrieval.\n\nI LLM Prompts\n\nThe prompts we used for indexing and query NER are shown in Figure 7 and Figure 8, while the OpenIE prompt is shown in Figure 9.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Radio City is India's first private FM radio station and was started on 3 July 2001.",
        "evidence_page_no": 26,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04640385-3e14-4555-b323-2c2a52428b6a",
        "questions": "In which month and year did Radio City launch its music portal PlanetRadiocity.com?",
        "answers": "May 2008",
        "context": "Passage NER (Indexing)\n\n\nInstruction:\n\nYour task is to extract named entities from the given paragraph.\nRespond with a JSON list of entities.\n\nOne-Shot Demonstration:\n```\nParagraph:\nRadio City\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English\nand regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music\nportal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related\nfeatures.\n``\n{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]}\n```\nInput:\nParagraph:\nPASSAGE TO INDEX\n\nFigure 7: Prompt for passage NER during indexing.\nInstruction:\nYou're a very effective entity extraction system. Please extract all named entities that are important for\nsolving the questions below. Place the named entities in JSON format.\nOne-Shot Demonstration:\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\n\\{\"named_entities\": [\"First for Women\", \"Arthur's Magazine\"]\\}\nInput:\nQuestion: QUERY TO INDEX\n\nFigure 8: Prompt for query NER during retrieval.\n\nI LLM Prompts\n\nThe prompts we used for indexing and query NER are shown in Figure 7 and Figure 8, while the OpenIE prompt is shown in Figure 9.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com.",
        "evidence_page_no": 26,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "046736c5-6cd9-41bb-9db4-50ccff730758",
        "questions": "Does Radio City play songs in multiple languages including Hindi and English?",
        "answers": "Yes",
        "context": "Passage NER (Indexing)\n\n\nInstruction:\n\nYour task is to extract named entities from the given paragraph.\nRespond with a JSON list of entities.\n\nOne-Shot Demonstration:\n```\nParagraph:\nRadio City\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English\nand regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music\nportal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related\nfeatures.\n``\n{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]}\n```\nInput:\nParagraph:\nPASSAGE TO INDEX\n\nFigure 7: Prompt for passage NER during indexing.\nInstruction:\nYou're a very effective entity extraction system. Please extract all named entities that are important for\nsolving the questions below. Place the named entities in JSON format.\nOne-Shot Demonstration:\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\n\\{\"named_entities\": [\"First for Women\", \"Arthur's Magazine\"]\\}\nInput:\nQuestion: QUERY TO INDEX\n\nFigure 8: Prompt for query NER during retrieval.\n\nI LLM Prompts\n\nThe prompts we used for indexing and query NER are shown in Figure 7 and Figure 8, while the OpenIE prompt is shown in Figure 9.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "It plays Hindi, English and regional songs.",
        "evidence_page_no": 26,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "047cd56e-fd6c-4364-8191-cbeb175bf448",
        "questions": "What is the name of India's first private FM radio station that started on 3 July 2001?",
        "answers": "Radio City",
        "context": "Open Information Extraction\n\n\nInstruction:\n\nYour task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists.\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph.\nPay attention to the following requirements:\n- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n- Clearly resolve pronouns to their specific names to maintain clarity.\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list.\n\nOne-Shot Demonstration:\n\nParagraph:\n\nRadio City\n\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English and regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n\u2025\n\\{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]\\}\n\\{\"triples\":\n[\n[\"Radio City\", \"located in\", \"India\"],\n[\"Radio City\", \"is\", \"private FM radio station\"],\n[\"Radio City\", \"started on\", \"3 July 2001\"],\n[\"Radio City\", \"plays songs in\", \"Hindi\"],\n[\"Radio City\", \"plays songs in\", \"English\"],\n[\"Radio City\", \"forayed into\", \"New Media\"],\n[\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n[\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n[\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n[\"PlanetRadiocity.com\", \"offers\", \"news\"],\n[\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n[\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n]\n\\}\n\nInput:\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list. Paragraph:\n\nPASSAGE TO INDEX\n$\\{$ \"named_entities\": [NER LIST]\\}\n\nFigure 9: Prompt for OpenIE during indexing.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Radio City is India's first private FM radio station and was started on 3 July 2001.",
        "evidence_page_no": 27,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0482d7b1-0b5b-4161-9eda-995b78c0aefd",
        "questions": "In which month and year did Radio City foray into New Media with the launch of PlanetRadiocity.com?",
        "answers": "May 2008",
        "context": "Open Information Extraction\n\n\nInstruction:\n\nYour task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists.\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph.\nPay attention to the following requirements:\n- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n- Clearly resolve pronouns to their specific names to maintain clarity.\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list.\n\nOne-Shot Demonstration:\n\nParagraph:\n\nRadio City\n\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English and regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n\u2025\n\\{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]\\}\n\\{\"triples\":\n[\n[\"Radio City\", \"located in\", \"India\"],\n[\"Radio City\", \"is\", \"private FM radio station\"],\n[\"Radio City\", \"started on\", \"3 July 2001\"],\n[\"Radio City\", \"plays songs in\", \"Hindi\"],\n[\"Radio City\", \"plays songs in\", \"English\"],\n[\"Radio City\", \"forayed into\", \"New Media\"],\n[\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n[\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n[\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n[\"PlanetRadiocity.com\", \"offers\", \"news\"],\n[\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n[\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n]\n\\}\n\nInput:\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list. Paragraph:\n\nPASSAGE TO INDEX\n$\\{$ \"named_entities\": [NER LIST]\\}\n\nFigure 9: Prompt for OpenIE during indexing.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.",
        "evidence_page_no": 27,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0487ff4a-7967-4ae8-912e-368d633344fb",
        "questions": "Does PlanetRadiocity.com offer music-related news as one of its features?",
        "answers": "Yes",
        "context": "Open Information Extraction\n\n\nInstruction:\n\nYour task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists.\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph.\nPay attention to the following requirements:\n- Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\n- Clearly resolve pronouns to their specific names to maintain clarity.\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list.\n\nOne-Shot Demonstration:\n\nParagraph:\n\nRadio City\n\nRadio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English and regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.\n\u2025\n\\{\"named_entities\": [\"Radio City\", \"India\", \"3 July 2001\", \"Hindi\",\"English\", \"May 2008\",\n\"PlanetRadiocity.com\"]\\}\n\\{\"triples\":\n[\n[\"Radio City\", \"located in\", \"India\"],\n[\"Radio City\", \"is\", \"private FM radio station\"],\n[\"Radio City\", \"started on\", \"3 July 2001\"],\n[\"Radio City\", \"plays songs in\", \"Hindi\"],\n[\"Radio City\", \"plays songs in\", \"English\"],\n[\"Radio City\", \"forayed into\", \"New Media\"],\n[\"Radio City\", \"launched\", \"PlanetRadiocity.com\"],\n[\"PlanetRadiocity.com\", \"launched in\", \"May 2008\"],\n[\"PlanetRadiocity.com\", \"is\", \"music portal\"],\n[\"PlanetRadiocity.com\", \"offers\", \"news\"],\n[\"PlanetRadiocity.com\", \"offers\", \"videos\"],\n[\"PlanetRadiocity.com\", \"offers\", \"songs\"]\n]\n\\}\n\nInput:\n\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list. Paragraph:\n\nPASSAGE TO INDEX\n$\\{$ \"named_entities\": [NER LIST]\\}\n\nFigure 9: Prompt for OpenIE during indexing.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.",
        "evidence_page_no": 27,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "048a07c4-2e08-4509-a5d2-73d0f1e0d225",
        "questions": "What is the API cost for using HippoRAG for online retrieval on 1,000 queries?",
        "answers": "0.1",
        "context": "Table 15: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries.\n\\begin{tabular}{lccc}\n  & ColBERTv2 & IRCoT & HippoRAG \\\\\n  API Cost (\\$) & 0 & $1-3$ & 0.1 \\\\\nTime (minutes) & 1 & $20-40$ & 3 \\\\\n \n\\end{tabular}\n\nAlthough offline indexing time and costs are higher for HippoRAG than IRCoT\u2014around 10 times slower and $\\$ 15$ more expensive for every 10,000 passages ${ }^{7}$, these costs can be dramatically reduced by leveraging open source LLMs. As shown in our ablation study in Table 5 Llama-3-8B-Instruct [1] performs similarly to GPT-3.5 Turbo without any specific prompt tuning. As seen in Table 16, this 8B Llama-3 model is about half the cost as GPT-3.5 Turbo when using an LLM deploying service like Together $\\mathrm{AI}^{8}$. Additionally, since these costs could be even further reduced by locally deploying this model, the barriers for using HippoRAG at scale could be well within the computational budget of many organizations. Finally, we note that even if LLM generation cost drops significantly, the online retrieval efficiency gains discussed above remain intact given that the number of tokens required for IRCoT vs. HippoRAG stay constant and LLM use is likely to also remain the system's main computational bottleneck.\n\nTable 16: Average cost and latency measurements for offline indexing using GPT-3.5 Turbo and Llama-3 (8B and 70B) through the TogetherAI API on 10,000 passages.\n\\begin{tabular}{lcccc}\n  Model & Metric & ColBERTv2 & IRCoT & HippoRAG \\\\\n  GPT-3.5 Turbo-1106 (Main Results) & API Cost (\\$) & 0 & 0 & 15 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  GPT-3.5 Turbo & API Cost (\\$) & 0 & 0 & 8 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  Llama-3-8B-Instruct & API Cost (\\$) & 0 & 0 & 3 \\\\\n& Time (minutes) & 7 & 7 & 45 \\\\\n  Llama-3-70B-Instruct & API Cost (\\$) & 0 & 0 & 7 \\\\\n& Time (minutes) & 7 & 7 & 100 \\\\\n \n\\end{tabular}\n\n\nH Implementation Details \\& Compute Requirements\n\n\nApart from the details included in \u00a73.4, we use implementations based on PyTorch [44] and HuggingFace [68] for both Contriever [27] and ColBERTv2 [53]. We use the python-igraph [11] implementation of the PPR algorithm. For BM25, we employ Elastic Search [18]. For multi-step retrieval, we use the same prompt implementation as IRCoT [61] and retrieve the top-10 passages at each step. We set the maximum number of reasoning steps to 2 for HotpotQA and 2WikiMultiHopQA and 4 for MuSiQue due to their maximum reasoning chain length. We combine IRCoT with different retrievers by replacing its base retriever BM25 with each retrieval method, including HippoRAG, noted as \"IRCoT + HippoRAG\" below. ${ }^{9}$ For the QA reader, we use top-5 retrieved passages as the context and 1-shot QA demonstration with CoT prompting strategy [61].\n\nIn terms of compute requirements, most of our compute requirements are unfortunately not disclosed by the OpenAI and TogetherAI APIs. We run ColBERTv2 and Contriever for indexing and retrieval we use 4 NVIDIA RTX A6000 GPUs with 48GB of memory. Finally, we used 2 AMD EPYC 7513 32-Core Processors to run the Personalized PageRank algorithm.\n\n\\footnotetext{\n${ }^{7}$ To speed up indexing, we use 10 threads querying gpt-3.5-turbo-1106 through the OpenAI API in parallel. At the time of writing, the cost of the API is $\\$ 1$ for a million input tokens and $\\$ 2$ for a million output tokens.\n${ }^{8}$ https://www.together.ai/products\\#inference\n${ }^{9}$ Since the original IRCoT does not provide a score for each retrieved passage, we employ beam search for the iterative retrieval process. Each candidate passage maintains the highest historical score during beam search.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Table 15: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries. \n\\begin{tabular}{lccc} \n\\hline & ColBERTv2 & IRCoT & HippoRAG \\\\ \n\\hline API Cost (\\$) & 0 & $1-3$ & 0.1 \\\\ \nTime (minutes) & 1 & $20-40$ & 3 \\\\ \n\\hline \n\\end{tabular}",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "049167c9-b519-4f8e-85f8-7f5c71347c31",
        "questions": "How much more expensive is offline indexing for HippoRAG compared to IRCoT for every 10,000 passages?",
        "answers": "$15",
        "context": "Table 15: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries.\n\\begin{tabular}{lccc}\n  & ColBERTv2 & IRCoT & HippoRAG \\\\\n  API Cost (\\$) & 0 & $1-3$ & 0.1 \\\\\nTime (minutes) & 1 & $20-40$ & 3 \\\\\n \n\\end{tabular}\n\nAlthough offline indexing time and costs are higher for HippoRAG than IRCoT\u2014around 10 times slower and $\\$ 15$ more expensive for every 10,000 passages ${ }^{7}$, these costs can be dramatically reduced by leveraging open source LLMs. As shown in our ablation study in Table 5 Llama-3-8B-Instruct [1] performs similarly to GPT-3.5 Turbo without any specific prompt tuning. As seen in Table 16, this 8B Llama-3 model is about half the cost as GPT-3.5 Turbo when using an LLM deploying service like Together $\\mathrm{AI}^{8}$. Additionally, since these costs could be even further reduced by locally deploying this model, the barriers for using HippoRAG at scale could be well within the computational budget of many organizations. Finally, we note that even if LLM generation cost drops significantly, the online retrieval efficiency gains discussed above remain intact given that the number of tokens required for IRCoT vs. HippoRAG stay constant and LLM use is likely to also remain the system's main computational bottleneck.\n\nTable 16: Average cost and latency measurements for offline indexing using GPT-3.5 Turbo and Llama-3 (8B and 70B) through the TogetherAI API on 10,000 passages.\n\\begin{tabular}{lcccc}\n  Model & Metric & ColBERTv2 & IRCoT & HippoRAG \\\\\n  GPT-3.5 Turbo-1106 (Main Results) & API Cost (\\$) & 0 & 0 & 15 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  GPT-3.5 Turbo & API Cost (\\$) & 0 & 0 & 8 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  Llama-3-8B-Instruct & API Cost (\\$) & 0 & 0 & 3 \\\\\n& Time (minutes) & 7 & 7 & 45 \\\\\n  Llama-3-70B-Instruct & API Cost (\\$) & 0 & 0 & 7 \\\\\n& Time (minutes) & 7 & 7 & 100 \\\\\n \n\\end{tabular}\n\n\nH Implementation Details \\& Compute Requirements\n\n\nApart from the details included in \u00a73.4, we use implementations based on PyTorch [44] and HuggingFace [68] for both Contriever [27] and ColBERTv2 [53]. We use the python-igraph [11] implementation of the PPR algorithm. For BM25, we employ Elastic Search [18]. For multi-step retrieval, we use the same prompt implementation as IRCoT [61] and retrieve the top-10 passages at each step. We set the maximum number of reasoning steps to 2 for HotpotQA and 2WikiMultiHopQA and 4 for MuSiQue due to their maximum reasoning chain length. We combine IRCoT with different retrievers by replacing its base retriever BM25 with each retrieval method, including HippoRAG, noted as \"IRCoT + HippoRAG\" below. ${ }^{9}$ For the QA reader, we use top-5 retrieved passages as the context and 1-shot QA demonstration with CoT prompting strategy [61].\n\nIn terms of compute requirements, most of our compute requirements are unfortunately not disclosed by the OpenAI and TogetherAI APIs. We run ColBERTv2 and Contriever for indexing and retrieval we use 4 NVIDIA RTX A6000 GPUs with 48GB of memory. Finally, we used 2 AMD EPYC 7513 32-Core Processors to run the Personalized PageRank algorithm.\n\n\\footnotetext{\n${ }^{7}$ To speed up indexing, we use 10 threads querying gpt-3.5-turbo-1106 through the OpenAI API in parallel. At the time of writing, the cost of the API is $\\$ 1$ for a million input tokens and $\\$ 2$ for a million output tokens.\n${ }^{8}$ https://www.together.ai/products\\#inference\n${ }^{9}$ Since the original IRCoT does not provide a score for each retrieved passage, we employ beam search for the iterative retrieval process. Each candidate passage maintains the highest historical score during beam search.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Although offline indexing time and costs are higher for HippoRAG than IRCoT\u2014around 10 times slower and $\\$ 15$ more expensive for every 10,000 passages ${ }^{7}$, these costs can be dramatically reduced by leveraging open source LLMs.",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04937838-ab1d-44e3-86ce-8b0389aca0da",
        "questions": "What is the time taken in minutes for offline indexing using Llama-3-70B-Instruct through the TogetherAI API on 10,000 passages?",
        "answers": "100",
        "context": "Table 15: Average cost and efficiency measurements for online retrieval using GPT-3.5 Turbo on 1,000 queries.\n\\begin{tabular}{lccc}\n  & ColBERTv2 & IRCoT & HippoRAG \\\\\n  API Cost (\\$) & 0 & $1-3$ & 0.1 \\\\\nTime (minutes) & 1 & $20-40$ & 3 \\\\\n \n\\end{tabular}\n\nAlthough offline indexing time and costs are higher for HippoRAG than IRCoT\u2014around 10 times slower and $\\$ 15$ more expensive for every 10,000 passages ${ }^{7}$, these costs can be dramatically reduced by leveraging open source LLMs. As shown in our ablation study in Table 5 Llama-3-8B-Instruct [1] performs similarly to GPT-3.5 Turbo without any specific prompt tuning. As seen in Table 16, this 8B Llama-3 model is about half the cost as GPT-3.5 Turbo when using an LLM deploying service like Together $\\mathrm{AI}^{8}$. Additionally, since these costs could be even further reduced by locally deploying this model, the barriers for using HippoRAG at scale could be well within the computational budget of many organizations. Finally, we note that even if LLM generation cost drops significantly, the online retrieval efficiency gains discussed above remain intact given that the number of tokens required for IRCoT vs. HippoRAG stay constant and LLM use is likely to also remain the system's main computational bottleneck.\n\nTable 16: Average cost and latency measurements for offline indexing using GPT-3.5 Turbo and Llama-3 (8B and 70B) through the TogetherAI API on 10,000 passages.\n\\begin{tabular}{lcccc}\n  Model & Metric & ColBERTv2 & IRCoT & HippoRAG \\\\\n  GPT-3.5 Turbo-1106 (Main Results) & API Cost (\\$) & 0 & 0 & 15 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  GPT-3.5 Turbo & API Cost (\\$) & 0 & 0 & 8 \\\\\n& Time (minutes) & 7 & 7 & 60 \\\\\n  Llama-3-8B-Instruct & API Cost (\\$) & 0 & 0 & 3 \\\\\n& Time (minutes) & 7 & 7 & 45 \\\\\n  Llama-3-70B-Instruct & API Cost (\\$) & 0 & 0 & 7 \\\\\n& Time (minutes) & 7 & 7 & 100 \\\\\n \n\\end{tabular}\n\n\nH Implementation Details \\& Compute Requirements\n\n\nApart from the details included in \u00a73.4, we use implementations based on PyTorch [44] and HuggingFace [68] for both Contriever [27] and ColBERTv2 [53]. We use the python-igraph [11] implementation of the PPR algorithm. For BM25, we employ Elastic Search [18]. For multi-step retrieval, we use the same prompt implementation as IRCoT [61] and retrieve the top-10 passages at each step. We set the maximum number of reasoning steps to 2 for HotpotQA and 2WikiMultiHopQA and 4 for MuSiQue due to their maximum reasoning chain length. We combine IRCoT with different retrievers by replacing its base retriever BM25 with each retrieval method, including HippoRAG, noted as \"IRCoT + HippoRAG\" below. ${ }^{9}$ For the QA reader, we use top-5 retrieved passages as the context and 1-shot QA demonstration with CoT prompting strategy [61].\n\nIn terms of compute requirements, most of our compute requirements are unfortunately not disclosed by the OpenAI and TogetherAI APIs. We run ColBERTv2 and Contriever for indexing and retrieval we use 4 NVIDIA RTX A6000 GPUs with 48GB of memory. Finally, we used 2 AMD EPYC 7513 32-Core Processors to run the Personalized PageRank algorithm.\n\n\\footnotetext{\n${ }^{7}$ To speed up indexing, we use 10 threads querying gpt-3.5-turbo-1106 through the OpenAI API in parallel. At the time of writing, the cost of the API is $\\$ 1$ for a million input tokens and $\\$ 2$ for a million output tokens.\n${ }^{8}$ https://www.together.ai/products\\#inference\n${ }^{9}$ Since the original IRCoT does not provide a score for each retrieved passage, we employ beam search for the iterative retrieval process. Each candidate passage maintains the highest historical score during beam search.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Table 16: Average cost and latency measurements for offline indexing using GPT-3.5 Turbo and Llama-3 (8B and 70B) through the TogetherAI API on 10,000 passages. \n\\begin{tabular}{lcccc} \n\\hline Model & Metric & ColBERTv2 & IRCoT & HippoRAG \\\\ \n\\hline GPT-3.5 Turbo-1106 (Main Results) & API Cost (\\$) & 0 & 0 & 15 \\\\ \n& Time (minutes) & 7 & 7 & 60 \\\\ \n\\hline GPT-3.5 Turbo & API Cost (\\$) & 0 & 0 & 8 \\\\ \n& Time (minutes) & 7 & 7 & 60 \\\\ \n\\hline Llama-3-8B-Instruct & API Cost (\\$) & 0 & 0 & 3 \\\\ \n& Time (minutes) & 7 & 7 & 45 \\\\ \n\\hline Llama-3-70B-Instruct & API Cost (\\$) & 0 & 0 & 7 \\\\ \n& Time (minutes) & 7 & 7 & 100 \\\\ \n\\hline \n\\end{tabular}",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0495c348-a87c-4e70-8a1f-9e07e8510977",
        "questions": "What is the primary function of the hippocampus in the hippocampal memory indexing theory proposed by Teyler and Discenna?",
        "answers": "The hippocampus is responsible for pattern separation and pattern completion.",
        "context": "with IRCoT to provide complementary gains of up to $4 \\%$ and $20 \\%$ on the same datasets and even obtain improvements on HotpotQA, a less challenging multi-hop QA dataset. Finally, we provide a case study illustrating the limitations of current methods as well as our method's potential on the previously discussed path-finding multi-hop QA setting.\n\n\n2 HippoRAG\n\n\nIn this section, we first give a brief overview of the hippocampal memory indexing theory, followed by how HippoRAG's indexing and retrieval design was inspired by this theory, and finally offer a more detailed account of our methodology.\n\n2.1 The Hippocampal Memory Indexing Theory\n\nThe hippocampal memory indexing theory [58] is a well-established theory that provides a functional description of the components and circuitry involved in human long-term memory. In this theory, Teyler and Discenna [58] propose that human long-term memory is composed of three components that work together to accomplish two main objectives: pattern separation, which ensures that the representations of distinct perceptual experiences are unique, and pattern completion, which enables the retrieval of complete memories from partial stimuli $[15,59]$.\n\nThe theory suggests that pattern separation is primarily accomplished in the memory encoding process, which starts with the neocortex receiving and processing perceptual stimuli into more easily manipulatable, likely higher-level, features, which are then routed through the parahippocampal regions (PHR) to be indexed by the hippocampus. When they reach the hippocampus, salient signals are included in the hippocampal index and associated with each other.\nAfter the memory encoding process is completed, pattern completion drives the memory retrieval process whenever the hippocampus receives partial perceptual signals from the PHR pipeline. The hippocampus then leverages its context-dependent memory system, thought to be implemented through a densely connected network of neurons in the CA3 sub-region [59], to identify complete and relevant memories within the hippocampal index and route them back through the PHR for simulation in the neocortex. Thus, this complex process allows for new information to be integrated by changing only the hippocampal index instead of updating neocortical representations.\n\n2.2 Overview\n\nOur proposed approach, HippoRAG, is closely inspired by the process described above. As shown in Figure 2, each component of our method corresponds to one of the three components of human long-term memory. A detailed example of the HippoRAG process can be found in Appendix A.\nOffline Indexing. Our offline indexing phase, analogous to memory encoding, starts by leveraging a strong instruction-tuned $\\mathbf{L L M}$, our artificial neocortex, to extract knowledge graph (KG) triples. The KG is schemaless and this process is known as open information extraction (OpenIE) [3, 4, 45, 79]. This process extracts salient signals from passages in a retrieval corpus as discrete noun phrases rather than dense vector representations, allowing for more fine-grained pattern separation. It is therefore natural to define our artificial hippocampal index as this open KG, which is built on the whole retrieval corpus passage-by-passage. Finally, to connect both components as is done by the parahippocampal regions, we use off-the-shelf dense encoders fine-tuned for retrieval (retrieval encoders). These retrieval encoders provide additional edges between similar but not identical noun phrases within this KG to aid in downstream pattern completion.\nOnline Retrieval. These same three components are then leveraged to perform online retrieval by mirroring the human brain's memory retrieval process. Just as the hippocampus receives input processed through the neocortex and PHR, our LLM-based neocortex extracts a set of salient named entities from a query which we call query named entities. These named entities are then linked to nodes in our KG based on the similarity determined by retrieval encoders; we refer to these selected nodes as query nodes. Once the query nodes are chosen, they become the partial cues from which our synthetic hippocampus performs pattern completion. In the hippocampus, neural pathways between elements of the hippocampal index enable relevant neighborhoods to become activated and recalled upstream. To imitate this efficient graph search process, we leverage the Personalized PageRank",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this theory, Teyler and Discenna [58] propose that human long-term memory is composed of three components that work together to accomplish two main objectives: pattern separation, which ensures that the representations of distinct perceptual experiences are unique, and pattern completion, which enables the retrieval of complete memories from partial stimuli $[15,59]$.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04a2c02e-1daa-4afc-b732-85da35ca1135",
        "questions": "How does HippoRAG's offline indexing phase relate to the human memory encoding process?",
        "answers": "HippoRAG's offline indexing phase is analogous to memory encoding, starting by leveraging a strong instruction-tuned LLM, our artificial neocortex, to extract knowledge graph (KG) triples.",
        "context": "with IRCoT to provide complementary gains of up to $4 \\%$ and $20 \\%$ on the same datasets and even obtain improvements on HotpotQA, a less challenging multi-hop QA dataset. Finally, we provide a case study illustrating the limitations of current methods as well as our method's potential on the previously discussed path-finding multi-hop QA setting.\n\n\n2 HippoRAG\n\n\nIn this section, we first give a brief overview of the hippocampal memory indexing theory, followed by how HippoRAG's indexing and retrieval design was inspired by this theory, and finally offer a more detailed account of our methodology.\n\n2.1 The Hippocampal Memory Indexing Theory\n\nThe hippocampal memory indexing theory [58] is a well-established theory that provides a functional description of the components and circuitry involved in human long-term memory. In this theory, Teyler and Discenna [58] propose that human long-term memory is composed of three components that work together to accomplish two main objectives: pattern separation, which ensures that the representations of distinct perceptual experiences are unique, and pattern completion, which enables the retrieval of complete memories from partial stimuli $[15,59]$.\n\nThe theory suggests that pattern separation is primarily accomplished in the memory encoding process, which starts with the neocortex receiving and processing perceptual stimuli into more easily manipulatable, likely higher-level, features, which are then routed through the parahippocampal regions (PHR) to be indexed by the hippocampus. When they reach the hippocampus, salient signals are included in the hippocampal index and associated with each other.\nAfter the memory encoding process is completed, pattern completion drives the memory retrieval process whenever the hippocampus receives partial perceptual signals from the PHR pipeline. The hippocampus then leverages its context-dependent memory system, thought to be implemented through a densely connected network of neurons in the CA3 sub-region [59], to identify complete and relevant memories within the hippocampal index and route them back through the PHR for simulation in the neocortex. Thus, this complex process allows for new information to be integrated by changing only the hippocampal index instead of updating neocortical representations.\n\n2.2 Overview\n\nOur proposed approach, HippoRAG, is closely inspired by the process described above. As shown in Figure 2, each component of our method corresponds to one of the three components of human long-term memory. A detailed example of the HippoRAG process can be found in Appendix A.\nOffline Indexing. Our offline indexing phase, analogous to memory encoding, starts by leveraging a strong instruction-tuned $\\mathbf{L L M}$, our artificial neocortex, to extract knowledge graph (KG) triples. The KG is schemaless and this process is known as open information extraction (OpenIE) [3, 4, 45, 79]. This process extracts salient signals from passages in a retrieval corpus as discrete noun phrases rather than dense vector representations, allowing for more fine-grained pattern separation. It is therefore natural to define our artificial hippocampal index as this open KG, which is built on the whole retrieval corpus passage-by-passage. Finally, to connect both components as is done by the parahippocampal regions, we use off-the-shelf dense encoders fine-tuned for retrieval (retrieval encoders). These retrieval encoders provide additional edges between similar but not identical noun phrases within this KG to aid in downstream pattern completion.\nOnline Retrieval. These same three components are then leveraged to perform online retrieval by mirroring the human brain's memory retrieval process. Just as the hippocampus receives input processed through the neocortex and PHR, our LLM-based neocortex extracts a set of salient named entities from a query which we call query named entities. These named entities are then linked to nodes in our KG based on the similarity determined by retrieval encoders; we refer to these selected nodes as query nodes. Once the query nodes are chosen, they become the partial cues from which our synthetic hippocampus performs pattern completion. In the hippocampus, neural pathways between elements of the hippocampal index enable relevant neighborhoods to become activated and recalled upstream. To imitate this efficient graph search process, we leverage the Personalized PageRank",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Our offline indexing phase, analogous to memory encoding, starts by leveraging a strong instruction-tuned $\\mathbf{L L M}$, our artificial neocortex, to extract knowledge graph (KG) triples.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04a3e176-2871-468c-a7f2-0af9cbacc159",
        "questions": "What method does HippoRAG use to perform pattern completion during online retrieval, and how is it inspired by the human brain's memory retrieval process?",
        "answers": "HippoRAG uses Personalized PageRank to imitate the efficient graph search process of the hippocampus, which performs pattern completion by activating and recalling relevant neighborhoods upstream.",
        "context": "with IRCoT to provide complementary gains of up to $4 \\%$ and $20 \\%$ on the same datasets and even obtain improvements on HotpotQA, a less challenging multi-hop QA dataset. Finally, we provide a case study illustrating the limitations of current methods as well as our method's potential on the previously discussed path-finding multi-hop QA setting.\n\n\n2 HippoRAG\n\n\nIn this section, we first give a brief overview of the hippocampal memory indexing theory, followed by how HippoRAG's indexing and retrieval design was inspired by this theory, and finally offer a more detailed account of our methodology.\n\n2.1 The Hippocampal Memory Indexing Theory\n\nThe hippocampal memory indexing theory [58] is a well-established theory that provides a functional description of the components and circuitry involved in human long-term memory. In this theory, Teyler and Discenna [58] propose that human long-term memory is composed of three components that work together to accomplish two main objectives: pattern separation, which ensures that the representations of distinct perceptual experiences are unique, and pattern completion, which enables the retrieval of complete memories from partial stimuli $[15,59]$.\n\nThe theory suggests that pattern separation is primarily accomplished in the memory encoding process, which starts with the neocortex receiving and processing perceptual stimuli into more easily manipulatable, likely higher-level, features, which are then routed through the parahippocampal regions (PHR) to be indexed by the hippocampus. When they reach the hippocampus, salient signals are included in the hippocampal index and associated with each other.\nAfter the memory encoding process is completed, pattern completion drives the memory retrieval process whenever the hippocampus receives partial perceptual signals from the PHR pipeline. The hippocampus then leverages its context-dependent memory system, thought to be implemented through a densely connected network of neurons in the CA3 sub-region [59], to identify complete and relevant memories within the hippocampal index and route them back through the PHR for simulation in the neocortex. Thus, this complex process allows for new information to be integrated by changing only the hippocampal index instead of updating neocortical representations.\n\n2.2 Overview\n\nOur proposed approach, HippoRAG, is closely inspired by the process described above. As shown in Figure 2, each component of our method corresponds to one of the three components of human long-term memory. A detailed example of the HippoRAG process can be found in Appendix A.\nOffline Indexing. Our offline indexing phase, analogous to memory encoding, starts by leveraging a strong instruction-tuned $\\mathbf{L L M}$, our artificial neocortex, to extract knowledge graph (KG) triples. The KG is schemaless and this process is known as open information extraction (OpenIE) [3, 4, 45, 79]. This process extracts salient signals from passages in a retrieval corpus as discrete noun phrases rather than dense vector representations, allowing for more fine-grained pattern separation. It is therefore natural to define our artificial hippocampal index as this open KG, which is built on the whole retrieval corpus passage-by-passage. Finally, to connect both components as is done by the parahippocampal regions, we use off-the-shelf dense encoders fine-tuned for retrieval (retrieval encoders). These retrieval encoders provide additional edges between similar but not identical noun phrases within this KG to aid in downstream pattern completion.\nOnline Retrieval. These same three components are then leveraged to perform online retrieval by mirroring the human brain's memory retrieval process. Just as the hippocampus receives input processed through the neocortex and PHR, our LLM-based neocortex extracts a set of salient named entities from a query which we call query named entities. These named entities are then linked to nodes in our KG based on the similarity determined by retrieval encoders; we refer to these selected nodes as query nodes. Once the query nodes are chosen, they become the partial cues from which our synthetic hippocampus performs pattern completion. In the hippocampus, neural pathways between elements of the hippocampal index enable relevant neighborhoods to become activated and recalled upstream. To imitate this efficient graph search process, we leverage the Personalized PageRank",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "To imitate this efficient graph search process, we leverage the Personalized PageRank.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04a4bb91-83d6-45f7-9654-3df09e5b9e4a",
        "questions": "What is the title of the work by Y. Chen, P. Cao, Y. Chen, K. Liu, and J. Zhao published in the Proceedings of the AAAI Conference on Artificial Intelligence in March 2024?",
        "answers": "Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons",
        "context": "[10] Y. Chen, P. Cao, Y. Chen, K. Liu, and J. Zhao. Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17817-17825, Mar. 2024. doi: 10.1609/aaai.v38i16.29735. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29735.\n[11] G. Cs\u00e1rdi and T. Nepusz. The igraph software package for complex network research. 2006. URL https://igraph.org/.\n[12] N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language models. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.522. URL https://aclanthology .org/2021.emnlp-main. 522.\n[13] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope: Extending llm context window beyond 2 million tokens. ArXiv, abs/2402.13753, 2024. URL https://api.semanticscholar.org/CorpusID:267770308.\n[14] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson. From local to global: A graph rag approach to query-focused summarization. 2024. URL https: //arxiv.org/abs/2404.16130.\n[15] H. Eichenbaum. A cortical-hippocampal system for declarative memory. Nature Reviews Neuroscience, 1:41-50, 2000. URL https://www.nature.com/articles/35036213.\n[16] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for scaling language models to 128 k context, 2024.\n[17] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12216-12235. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.751. URL https://doi.org/10. 18653/v1/2023. emnlp-main. 751.\n[18] C. Gormley and Z. J. Tong. Elasticsearch: The definitive guide. 2015. URL https://www. elastic.co/guide/en/elasticsearch/guide/master/index.html.\n[19] T. L. Griffiths, M. Steyvers, and A. J. Firl. Google and the mind. Psychological Science, 18: 1069 - 1076, 2007. URL https://cocosci.princeton.edu/tom/papers/google.pdf.\n[20] J.-C. Gu, H.-X. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K.-W. Chang, and N. Peng. Model Editing Can Hurt General Abilities of Large Language Models, 2024.\n[21] W. Gurnee and M. Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=jE8xbmvFin.\n[22] J. Han, N. Collier, W. Buntine, and E. Shareghi. PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs, 2023.\n[23] T. H. Haveliwala. Topic-sensitive pagerank. In D. Lassner, D. D. Roure, and A. Iyengar, editors, Proceedings of the Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002, Honolulu, Hawaii, USA, pages 517-526. ACM, 2002. doi: 10.1145/511446.511513. URL https://dl.acm.org/doi/10.1145/511446.511513.\n[24] Q. He, Y. Wang, and W. Wang. Can language models act as knowledge bases at scale?, 2024.\n[25] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609-6625, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / 2020$.coling-main.580. URL https://aclanthology.org/ 2020.coling-main. 580.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Y. Chen, P. Cao, Y. Chen, K. Liu, and J. Zhao. Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17817-17825, Mar. 2024.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04aa72d1-4bc6-4ffd-8610-53ed451b1c8c",
        "questions": "In which year was the igraph software package for complex network research by G. Cs\u00e1rdi and T. Nepusz released?",
        "answers": "2006",
        "context": "[10] Y. Chen, P. Cao, Y. Chen, K. Liu, and J. Zhao. Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17817-17825, Mar. 2024. doi: 10.1609/aaai.v38i16.29735. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29735.\n[11] G. Cs\u00e1rdi and T. Nepusz. The igraph software package for complex network research. 2006. URL https://igraph.org/.\n[12] N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language models. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.522. URL https://aclanthology .org/2021.emnlp-main. 522.\n[13] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope: Extending llm context window beyond 2 million tokens. ArXiv, abs/2402.13753, 2024. URL https://api.semanticscholar.org/CorpusID:267770308.\n[14] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson. From local to global: A graph rag approach to query-focused summarization. 2024. URL https: //arxiv.org/abs/2404.16130.\n[15] H. Eichenbaum. A cortical-hippocampal system for declarative memory. Nature Reviews Neuroscience, 1:41-50, 2000. URL https://www.nature.com/articles/35036213.\n[16] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for scaling language models to 128 k context, 2024.\n[17] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12216-12235. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.751. URL https://doi.org/10. 18653/v1/2023. emnlp-main. 751.\n[18] C. Gormley and Z. J. Tong. Elasticsearch: The definitive guide. 2015. URL https://www. elastic.co/guide/en/elasticsearch/guide/master/index.html.\n[19] T. L. Griffiths, M. Steyvers, and A. J. Firl. Google and the mind. Psychological Science, 18: 1069 - 1076, 2007. URL https://cocosci.princeton.edu/tom/papers/google.pdf.\n[20] J.-C. Gu, H.-X. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K.-W. Chang, and N. Peng. Model Editing Can Hurt General Abilities of Large Language Models, 2024.\n[21] W. Gurnee and M. Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=jE8xbmvFin.\n[22] J. Han, N. Collier, W. Buntine, and E. Shareghi. PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs, 2023.\n[23] T. H. Haveliwala. Topic-sensitive pagerank. In D. Lassner, D. D. Roure, and A. Iyengar, editors, Proceedings of the Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002, Honolulu, Hawaii, USA, pages 517-526. ACM, 2002. doi: 10.1145/511446.511513. URL https://dl.acm.org/doi/10.1145/511446.511513.\n[24] Q. He, Y. Wang, and W. Wang. Can language models act as knowledge bases at scale?, 2024.\n[25] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609-6625, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / 2020$.coling-main.580. URL https://aclanthology.org/ 2020.coling-main. 580.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "G. Cs\u00e1rdi and T. Nepusz. The igraph software package for complex network research. 2006.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04ae5e3e-f0c5-4802-8474-4ac61df73276",
        "questions": "Does the work titled 'Model Editing Can Hurt General Abilities of Large Language Models' involve authors J.-C. Gu, H.-X. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K.-W. Chang, and N. Peng?",
        "answers": "Yes",
        "context": "[10] Y. Chen, P. Cao, Y. Chen, K. Liu, and J. Zhao. Journey to the center of the knowledge neurons: Discoveries of language-independent knowledge neurons and degenerate knowledge neurons. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17817-17825, Mar. 2024. doi: 10.1609/aaai.v38i16.29735. URL https://ojs.aaai.org/index.php/AAAI/ article/view/29735.\n[11] G. Cs\u00e1rdi and T. Nepusz. The igraph software package for complex network research. 2006. URL https://igraph.org/.\n[12] N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language models. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.522. URL https://aclanthology .org/2021.emnlp-main. 522.\n[13] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and M. Yang. Longrope: Extending llm context window beyond 2 million tokens. ArXiv, abs/2402.13753, 2024. URL https://api.semanticscholar.org/CorpusID:267770308.\n[14] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson. From local to global: A graph rag approach to query-focused summarization. 2024. URL https: //arxiv.org/abs/2404.16130.\n[15] H. Eichenbaum. A cortical-hippocampal system for declarative memory. Nature Reviews Neuroscience, 1:41-50, 2000. URL https://www.nature.com/articles/35036213.\n[16] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data engineering for scaling language models to 128 k context, 2024.\n[17] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12216-12235. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.751. URL https://doi.org/10. 18653/v1/2023. emnlp-main. 751.\n[18] C. Gormley and Z. J. Tong. Elasticsearch: The definitive guide. 2015. URL https://www. elastic.co/guide/en/elasticsearch/guide/master/index.html.\n[19] T. L. Griffiths, M. Steyvers, and A. J. Firl. Google and the mind. Psychological Science, 18: 1069 - 1076, 2007. URL https://cocosci.princeton.edu/tom/papers/google.pdf.\n[20] J.-C. Gu, H.-X. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K.-W. Chang, and N. Peng. Model Editing Can Hurt General Abilities of Large Language Models, 2024.\n[21] W. Gurnee and M. Tegmark. Language models represent space and time. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=jE8xbmvFin.\n[22] J. Han, N. Collier, W. Buntine, and E. Shareghi. PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs, 2023.\n[23] T. H. Haveliwala. Topic-sensitive pagerank. In D. Lassner, D. D. Roure, and A. Iyengar, editors, Proceedings of the Eleventh International World Wide Web Conference, WWW 2002, May 7-11, 2002, Honolulu, Hawaii, USA, pages 517-526. ACM, 2002. doi: 10.1145/511446.511513. URL https://dl.acm.org/doi/10.1145/511446.511513.\n[24] Q. He, Y. Wang, and W. Wang. Can language models act as knowledge bases at scale?, 2024.\n[25] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 6609-6625, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / 2020$.coling-main.580. URL https://aclanthology.org/ 2020.coling-main. 580.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "J.-C. Gu, H.-X. Xu, J.-Y. Ma, P. Lu, Z.-H. Ling, K.-W. Chang, and N. Peng. Model Editing Can Hurt General Abilities of Large Language Models, 2024.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04c3743b-d791-4017-b463-619acf7740f3",
        "questions": "What is a major advantage of HippoRAG over conventional RAG methods in multi-hop QA?",
        "answers": "Its ability to perform multi-hop retrieval in a single step.",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "A major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04c8ead7-5bf3-4980-bf15-e8a5b5812f88",
        "questions": "By how much does the gap between HippoRAG and ColBERTv2 increase on 2WikiMultiHopQA when using the top-5 passages?",
        "answers": "From 20% to 38%",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04d35f1f-6d09-4ec0-be51-894d4834bb3a",
        "questions": "Which Stanford professor is identified by HippoRAG as working on the neuroscience of Alzheimer's in the path-finding multi-hop question example?",
        "answers": "Thomas S\u00fcdhof",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04d6466e-085c-42f0-b19d-09dd9bb6cafb",
        "questions": "What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53?",
        "answers": "Chlorambucil",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important...",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04da7224-8591-4820-90ac-cfc9ec8dc105",
        "questions": "What percentage of errors in the MuSiQue dataset analysis were attributed to NER limitations?",
        "answers": "48",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Table 10: Error analysis on MuSiQue. \n\\begin{tabular}{lc} \n\\hline Error Type & Error Percentage (\\%) \n\\hline NER Limitation & 48 \\",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04e93690-0b4e-4fdc-8907-68b00530aee3",
        "questions": "Does the PPR algorithm sometimes fail to identify relevant passages in the graph correctly despite the correct functioning of NER and OpenIE?",
        "answers": "Yes",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly.",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04eabde4-2257-4ed6-93e7-2b095889c1b9",
        "questions": "What is the full name and birth date of the retired Portuguese footballer known as Alhandra?",
        "answers": "Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim, born 5 March 1979",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.",
        "evidence_page_no": 19,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04eef3ab-854c-4d41-aaee-afbe1840a1ee",
        "questions": "Which district is Vila Franca de Xira located in and what was its population in 2011?",
        "answers": "Lisbon District, 136,886",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Vila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886, in an area of $318.19 \\mathrm{~km}^{2}$.",
        "evidence_page_no": 19,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04ef5ca7-e196-4f7a-9d67-829c937a8c37",
        "questions": "What are the geographical coordinates of Dili, the capital and largest city of East Timor?",
        "answers": "8\u00b0 20\u2032 S 125\u00b0 20\u2032 E / 8.34\u00b0 S 125.34\u00b0 E",
        "context": "Retrieval: Query NER \\& Node Retrieval\n\n\n\nQuestion In which district was Alhandra born? \\\\ NER [\"Alhandra\"] \\\\ Node Retrieval $\\quad\\{$ \"Alhandra\": \"Alhandra\"\\\n}\n\nRetrieval: PPR\n\nNode Probabilities Changes by PPR\n\\begin{tabular}{llll} \nAlhandra & $1.000 \\Rightarrow \\boldsymbol{0 . 5 3 3}$ & 5 March 1979 & $0.000 \\Rightarrow 0.045$ \\\\\nVila Franca de Xira & $0.000 \\Rightarrow \\mathbf{0 . 0 5 4}$ & Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim & $0.000 \\Rightarrow 0.044$ \\\\\nLisbon & $0.000 \\Rightarrow 0.049$ & Portugal & $0.000 \\Rightarrow 0.009$ \\\\\nfootballer & $0.000 \\Rightarrow 0.047$ & Tagus River & $0.000 \\Rightarrow 0.007$ \\\\\nPortuguese & $0.000 \\Rightarrow 0.046$ & Jos\u00e9 Pinto Coelho & $0.000 \\Rightarrow 0.004$\n\\end{tabular}\n\nRetrieval: Top Results\n*Top-ranked nodes from PPR are highlighted.\n\n1. Alhandra (footballer)\n\nLu\u00eds Miguel Assun\u00e7\u00e3o Joaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra, is a Portuguese retired footballer who played mainly as a left back - he could also appear as a midfielder.\n\n2. Vila Franca de Xira\n\nVila Franca de Xira is a municipality in the Lisbon District in Portugal. The population in 2011 was 136,886 , in an area of $318.19 \\mathrm{~km}^{2}$. Situated on both banks of the Tagus River, 32 km north-east of the Portuguese capital Lisbon, settlement in the area dates back to neolithic times, as evidenced by findings in the Cave of Pedra Furada. Vila Franca de Xira is said to have been founded by French followers of Portugal's first king, Afonso Henriques, around 1200.\n\n3. Portugal\n\nPortuguese is the official language of Portugal. Portuguese is a Romance language that originated in what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the common language of the Galician and Portuguese people until the independence of Portugal. Particularly in the North of Portugal, there are still many similarities between the Galician culture and the Portuguese culture. Galicia is a consultative observer of the Community of Portuguese Language Countries. According to the Ethnologue of Languages, Portuguese and Spanish have a lexical similarity of $89 \\%$ - educated speakers of each language can communicate easily with one another.\n\n4. Huguenots\n\nThe first Huguenots to leave France sought freedom from persecution in Switzerland and the Netherlands ... A fort, named Fort Coligny, was built to protect them from attack from the Portuguese troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America. The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese threatened the prisoners with death if they did not convert to Catholicism ...\n\n5. East Timor\n\nDemocratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: ``Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34 \\ldots$\n\nFigure 5: HippoRAG Pipeline Example (Retrieval). For retrieval, the named entities in the query are extracted from the question (Top), after which the query nodes are chosen using a retrieval encoder. In this case, the name of the query named entity, \"Alhandra\", is equivalent to its KG node. (Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes. After PPR, the query node probability is distributed according to the subgraph in Figure 4, leading to some probability mass on the node \"Vila France de Xira\". (Bottom) These node probabilities are then summed over the passages they appear in to obtain the passage-level ranking. The top-ranked nodes after PPR are highlighted in the top-ranked passages.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Democratic Republic of Timor - Leste Rep\u00fablika Demokr\u00e1tika Tim\u00f3r Lorosa'e (Tetum) Rep\u00fablica Democr\u00e1tica de Timor - Leste (Portuguese) Flag Coat of arms Motto: Unidade, Ac\u00e7\u00e3o, Progresso (Portuguese) Unidade, Asaun, Progresu (Tetum) (English: \"Unity, Action, Progress \") Anthem: P\u00e1tria (Portuguese) (English:\" Fatherland\") Capital and largest city Dili $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ}$ $\\mathrm{E} /-8.34 ; 125.34$ Coordinates: $8^{\\circ} 20^{\\prime} \\mathrm{S} 125^{\\circ} 20^{\\prime} \\mathrm{E} / 8.34^{\\circ} \\mathrm{S} 125.34^{\\circ} \\mathrm{E} /-8.34 ; 125.34",
        "evidence_page_no": 19,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04f65b19-4e68-4ad2-b5d7-4923c983453c",
        "questions": "Which war film based on a non-fiction book was directed by someone famous in the science fiction and crime genres according to HippoRAG?",
        "answers": "Black Hawk Down",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "1. War Film \n2. Time de Zarn \n3. Outline of Sci-Fi \n4. Black Hawk Down",
        "evidence_page_no": 22,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "04f980a2-a168-47b0-ae2d-33bdb46c7a54",
        "questions": "What is the percentage of error attributed to the NER Limitation type on the MuSiQue dataset?",
        "answers": "48%",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "NER Limitation & 48",
        "evidence_page_no": 22,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "051266c2-d444-4d7f-995a-f0cfd16a2182",
        "questions": "Out of the three approaches mentioned in the table, which one lists Chlorambucil as the first drug used to treat chronic lymphocytic leukemia by interacting with cytosolic p53?",
        "answers": "HippoRAG",
        "context": "Finally, the third question is more similar to the motivating example in the main paper and shows the importance of this type of question in real-world domains. In this question, we ask for a drug used to treat lymphocytic leukemia through a specific mechanism (cytosolic p53 interaction). While HippoRAG is able to leverage the associations within the supporting passages to identify the Chlorambucil passage as the most important, ColBERTv2 and IRCoT are only able to extract passages associated with lymphocytic leukemia. Interestingly enough, IRCoT uses its parametric knowledge to guess that Venetoclax, which also treats leukemia, would do so through the relevant mechanism even though no passage in the curated dataset explicitly stated this.\n\nTable 9: Ranking result examples for different approaches on several path-finding multi-hop questions.\n\\begin{tabular}{|c|c|c|c|}\n  Question & HippoRAG & ColBERTv2 & IRCoT \\\\\n  Which book was published in 2012 by an English author who is a Whitbread Award winner? & \\begin{tabular}{l}\n1. Oranges Are Not the Only Fruit \\\\\n2. William Trevor Legacies \\\\\n3. Mark Haddon\n\\end{tabular} & \\begin{tabular}{l}\n1. World Book Club Prize winners \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar \\\\\nBlues (novel)\n\\end{tabular} & \\begin{tabular}{l}\n1. Kate Atkinson \\\\\n2. Leon Garfield Awards \\\\\n3. Twelve Bar Blues (novel)\n\\end{tabular} \\\\\n  Which war film based on a non fiction book was directed by someone famous in the science fiction and crime genres? & \\begin{tabular}{l}\n1. War Film \\\\\n2. Time de Zarn \\\\\n3. Outline of Sci-Fi \\\\\n4. Black Hawk \\\\\nDown\n\\end{tabular} & \\begin{tabular}{l}\n1. Paul Greengrass \\\\\n2. List of book-based war films \\\\\n3. Korean War Films \\\\\n4. All the King's \\\\\nMen Book\n\\end{tabular} & \\begin{tabular}{l}\n1. Ridley Scott \\\\\n2. Peter Hyams \\\\\n3. Paul Greengrass \\\\\n4. List of book-based war films\n\\end{tabular} \\\\\n  What drug is used to treat chronic lymphocytic leukemia by interacting with cytosolic p53? & \\begin{tabular}{l}\n1. Chlorambucil \\\\\n2. Lymphocytic leukemia \\\\\n3. Mosquito bite allergy\n\\end{tabular} & \\begin{tabular}{l}\n1. Lymphocytic leukemia \\\\\n2. Obinutuzumab \\\\\n3. Venetoclax\n\\end{tabular} & \\begin{tabular}{l}\n1. Venetoclax \\\\\n2. Lymphocytic leukemia \\\\\n3. Idelalisib\n\\end{tabular} \\\\\n \n\\end{tabular}\n\n\nF Error Analysis\n\n\nF. 1 Overview\n\nWe mainly analyze the errors by HippoRAG on the MuSiQue dataset. As shown in Table 10, these errors can be attributed to three types. The NER limitation is the main error type (nearly half), as NER may not extract enough information from the query for retrieval, e.g., for the question \"When was one internet browser's version of Windows 8 made accessible?\", only the phrase \"Windows 8 \" is extracted. Nothing about \"browser\" or \"accessibility\" is extracted for subsequent graph search.\n\nFor OpenIE errors, we discuss some examples in \u00a7F.3. Despite the correct functioning of NER and OpenIE, the PPR algorithm sometimes fails to identify relevant passages in the graph correctly. For instance, consider the question \"How many refugees emigrated to the European country where Huguenots felt a kinship for emigration?\". Despite the supporting passages being titled \"Huguenots\" as shown in the question, the PPR algorithm struggles due to multiple passages with similar topics within the corpus. Even when the term \"Huguenots\" is accurately extracted from both the question and the supporting passages, and the PPR algorithm initiates searches with the nodes labeled \"European\" and \"Huguenots\", it incorrectly prioritizes other passages containing \"European\" and \"Huguenots\" that do not actually support the question. This misdirection occurs because the algorithm does not effectively discriminate between passages with similar topics but different relevance to the question's context.\n\nTable 10: Error analysis on MuSiQue.\n\\begin{tabular}{lc}\n  Error Type & Error Percentage (\\%) \\\\\n  NER Limitation & 48 \\\\\nIncorrect/Missing OpenIE & 28 \\\\\nPPR & 24 \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "1. Chlorambucil \n2. Lymphocytic leukemia \n3. Mosquito bite allergy",
        "evidence_page_no": 22,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "05147fea-9644-4c04-bfec-7d978e13c327",
        "questions": "In Table 6 of the document, which named entities were retrieved by HippoRAG for the path-finding multi-hop question 'Which Stanford professor works on the neuroscience of Alzheimer's?'?",
        "answers": "1. Thomas S\u00fcdhof, 2. Karl Deisseroth, 3. Robert Sapolsky",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Which Stanford professor works on the neuroscience of Alzheimer's? & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\ & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\ & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "051dcf24-03f2-4d3e-b8e6-d3bddd298a57",
        "questions": "In which districts did Alhandra receive different retrieved answers from HippoRAG, ColBERTv2, and IRCoT for the multi-hop question 'In which district was Alhandra born?' as shown in Table 6 of the document?",
        "answers": "HippoRAG: Vila de Xira; ColBERTv2: Dimuthu Abayakoon; IRCoT: Vila de Xira",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "In which & \\multirow[b]{4}{*}{\begin{tabular}{l} 1. Alhandra \\ 2. Vila de Xira \\ 3. Portugal \\end{tabular}} & 1. Alhandra & 1. Alhandra \\ & district was & 2. Dimuthu & 2. Vila de Xira \\ & Alhandra & & Abayakoon & 3. P\u00f3voa de \\ & born? & & 3. Ja'ar & Santa Iria",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "052daad3-26c0-4984-8734-bb05788c31f6",
        "questions": "According to the results in Table 6 of the document, which professor received different retrieval results across all three methods (HippoRAG, ColBERTv2, and IRCoT) for the path-finding multi-hop question 'Which Stanford professor works on the neuroscience of Alzheimer's?'",
        "answers": "None of the three professors received different retrieval results across all three methods.",
        "context": "direct neighbors of each query node (row 6). First, we find that PPR is a much more effective method for including associations for retrieval on all three datasets compared to both simple baselines. It is interesting to note that adding the neighborhood of $R_{q}$ nodes without PPR leads to worse performance than only using the query nodes themselves.\nAblations. As seen in Table 5 (rows 7-8), node specificity obtains considerable improvements on MuSiQue and HotpotQA and yields almost no change in 2WikiMultiHopQA. This is likely because 2WikiMultiHopQA relies on named entities with little differences in terms of term weighting. In contrast, synonymy edges have the largest effect on 2WikiMultiHopQA, suggesting that noisy entity standardization is useful when most relevant concepts are named entities, and improvements to synonymy detection could lead to stronger performance in other datasets.\n\n\n5.2 HippoRAG's Advantage: Single-Step Multi-Hop Retrieval\n\n\nA major advantage of HippoRAG over conventional RAG methods in multi-hop QA is its ability to perform multi-hop retrieval in a single step. We demonstrate this by measuring the percentage of queries where all the supporting passages are retrieved successfully, a feat that can only be accomplished through successful multi-hop reasoning. Table 8 in Appendix D shows that the gap between our method and ColBERTv2, using the top-5 passages, increases even more from $3 \\%$ to $6 \\%$ on MuSiQue and from $20 \\%$ to $38 \\%$ on 2 WikiMultiHopQA, suggesting that large improvements come from obtaining all supporting documents rather than achieving partially retrieval on more questions.\nWe further illustrate HippoRAG's unique single-step multi-hop retrieval ability through the first example in Table 6. In this example, even though Alhandra was not mentioned in Vila de Xira's passage, HippoRAG can directly leverage Vila de Xira's connection to Alhandra as his place of birth to determine its importance, something that standard RAG methods would be unable to do directly. Additionally, even though IRCoT can also solve this multi-hop retrieval problem, as shown in Appendix G, it is 10-30 times more expensive and 6-13 times slower than ours in terms of online retrieval, arguably the most important factor when it comes to serving end users.\n\nTable 6: Multi-hop question types. We show example results for different approaches on path-finding vs. path-following multi-hop questions.\n\\begin{tabular}{|c|c|c|c|c|}\n  & Question & Hipporag & ColBERTv2 & IRCoT \\\\\n  \\multirow{4}{*}{\\begin{tabular}{l}\nPath- \\\\\nFollowing\n\\end{tabular}} & In which & \\multirow[b]{4}{*}{\\begin{tabular}{l}\n1. Alhandra \\\\\n2. Vila de Xira \\\\\n3. Portugal\n\\end{tabular}} & 1. Alhandra & 1. Alhandra \\\\\n  & district was & & 2. Dimuthu & 2. Vila de Xira \\\\\n  & Alhandra & & Abayakoon & 3. P\u00f3voa de \\\\\n  & born? & & 3. Ja'ar & Santa Iria \\\\\n  \\multirow{3}{*}{\\begin{tabular}{l}\nPath- \\\\\nFinding\n\\end{tabular}} & \\multirow[t]{3}{*}{Which Stanford professor works on the neuroscience of Alzheimer's?} & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\\\\n  & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\\\\n  & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo \\\\\n \n\\end{tabular}\n\n5.3 HippoRAG's Potential: Path-Finding Multi-Hop Retrieval\n\nThe second example in Table 6, also present in Figure 1, shows a type of questions that is trivial for informed humans but out of reach for current retrievers without further training. This type of questions, which we call path-finding multi-hop questions, requires identifying one path between a set of entities when many paths exist to explore instead of following a specific path, as in standard multi-hop questions. ${ }^{5}$\nMore specifically, a simple iterative process can retrieve the appropriate passages for the first question by following the one path set by Alhandra's one place of birth, as seen by IRCoT's perfect performance. However, an iterative process would struggle to answer the second question given the many possible paths to explore-either through professors at Stanford University or professors working on the neuroscience of Alzheimer's. It is only by associating disparate information about Thomas S\u00fcdhof that someone who knows about this professor would be able to answer this question\n\n\\footnotetext{\n${ }^{5}$ Path-finding questions require knowledge integration when search entities like Stanford and Alzheimer's do not happen to appear together in a passage, a condition which is often satisfied for new information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Which Stanford professor works on the neuroscience of Alzheimer's? & 1. Thomas S\u00fcdhof & 1. Brian Knutson & 1. Brian Knutson \\ & & 2. Karl Deisseroth & 2. Eric Knudsen & 2. Eric Knudsen \\ & & 3. Robert Sapolsky & 3. Lisa Giocomo & 3. Lisa Giocomo",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0534a252-a6d3-46c4-b810-c0405cec2965",
        "questions": "What is the birth date and birthplace of the footballer Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim, also known as Alhandra?",
        "answers": "5 March 1979, Lisbon",
        "context": "Indexing: Passage NER and OpenIE for Supporting Passages\n\n\n1. Alhandra (footballer)\n\nNER:\n[\"5 March 1979\", \"Alhandra\", \"Lisbon\", \"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"Portuguese\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Alhandra\", \"is a\", \"footballer\"),\n(\"Alhandra\", \"born in\", \"Vila Franca de Xira\"),\n(\"Alhandra\", \"born in\", \"Lisbon\"),\n(\"Alhandra\", \"born on\", \"5 March 1979\"),\n(\"Alhandra\", \"is\", \"Portuguese\"),\n(\"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"is also known as\", \"Alhandra\")]\n\n2. Vila Franca de Xira\n\nNER:\n[\"2011\", \"Afonso Henriques\", \"Cave of Pedra Furada\", \"French\", \"Lisbon\", \"Lisbon District\", \"Portugal\", \"Tagus River\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Vila Franca de Xira\", \"is a municipality in\", \"Lisbon District\"),\n(\"Vila Franca de Xira\", \"located in\", \"Portugal\"),\n(\"Vila Franca de Xira\", \"situated on\", \"Tagus River\"),\n(\"Vila Franca de Xira\", \"is\", \"founded by French followers of Afonso Henriques\"),\n(\"Tagus River\", \"located near\", \"Lisbon\"),\n(\"Cave of Pedra Furada\", \"evidenced settlement in\", \"neolithic times\"),\n(\"Afonso Henriques\", \"was Portugal's first king in\", \"1200\"),\n(\"Vila Franca de Xira\", \"had population of\", \"136,886 in 2011\"),\n(\"Vila Franca de Xira\", \"has area of\", \"318.19 km\"2)]\n\n\nFigure 4: HippoRAG Pipeline Example (Indexing). NER and OpenIE are sequentially conducted on each passage of the corpus. Thus, an open knowledge graph is formed for the entire corpus. We only show the relevant subgraph from the KG.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "(\"Alhandra\", \"born on\", \"5 March 1979\"), (\"Alhandra\", \"born in\", \"Lisbon\")",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "054961d6-0438-43d4-a81b-3fe3fda48455",
        "questions": "What was the population of Vila Franca de Xira in 2011 and its total area?",
        "answers": "136,886 people, 318.19 km^2",
        "context": "Indexing: Passage NER and OpenIE for Supporting Passages\n\n\n1. Alhandra (footballer)\n\nNER:\n[\"5 March 1979\", \"Alhandra\", \"Lisbon\", \"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"Portuguese\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Alhandra\", \"is a\", \"footballer\"),\n(\"Alhandra\", \"born in\", \"Vila Franca de Xira\"),\n(\"Alhandra\", \"born in\", \"Lisbon\"),\n(\"Alhandra\", \"born on\", \"5 March 1979\"),\n(\"Alhandra\", \"is\", \"Portuguese\"),\n(\"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"is also known as\", \"Alhandra\")]\n\n2. Vila Franca de Xira\n\nNER:\n[\"2011\", \"Afonso Henriques\", \"Cave of Pedra Furada\", \"French\", \"Lisbon\", \"Lisbon District\", \"Portugal\", \"Tagus River\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Vila Franca de Xira\", \"is a municipality in\", \"Lisbon District\"),\n(\"Vila Franca de Xira\", \"located in\", \"Portugal\"),\n(\"Vila Franca de Xira\", \"situated on\", \"Tagus River\"),\n(\"Vila Franca de Xira\", \"is\", \"founded by French followers of Afonso Henriques\"),\n(\"Tagus River\", \"located near\", \"Lisbon\"),\n(\"Cave of Pedra Furada\", \"evidenced settlement in\", \"neolithic times\"),\n(\"Afonso Henriques\", \"was Portugal's first king in\", \"1200\"),\n(\"Vila Franca de Xira\", \"had population of\", \"136,886 in 2011\"),\n(\"Vila Franca de Xira\", \"has area of\", \"318.19 km\"2)]\n\n\nFigure 4: HippoRAG Pipeline Example (Indexing). NER and OpenIE are sequentially conducted on each passage of the corpus. Thus, an open knowledge graph is formed for the entire corpus. We only show the relevant subgraph from the KG.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "(\"Vila Franca de Xira\", \"had population of\", \"136,886 in 2011\"), (\"Vila Franca de Xira\", \"has area of\", \"318.19 km\"2)",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "054b34e9-6922-4a46-8dd4-8f958b04fdf2",
        "questions": "Which river is Vila Franca de Xira situated on and who founded it?",
        "answers": "Tagus River, French followers of Afonso Henriques",
        "context": "Indexing: Passage NER and OpenIE for Supporting Passages\n\n\n1. Alhandra (footballer)\n\nNER:\n[\"5 March 1979\", \"Alhandra\", \"Lisbon\", \"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"Portuguese\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Alhandra\", \"is a\", \"footballer\"),\n(\"Alhandra\", \"born in\", \"Vila Franca de Xira\"),\n(\"Alhandra\", \"born in\", \"Lisbon\"),\n(\"Alhandra\", \"born on\", \"5 March 1979\"),\n(\"Alhandra\", \"is\", \"Portuguese\"),\n(\"Lu\u00eds Miguel Assun\u00e7\u00e3o Joaquim\", \"is also known as\", \"Alhandra\")]\n\n2. Vila Franca de Xira\n\nNER:\n[\"2011\", \"Afonso Henriques\", \"Cave of Pedra Furada\", \"French\", \"Lisbon\", \"Lisbon District\", \"Portugal\", \"Tagus River\", \"Vila Franca de Xira\"]\n\nOpenIE:\n[(\"Vila Franca de Xira\", \"is a municipality in\", \"Lisbon District\"),\n(\"Vila Franca de Xira\", \"located in\", \"Portugal\"),\n(\"Vila Franca de Xira\", \"situated on\", \"Tagus River\"),\n(\"Vila Franca de Xira\", \"is\", \"founded by French followers of Afonso Henriques\"),\n(\"Tagus River\", \"located near\", \"Lisbon\"),\n(\"Cave of Pedra Furada\", \"evidenced settlement in\", \"neolithic times\"),\n(\"Afonso Henriques\", \"was Portugal's first king in\", \"1200\"),\n(\"Vila Franca de Xira\", \"had population of\", \"136,886 in 2011\"),\n(\"Vila Franca de Xira\", \"has area of\", \"318.19 km\"2)]\n\n\nFigure 4: HippoRAG Pipeline Example (Indexing). NER and OpenIE are sequentially conducted on each passage of the corpus. Thus, an open knowledge graph is formed for the entire corpus. We only show the relevant subgraph from the KG.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "(\"Vila Franca de Xira\", \"situated on\", \"Tagus River\"), (\"Vila Franca de Xira\", \"is\", \"founded by French followers of Afonso Henriques\")",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "054e5126-7953-40f7-8a13-93ab39b3c49d",
        "questions": "What is the R@5 score for HippoRAG with Uncertainty-based Ensemble using ColBERTv2 on the 2Wiki dataset?",
        "answers": "89.0",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Uncertainty-based Ensemble & ColBERTv2 & 42.5 & 54.8 & 71.9 & 89.0 & 62.5 & 80.0 & 59.0 & 74.6",
        "evidence_page_no": 23,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "054f0566-bb1a-48e2-90b9-1e51681e5658",
        "questions": "Which retriever along with HippoRAG achieves a higher R@5 score on the 2Wiki dataset, Contriever or ColBERTv2?",
        "answers": "Contriever",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "HippoRAG & Contriever & 41.0 & 52.1 & 71.5 & 89.5 & 59.0 & 76.2 & 57.2 & 72.6",
        "evidence_page_no": 23,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0567732a-6739-4bfc-acc1-6b269efd95e1",
        "questions": "Does the combination of HippoRAG with Uncertainty-based Ensemble outperform all baselines in R@5 on the HotpotQA dataset?",
        "answers": "Yes",
        "context": "F. 2 Concepts vs. Context Tradeoff\n\n\nGiven our method's entity-centric nature in extraction and indexing, it has a strong bias towards concepts that leaves many contextual signals unused. This design enables single-step multi-hop retrieval while also enabling contextual cues to avoid distracting from more salient entities. As seen in the first example in Table 11, ColBERTv2 uses the context to retrieve passages that are related to famous Spanish navigators but not \"Sergio Villanueva\", who is a boxer. In contrast, HippoRAG is able to hone in on \"Sergio\" and retrieve one relevant passage.\nUnfortunately, this design is also one of our method's greatest limitations since ignoring contextual cues accounts for around $48 \\%$ of errors in a small-scale error analysis; more details can be found in Appendix F. This problem is more apparent in the second example since the concepts are general, making the context more important. Since the only concept tagged by HippoRAG is \"protons\", it extracts passages related to \"Uranium\" and \"nuclear weapons\" while ColBERTv2 uses the context to extract more relevant passages associated with the discovery of atomic numbers.\n\nTable 11: Examples showing the concept-context tradeoff on MuSiQue.\n\\begin{tabular}{|c|c|c|}\n  Question & HippoRAG & ColBERTv2 \\\\\n  Whose father was a navigator who explored the east coast & Sergio Villanueva & \\\\\n  of the continental region where & C\u00e9sar Gaytan & Exploration of N. America \\\\\n  Sergio Villanueva would later be born? & Faustino Reyes & Vicente Pinz\u00f3n (navigator) \\\\\n  What undertaking included the person who discovered that the number of protons in each element's atoms is unique? & \\begin{tabular}{l}\nUranium \\\\\nChemical element \\\\\nHistory of nuclear weapons\n\\end{tabular} & Atomic number Atomic theory Atomic nucleus \\\\\n \n\\end{tabular}\n\nTable 12: Single-step retrieval performance. HippoRAG performs significantly better on MuSiQue and 2WikiMultiHopQA than all baselines and achieves comparable performance on the less challenging HotpotQA dataset.\n\\begin{tabular}{llcccccc|cc}\n  Model & Retriever & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 3 - 10 } & & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 & R@2 & R@5 \\\\\n  \\multirow{2}{*}{ Baseline } & Contriever & 34.8 & 46.6 & 46.6 & 57.5 & 57.2 & 75.5 & 46.2 & 59.9 \\\\\n& ColBERTv2 & 37.9 & 49.2 & 59.2 & 68.2 & $\\mathbf{6 4 . 7}$ & $\\underline{79.3}$ & 53.9 & 65.6 \\\\\n  \\multirow{2}{*}{ HippoRAG } & Contriever & 41.0 & 52.1 & 71.5 & $\\mathbf{8 9 . 5}$ & 59.0 & 76.2 & 57.2 & 72.6 \\\\\n& ColBERTv2 & 40.9 & 51.9 & 70.7 & $\\underline{89.1}$ & 60.5 & 77.7 & 57.4 & 72.9 \\\\\n  HippoRAG w/ & Contriever & $\\underline{42.3}$ & $\\underline{54.5}$ & 71.3 & 87.2 & 60.6 & 79.1 & $\\underline{58.1}$ & $\\underline{73.6}$ \\\\\nUncertainty-based Ensemble & ColBERTv2 & $\\mathbf{4 2 . 5}$ & $\\mathbf{5 4 . 8}$ & $\\mathbf{7 1 . 9}$ & 89.0 & $\\underline{62.5}$ & $\\mathbf{8 0 . 0}$ & $\\mathbf{5 9 . 0}$ & $\\mathbf{7 4 . 6}$ \\\\\n \n\\end{tabular}\n\nTo get a better trade-off between concepts and context, we introduce an ensembling setting where HippoRAG scores are ensembled with dense retrievers when our parahippocampal region shows uncertainty regarding the link between query and KG entities. This process represents instances when no hippocampal index was fully activated by the upstream parahippocampal signal and thus the neocortex must be relied on more strongly. We only use uncertainty-based ensembling if one of the query-KG entity scores cosine_similarity $\\left(M\\left(c_{i}\\right), M\\left(e_{j}\\right)\\right)$ is lower than a threshold $\\theta$, for example, if there was no Stanford node in the KG and the closest node in the KG is something that has a cosine similarity lower than $\\theta$ such as Stanford Medical Center. The final passage score for uncertainty-based ensembling is the average of the HippoRAG scores and standard passage retrieval using model $M$, both of which are first normalized into the 0 to 1 over all passages.\n\nWhen HippoRAG is ensembled with $M$ under \"Uncertainty-based Ensembling\", it further improves on MuSiQue and outperforms our baselines in R @ 5 for HotpotQA, as shown in Table 12. When used in combination with IRCoT, as shown in Table 13, the ColBERTv2 ensemble outperforms all previous baselines in both R@2 and R@5 on HotpotQA. Although the simplicity of this approach is",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Uncertainty-based Ensemble & ColBERTv2 & 42.5 & 54.8 & 71.9 & 89.0 & 62.5 & 80.0 & 59.0 & 74.6",
        "evidence_page_no": 23,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0571bb4d-3cd7-4d44-88f9-05859805c3f5",
        "questions": "What is the AR@5 percentage for HippoRAG on the 2Wiki dataset in Table 8?",
        "answers": "75.7",
        "context": "the percentage of queries where all supporting passages are retrieved successfully, something that can only be achieved through successful multi-hop reasoning. The fact that the gaps shown in this table are much larger than the gaps in Table 2 indicates that much of HippoRAG's improvements are coming from the ability to retrieve all supporting documents not by performing better partial retrieval.\n\nTable 8: All-Recall metric. We measure the percentage of queries for which all supporting passages are successfully retrieved (all-recall, denoted as AR@2 or AR@5) and find even larger performance improvements for HippoRAG.\n\\begin{tabular}{lrrrrrr|rr}\n  & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 2 - 9 } & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 \\\\\n  ColBERTv2 [53] & 6.8 & 16.1 & 25.1 & 37.1 & 33.3 & 59.0 & 21.7 & 37.4 \\\\\nHippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0 \\\\\n \n\\end{tabular}\n\n\nE Case Study on Path-Finding Multi-Hop QA\n\n\nAs discussed above, path-finding multi-hop questions across passages are exceedingly challenging for single-step and multi-step RAG methods such as ColBERTv2 and IRCoT. These questions require integrating information across multiple passages to find relevant entities among many possible candidates, such as finding all Stanford professors who work on the neuroscience of Alzheimer's.\n\nE. 1 Path-Finding Multi-Hop Question Construction Process\n\nThese questions and the curated corpora around them were built through the following procedure. The first two questions follow a slightly separate process as the third one as well as the motivating example in the main paper. For the first two, we first identify a book or movie and then found the book's author or the movie's director. We would then find 1) a trait for either the book/movie and 2) another trait for the author/director. These two traits would then be used to extract distractors from Wikipedia for each question.\n\nFor the third question and our motivating example, we first choose a professor or a drug at random as the answer for each question. We then obtain the university the professor works at or the disease the drug treats as well as one other trait for the professor or drug (in these questions research topic and mechanism of action were chosen). In these questions, distractors were extracted from Wikipedia using the University or disease on the one hand and the research topic or mechanism of action on the other. This process, although quite tedious, allowed us to curate these challenging but realistic path-finding multi-hop questions.\n\nE. 2 Qualitative Analysis\n\nIn Table 9, we show three more examples from three different domains that illustrate HippoRAG's potential for solving retrieval tasks that require such cross-passage knowledge integration.\nIn the first question of Table 9, we want to find a book published in 2012 by an English author who won a specific award. In contrast to HippoRAG, ColBERTv2 and IRCoT are unable to identify Mark Haddon as such an author. ColBERTv2 focuses on passages related to awards while IRCoT mistakenly decides that Kate Atkinson is the answer to such question since she won the same award for a book published in 1995. For the second question, we wanted to find a war film based on a non-fiction book directed by someone famous for sci-fi and crime movies. HippoRAG is able to find our answer Black Hawk Down by Ridley Scott within the first four passages, while ColBERTv2 misses the answer completely and retrieves other films and film collections. In this instance, even though IRCoT is able to retrieve Ridley Scott, it does so mainly through parametric knowledge. The chain-of-thought output discusses his and Denis Villeneuve fame as well as their sci-fi and crime experience. Given the three-step iteration restriction used here and the need to explore two directors, the specific war film Black Hawk Down was not identified. Although a bit convoluted, people often ask these first two questions to remember a specific movie or book they watched or heard about from only a handful of disjointed details.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "HippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "0573e598-b86a-49e7-8c0c-fe12f98647a9",
        "questions": "How does HippoRAG's average AR@2 score compare to ColBERTv2 according to Table 8?",
        "answers": "HippoRAG's average AR@2 score is higher at 29.8 compared to ColBERTv2's 21.7.",
        "context": "the percentage of queries where all supporting passages are retrieved successfully, something that can only be achieved through successful multi-hop reasoning. The fact that the gaps shown in this table are much larger than the gaps in Table 2 indicates that much of HippoRAG's improvements are coming from the ability to retrieve all supporting documents not by performing better partial retrieval.\n\nTable 8: All-Recall metric. We measure the percentage of queries for which all supporting passages are successfully retrieved (all-recall, denoted as AR@2 or AR@5) and find even larger performance improvements for HippoRAG.\n\\begin{tabular}{lrrrrrr|rr}\n  & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 2 - 9 } & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 \\\\\n  ColBERTv2 [53] & 6.8 & 16.1 & 25.1 & 37.1 & 33.3 & 59.0 & 21.7 & 37.4 \\\\\nHippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0 \\\\\n \n\\end{tabular}\n\n\nE Case Study on Path-Finding Multi-Hop QA\n\n\nAs discussed above, path-finding multi-hop questions across passages are exceedingly challenging for single-step and multi-step RAG methods such as ColBERTv2 and IRCoT. These questions require integrating information across multiple passages to find relevant entities among many possible candidates, such as finding all Stanford professors who work on the neuroscience of Alzheimer's.\n\nE. 1 Path-Finding Multi-Hop Question Construction Process\n\nThese questions and the curated corpora around them were built through the following procedure. The first two questions follow a slightly separate process as the third one as well as the motivating example in the main paper. For the first two, we first identify a book or movie and then found the book's author or the movie's director. We would then find 1) a trait for either the book/movie and 2) another trait for the author/director. These two traits would then be used to extract distractors from Wikipedia for each question.\n\nFor the third question and our motivating example, we first choose a professor or a drug at random as the answer for each question. We then obtain the university the professor works at or the disease the drug treats as well as one other trait for the professor or drug (in these questions research topic and mechanism of action were chosen). In these questions, distractors were extracted from Wikipedia using the University or disease on the one hand and the research topic or mechanism of action on the other. This process, although quite tedious, allowed us to curate these challenging but realistic path-finding multi-hop questions.\n\nE. 2 Qualitative Analysis\n\nIn Table 9, we show three more examples from three different domains that illustrate HippoRAG's potential for solving retrieval tasks that require such cross-passage knowledge integration.\nIn the first question of Table 9, we want to find a book published in 2012 by an English author who won a specific award. In contrast to HippoRAG, ColBERTv2 and IRCoT are unable to identify Mark Haddon as such an author. ColBERTv2 focuses on passages related to awards while IRCoT mistakenly decides that Kate Atkinson is the answer to such question since she won the same award for a book published in 1995. For the second question, we wanted to find a war film based on a non-fiction book directed by someone famous for sci-fi and crime movies. HippoRAG is able to find our answer Black Hawk Down by Ridley Scott within the first four passages, while ColBERTv2 misses the answer completely and retrieves other films and film collections. In this instance, even though IRCoT is able to retrieve Ridley Scott, it does so mainly through parametric knowledge. The chain-of-thought output discusses his and Denis Villeneuve fame as well as their sci-fi and crime experience. Given the three-step iteration restriction used here and the need to explore two directors, the specific war film Black Hawk Down was not identified. Although a bit convoluted, people often ask these first two questions to remember a specific movie or book they watched or heard about from only a handful of disjointed details.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ColBERTv2 [53] & 6.8 & 16.1 & 25.1 & 37.1 & 33.3 & 59.0 & 21.7 & 37.4 \nHippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14831v1",
        "ID": "057ff1be-e77b-4bcf-8741-79713063197a",
        "questions": "In Table 8, what is the total difference in AR@5 scores across all datasets between HippoRAG and ColBERTv2?",
        "answers": "38.0",
        "context": "the percentage of queries where all supporting passages are retrieved successfully, something that can only be achieved through successful multi-hop reasoning. The fact that the gaps shown in this table are much larger than the gaps in Table 2 indicates that much of HippoRAG's improvements are coming from the ability to retrieve all supporting documents not by performing better partial retrieval.\n\nTable 8: All-Recall metric. We measure the percentage of queries for which all supporting passages are successfully retrieved (all-recall, denoted as AR@2 or AR@5) and find even larger performance improvements for HippoRAG.\n\\begin{tabular}{lrrrrrr|rr}\n  & \\multicolumn{2}{c}{ MuSiQue } & \\multicolumn{2}{c}{ 2Wiki } & \\multicolumn{2}{c}{ HotpotQA } & \\multicolumn{2}{c}{ Average } \\\\\n\\cline { 2 - 9 } & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 & AR@2 & AR@5 \\\\\n  ColBERTv2 [53] & 6.8 & 16.1 & 25.1 & 37.1 & 33.3 & 59.0 & 21.7 & 37.4 \\\\\nHippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0 \\\\\n \n\\end{tabular}\n\n\nE Case Study on Path-Finding Multi-Hop QA\n\n\nAs discussed above, path-finding multi-hop questions across passages are exceedingly challenging for single-step and multi-step RAG methods such as ColBERTv2 and IRCoT. These questions require integrating information across multiple passages to find relevant entities among many possible candidates, such as finding all Stanford professors who work on the neuroscience of Alzheimer's.\n\nE. 1 Path-Finding Multi-Hop Question Construction Process\n\nThese questions and the curated corpora around them were built through the following procedure. The first two questions follow a slightly separate process as the third one as well as the motivating example in the main paper. For the first two, we first identify a book or movie and then found the book's author or the movie's director. We would then find 1) a trait for either the book/movie and 2) another trait for the author/director. These two traits would then be used to extract distractors from Wikipedia for each question.\n\nFor the third question and our motivating example, we first choose a professor or a drug at random as the answer for each question. We then obtain the university the professor works at or the disease the drug treats as well as one other trait for the professor or drug (in these questions research topic and mechanism of action were chosen). In these questions, distractors were extracted from Wikipedia using the University or disease on the one hand and the research topic or mechanism of action on the other. This process, although quite tedious, allowed us to curate these challenging but realistic path-finding multi-hop questions.\n\nE. 2 Qualitative Analysis\n\nIn Table 9, we show three more examples from three different domains that illustrate HippoRAG's potential for solving retrieval tasks that require such cross-passage knowledge integration.\nIn the first question of Table 9, we want to find a book published in 2012 by an English author who won a specific award. In contrast to HippoRAG, ColBERTv2 and IRCoT are unable to identify Mark Haddon as such an author. ColBERTv2 focuses on passages related to awards while IRCoT mistakenly decides that Kate Atkinson is the answer to such question since she won the same award for a book published in 1995. For the second question, we wanted to find a war film based on a non-fiction book directed by someone famous for sci-fi and crime movies. HippoRAG is able to find our answer Black Hawk Down by Ridley Scott within the first four passages, while ColBERTv2 misses the answer completely and retrieves other films and film collections. In this instance, even though IRCoT is able to retrieve Ridley Scott, it does so mainly through parametric knowledge. The chain-of-thought output discusses his and Denis Villeneuve fame as well as their sci-fi and crime experience. Given the three-step iteration restriction used here and the need to explore two directors, the specific war film Black Hawk Down was not identified. Although a bit convoluted, people often ask these first two questions to remember a specific movie or book they watched or heard about from only a handful of disjointed details.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "ColBERTv2 [53] & 6.8 & 16.1 & 25.1 & 37.1 & 33.3 & 59.0 & 21.7 & 37.4 \nHippoRAG & 10.2 & 22.4 & 45.4 & 75.7 & 33.8 & 57.9 & 29.8 & 52.0",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0584d26c-f724-4a0c-8947-bf82b8ed3aa4",
        "questions": "What is the nDCG@10 score for the M3-w.skd model with the Multi-vec configuration on the MIRACL dev set?",
        "answers": "70.5",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Multi-vec & 70.5 \\",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0588d91d-fbe8-4a8e-a1cf-0d1d47b3df41",
        "questions": "Does the M3-Embedding model support more than 100 working languages?",
        "answers": "Yes",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Furthermore, we claim support for more than 100 working languages in M3-Embedding.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "058c4c3d-47be-46e8-95bc-077c94ebdc26",
        "questions": "What is the impact of pre-training on unsupervised data on the retrieval quality of the embedding model according to the document?",
        "answers": "Pre-training on unsupervised data can further enhance the retrieval quality of the embedding model.",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0597aab6-14d3-4dbc-a0ac-f4c9658de17e",
        "questions": "What is the average length of documents in the Arabic language dataset in the MultiLongDoc dataset?",
        "answers": "9,428",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05a3f71f-ab8c-4b93-a7ff-2b2894f55a32",
        "questions": "How many languages are included in the unsupervised data source MTP?",
        "answers": "2",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "MTP & EN, ZH & 291.1M",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05b827dc-f75f-4cd5-9516-c1a645d3fa2c",
        "questions": "Does enabling split-batch result in a significant increase in batch size for text lengths of 8192?",
        "answers": "Yes",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05c0826a-c67c-4d5a-8504-74442a6188b2",
        "questions": "What is the maximum batch size per device when using a sequence length of 4096 without split-batch?",
        "answers": "25",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$\\times$ & 262 & 25 & 6",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05d5859b-111e-4105-90fb-1596b3deed9f",
        "questions": "How many unique terms per article does the XLM-RoBERTa tokenizer produce on the MLDR dataset?",
        "answers": "1056",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05dd1ef0-0806-48c9-8967-49fbb2e02961",
        "questions": "Does the M3 model outperform BM25 on the MIRACL dataset when using the same tokenizer?",
        "answers": "Yes",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets.",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05e8b8d8-966f-42d4-9b2c-7c5e3c4442a4",
        "questions": "What is the title of the work by Tom Kwiatkowski and others that serves as a benchmark for question answering research?",
        "answers": "Natural questions: A benchmark for question answering research.",
        "context": "2023. Coliee 2022 summary: Methods for legal document retrieval and entailment. In New Frontiers in Artificial Intelligence, pages 51-67, Cham. Springer Nature Switzerland.\n\nTom\u00e1\u015b Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.\n\nHaitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2023. Lecardv2: A largescale chinese legal case retrieval dataset. arXiv preprint arXiv:2310.17609.\n\nJimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. arXiv preprint arXiv:2106.14807.\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 2356-2362, New York, NY, USA. Association for Computing Machinery.\n\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2022. Pretrained Transformers for Text Ranking: BERT and Beyond. Springer Nature.\n\nHongcheng Liu, Yusheng Liao, Yutong Meng, and Yuhao Wang. 2023. Xiezhi: Chinese law large language model. https://github.com/LiuHC0428/LAW_GPT.\n\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.\n\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389-1406.\n\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations for text retrieval. Transactions of the\n\nAssociation for Computational Linguistics, 9:329345 .\n\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.\n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics.\n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pretraining. arXiv preprint arXiv:2201.10005.\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. choice, 2640:660.\n\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022. Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.\n\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computational Linguistics.\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05eae756-111a-40c6-b6be-813738d0b720",
        "questions": "In which year was the paper 'Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations' presented at the International ACM SIGIR Conference?",
        "answers": "2021",
        "context": "2023. Coliee 2022 summary: Methods for legal document retrieval and entailment. In New Frontiers in Artificial Intelligence, pages 51-67, Cham. Springer Nature Switzerland.\n\nTom\u00e1\u015b Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.\n\nHaitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2023. Lecardv2: A largescale chinese legal case retrieval dataset. arXiv preprint arXiv:2310.17609.\n\nJimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. arXiv preprint arXiv:2106.14807.\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 2356-2362, New York, NY, USA. Association for Computing Machinery.\n\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2022. Pretrained Transformers for Text Ranking: BERT and Beyond. Springer Nature.\n\nHongcheng Liu, Yusheng Liao, Yutong Meng, and Yuhao Wang. 2023. Xiezhi: Chinese law large language model. https://github.com/LiuHC0428/LAW_GPT.\n\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.\n\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389-1406.\n\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations for text retrieval. Transactions of the\n\nAssociation for Computational Linguistics, 9:329345 .\n\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.\n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics.\n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pretraining. arXiv preprint arXiv:2201.10005.\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. choice, 2640:660.\n\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022. Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.\n\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computational Linguistics.\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 2356-2362, New York, NY, USA.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "05eb09ba-a028-4d1e-a595-04cada6b236d",
        "questions": "Is the work 'Sgpt: Gpt sentence embeddings for semantic search' by Niklas Muennighoff published as an arXiv preprint?",
        "answers": "Yes",
        "context": "2023. Coliee 2022 summary: Methods for legal document retrieval and entailment. In New Frontiers in Artificial Intelligence, pages 51-67, Cham. Springer Nature Switzerland.\n\nTom\u00e1\u015b Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.\n\nHaitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2023. Lecardv2: A largescale chinese legal case retrieval dataset. arXiv preprint arXiv:2310.17609.\n\nJimmy Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques. arXiv preprint arXiv:2106.14807.\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 2356-2362, New York, NY, USA. Association for Computing Machinery.\n\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2022. Pretrained Transformers for Text Ranking: BERT and Beyond. Springer Nature.\n\nHongcheng Liu, Yusheng Liao, Yutong Meng, and Yuhao Wang. 2023. Xiezhi: Chinese law large language model. https://github.com/LiuHC0428/LAW_GPT.\n\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.\n\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389-1406.\n\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representations for text retrieval. Transactions of the\n\nAssociation for Computational Linguistics, 9:329345 .\n\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.\n\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991-16111, Toronto, Canada. Association for Computational Linguistics.\n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pretraining. arXiv preprint arXiv:2201.10005.\n\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. choice, 2640:660.\n\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022. Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.\n\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996-5001, Florence, Italy. Association for Computational Linguistics.\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0615377a-6407-4a40-8f2f-1dd410e15c52",
        "questions": "What is the Recall@100 score for the M3-Embedding Dense model on the German (de) language in the MIRACL dataset?",
        "answers": "90.9",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0615594c-3ae1-4b40-8dfe-24b90d1b85c9",
        "questions": "Which model achieved the highest average Recall@20 score on the MKQA dataset for cross-lingual retrieval?",
        "answers": "Dense",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "061948c4-031f-40dc-9cab-745448ea5a56",
        "questions": "Did the mContriever model achieve a higher Recall@100 score than the mDPR model for the Finnish (fi) language in the MIRACL dataset?",
        "answers": "Yes",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06274afb-6071-411e-bb25-59fb54d6aa2d",
        "questions": "Who are the authors of the paper titled 'SentenceBERT: Sentence embeddings using Siamese BERT-networks'?",
        "answers": "Nils Reimers and Iryna Gurevych",
        "context": "Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333-389.\n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490-6500, Online. Association for Computational Linguistics.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368.\n\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-training retrieval-oriented language models via masked auto-encoder. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538-548, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597.\n\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. arXiv preprint arXiv:2304.03679.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.\n\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. $A I$ Open, 2:65-68.\n\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021a. Adversarial retriever-ranker for dense text retrieval. arXiv preprint arXiv:2110.03611.\n\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023a. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "062e35b8-2580-42a2-bf96-4bd9f93a0f7b",
        "questions": "In which conference proceedings was the paper 'RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking' published?",
        "answers": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
        "context": "Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333-389.\n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490-6500, Online. Association for Computational Linguistics.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368.\n\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-training retrieval-oriented language models via masked auto-encoder. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538-548, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597.\n\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. arXiv preprint arXiv:2304.03679.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.\n\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. $A I$ Open, 2:65-68.\n\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021a. Adversarial retriever-ranker for dense text retrieval. arXiv preprint arXiv:2110.03611.\n\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023a. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "062fce1b-34ed-4821-bae3-a848e79d67bc",
        "questions": "Is the paper 'T2ranking: A large-scale chinese benchmark for passage ranking' published in a conference or as an arXiv preprint?",
        "answers": "arXiv preprint",
        "context": "Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333-389.\n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490-6500, Online. Association for Computational Linguistics.\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368.\n\nShitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-training retrieval-oriented language models via masked auto-encoder. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538-548, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597.\n\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. arXiv preprint arXiv:2304.03679.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808.\n\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.\n\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. $A I$ Open, 2:65-68.\n\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021a. Adversarial retriever-ranker for dense text retrieval. arXiv preprint arXiv:2110.03611.\n\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023a. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. arXiv preprint arXiv:2304.03679.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0642f2b0-e84f-440f-b0ea-a3c6d844bf78",
        "questions": "What is the nDCG@10 score for the 'Dense' method in the 'bn' language according to the multi-lingual retrieval performance on the MIRACL dev set?",
        "answers": "80.0",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "064f94bf-0ef8-4629-893f-222b0b8fefe4",
        "questions": "Which method achieved the highest average nDCG@10 score in the multi-lingual retrieval performance on the MIRACL dev set?",
        "answers": "All",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0654f465-b311-49c4-9d86-fd12c8359b91",
        "questions": "Does the 'Dense' method outperform the 'E5mistral-7b' method in the 'hi' language according to the multi-lingual retrieval performance on the MIRACL dev set?",
        "answers": "Yes",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "065963f2-9471-45e5-8ded-3a7c0eeae85f",
        "questions": "What is the maximum number of input tokens that M3-Embedding can process?",
        "answers": "8,192",
        "context": "linguality, which is able to support more than 100 world languages. By learning a common semantic space for different languages, enables both multilingual retrieval within each language and crosslingual retrieval between different languages. Besides, it is able to generate versatile embeddings to support different retrieval functionalities, not just dense retrieval, but also sparse retrieval and multivector retrieval. Finally, M3-Embedding is learned to process different input granularities, spanning from short inputs like sentences and passages, to long documents of up to 8,192 input tokens.\n\nThe training of M3-Embedding poses a significant challenge. In our work, the following technical contributions are made to optimize the embedding quality. Firstly, we propose a novel self knowledge distillation framework, where the multiple retrieval functionalities can be jointly learned and mutually reinforced. In M3-Embedding, the [CLS] embedding is used for dense retrieval, while embeddings from other tokens are used for sparse retrieval and multi-vector retrieval. Based on the principle of ensemble learning (B\u00fchlmann, 2012), such heterogenous predictors can be combined as a stronger predictor. Thus, we integrate the relevance scores from different retrieval functions as the teacher signal, which is used to enhance the learning process via knowledge distillation. Secondly, we optimize the batching strategy to achieve a large batch size and high training throughput, which substantially contributes to the discriminativeness of embeddings. Last but not least, we perform extensive and high-quality data curation. Our dataset includes three sources: 1) the extraction of unsupervised data from massive multi-lingual corpora, 2) the integration of closely related supervised data, 3) the synthesization of scarce training data. The three data sources are complement to each other and applied to different training stages, which lays a solid foundation for the versatile text embeddings.\n\nM3-Embedding exhibits a remarkable versatility in our experiments. It achieves superior retrieval quality for a variety of languages, leading to state-of-the-art performances on popular multilingual and cross-lingual benchmarks like MIRACL (Zhang et al., 2023c) and MKQA (Longpre et al., 2021). It effectively learns the three retrieval functionalities, which can not only work individually but also work together for an even stronger retrieval quality. It also well maintains its superior capability across different input granularities within 8192 tokens, which outperforms the existing\nmethods by a notable advantage.\nOur contributions are summarized as follows. 1) We present M3-Embedding, which achieves unprecedented versatility in multi-linguality, multifunctionality, and multi-granularity. 2) We propose a novel training framework of self-knowledge distillation and optimize the batching strategy for efficient training. We also create high-quality training resource based on comprehensive data curation. 3) Our model, code, and data is publicly available, offering critical resources for both direct usage and future development of text embeddings.\n\n2 Related Work\n\nThe related works are reviewed from three aspects: general text embeddings, embedding models for neural retrieval, embeddings of multi-linguality.\n\nIn the past few years, substantial progress has been achieved in the field of text embedding. One major driving force is the popularity of pre-trained language models, where the underlying semantic of the data can be effectively encoded by such powerful text encoders (Reimers and Gurevych, 2019; Karpukhin et al., 2020; Ni et al., 2022). In addition, the progress of contrastive learning is another critical factor, especially the improvement of negative sampling (Xiong et al., 2020; Qu et al., 2021) and the exploitation of knowledge distillation (Hofst\u00e4tter et al., 2021; Ren et al., 2021; Zhang et al., 2021a). On top of these well-established techniques, it becomes increasingly popular to learn versatile embedding models, which are able to uniformly support a variety of application scenarios. So far, there have been many impactful methods in the direction, like Contriever (Izacard et al., 2022), LLM-Embedder (Zhang et al., 2023a), E5 (Wang et al., 2022), BGE (Xiao et al., 2023), SGPT (Muennighoff, 2022), and Open Text Embedding (Neelakantan et al., 2022), which significantly advance the usage of text embeddings for general tasks.\n\nOne major application of embedding models is neural retrieval (Lin et al., 2022). By measuring the semantic relationship with the text embeddings, the relevant answers to the input query can be retrieved based on the embedding similarity. The most common form of embedding-based retrieval method is dense retrieval (Karpukhin et al., 2020), where the text encoder's outputs are aggregated (e.g., via [CLS] or mean-pooling) to compute the embedding similarity. Another common alternative is known as multi-vecor retrieval (Khattab and Za -",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Finally, M3-Embedding is learned to process different input granularities, spanning from short inputs like sentences and passages, to long documents of up to 8,192 input tokens.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "065a13d4-32e9-433d-a49e-0f3a63e766b7",
        "questions": "Which benchmarks did M3-Embedding achieve state-of-the-art performances on?",
        "answers": "MIRACL and MKQA",
        "context": "linguality, which is able to support more than 100 world languages. By learning a common semantic space for different languages, enables both multilingual retrieval within each language and crosslingual retrieval between different languages. Besides, it is able to generate versatile embeddings to support different retrieval functionalities, not just dense retrieval, but also sparse retrieval and multivector retrieval. Finally, M3-Embedding is learned to process different input granularities, spanning from short inputs like sentences and passages, to long documents of up to 8,192 input tokens.\n\nThe training of M3-Embedding poses a significant challenge. In our work, the following technical contributions are made to optimize the embedding quality. Firstly, we propose a novel self knowledge distillation framework, where the multiple retrieval functionalities can be jointly learned and mutually reinforced. In M3-Embedding, the [CLS] embedding is used for dense retrieval, while embeddings from other tokens are used for sparse retrieval and multi-vector retrieval. Based on the principle of ensemble learning (B\u00fchlmann, 2012), such heterogenous predictors can be combined as a stronger predictor. Thus, we integrate the relevance scores from different retrieval functions as the teacher signal, which is used to enhance the learning process via knowledge distillation. Secondly, we optimize the batching strategy to achieve a large batch size and high training throughput, which substantially contributes to the discriminativeness of embeddings. Last but not least, we perform extensive and high-quality data curation. Our dataset includes three sources: 1) the extraction of unsupervised data from massive multi-lingual corpora, 2) the integration of closely related supervised data, 3) the synthesization of scarce training data. The three data sources are complement to each other and applied to different training stages, which lays a solid foundation for the versatile text embeddings.\n\nM3-Embedding exhibits a remarkable versatility in our experiments. It achieves superior retrieval quality for a variety of languages, leading to state-of-the-art performances on popular multilingual and cross-lingual benchmarks like MIRACL (Zhang et al., 2023c) and MKQA (Longpre et al., 2021). It effectively learns the three retrieval functionalities, which can not only work individually but also work together for an even stronger retrieval quality. It also well maintains its superior capability across different input granularities within 8192 tokens, which outperforms the existing\nmethods by a notable advantage.\nOur contributions are summarized as follows. 1) We present M3-Embedding, which achieves unprecedented versatility in multi-linguality, multifunctionality, and multi-granularity. 2) We propose a novel training framework of self-knowledge distillation and optimize the batching strategy for efficient training. We also create high-quality training resource based on comprehensive data curation. 3) Our model, code, and data is publicly available, offering critical resources for both direct usage and future development of text embeddings.\n\n2 Related Work\n\nThe related works are reviewed from three aspects: general text embeddings, embedding models for neural retrieval, embeddings of multi-linguality.\n\nIn the past few years, substantial progress has been achieved in the field of text embedding. One major driving force is the popularity of pre-trained language models, where the underlying semantic of the data can be effectively encoded by such powerful text encoders (Reimers and Gurevych, 2019; Karpukhin et al., 2020; Ni et al., 2022). In addition, the progress of contrastive learning is another critical factor, especially the improvement of negative sampling (Xiong et al., 2020; Qu et al., 2021) and the exploitation of knowledge distillation (Hofst\u00e4tter et al., 2021; Ren et al., 2021; Zhang et al., 2021a). On top of these well-established techniques, it becomes increasingly popular to learn versatile embedding models, which are able to uniformly support a variety of application scenarios. So far, there have been many impactful methods in the direction, like Contriever (Izacard et al., 2022), LLM-Embedder (Zhang et al., 2023a), E5 (Wang et al., 2022), BGE (Xiao et al., 2023), SGPT (Muennighoff, 2022), and Open Text Embedding (Neelakantan et al., 2022), which significantly advance the usage of text embeddings for general tasks.\n\nOne major application of embedding models is neural retrieval (Lin et al., 2022). By measuring the semantic relationship with the text embeddings, the relevant answers to the input query can be retrieved based on the embedding similarity. The most common form of embedding-based retrieval method is dense retrieval (Karpukhin et al., 2020), where the text encoder's outputs are aggregated (e.g., via [CLS] or mean-pooling) to compute the embedding similarity. Another common alternative is known as multi-vecor retrieval (Khattab and Za -",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "It achieves superior retrieval quality for a variety of languages, leading to state-of-the-art performances on popular multilingual and cross-lingual benchmarks like MIRACL (Zhang et al., 2023c) and MKQA (Longpre et al., 2021).",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "065a1e3f-aa81-415f-b803-a5e4194cfb47",
        "questions": "Does M3-Embedding use a self-knowledge distillation framework to enhance its learning process?",
        "answers": "Yes",
        "context": "linguality, which is able to support more than 100 world languages. By learning a common semantic space for different languages, enables both multilingual retrieval within each language and crosslingual retrieval between different languages. Besides, it is able to generate versatile embeddings to support different retrieval functionalities, not just dense retrieval, but also sparse retrieval and multivector retrieval. Finally, M3-Embedding is learned to process different input granularities, spanning from short inputs like sentences and passages, to long documents of up to 8,192 input tokens.\n\nThe training of M3-Embedding poses a significant challenge. In our work, the following technical contributions are made to optimize the embedding quality. Firstly, we propose a novel self knowledge distillation framework, where the multiple retrieval functionalities can be jointly learned and mutually reinforced. In M3-Embedding, the [CLS] embedding is used for dense retrieval, while embeddings from other tokens are used for sparse retrieval and multi-vector retrieval. Based on the principle of ensemble learning (B\u00fchlmann, 2012), such heterogenous predictors can be combined as a stronger predictor. Thus, we integrate the relevance scores from different retrieval functions as the teacher signal, which is used to enhance the learning process via knowledge distillation. Secondly, we optimize the batching strategy to achieve a large batch size and high training throughput, which substantially contributes to the discriminativeness of embeddings. Last but not least, we perform extensive and high-quality data curation. Our dataset includes three sources: 1) the extraction of unsupervised data from massive multi-lingual corpora, 2) the integration of closely related supervised data, 3) the synthesization of scarce training data. The three data sources are complement to each other and applied to different training stages, which lays a solid foundation for the versatile text embeddings.\n\nM3-Embedding exhibits a remarkable versatility in our experiments. It achieves superior retrieval quality for a variety of languages, leading to state-of-the-art performances on popular multilingual and cross-lingual benchmarks like MIRACL (Zhang et al., 2023c) and MKQA (Longpre et al., 2021). It effectively learns the three retrieval functionalities, which can not only work individually but also work together for an even stronger retrieval quality. It also well maintains its superior capability across different input granularities within 8192 tokens, which outperforms the existing\nmethods by a notable advantage.\nOur contributions are summarized as follows. 1) We present M3-Embedding, which achieves unprecedented versatility in multi-linguality, multifunctionality, and multi-granularity. 2) We propose a novel training framework of self-knowledge distillation and optimize the batching strategy for efficient training. We also create high-quality training resource based on comprehensive data curation. 3) Our model, code, and data is publicly available, offering critical resources for both direct usage and future development of text embeddings.\n\n2 Related Work\n\nThe related works are reviewed from three aspects: general text embeddings, embedding models for neural retrieval, embeddings of multi-linguality.\n\nIn the past few years, substantial progress has been achieved in the field of text embedding. One major driving force is the popularity of pre-trained language models, where the underlying semantic of the data can be effectively encoded by such powerful text encoders (Reimers and Gurevych, 2019; Karpukhin et al., 2020; Ni et al., 2022). In addition, the progress of contrastive learning is another critical factor, especially the improvement of negative sampling (Xiong et al., 2020; Qu et al., 2021) and the exploitation of knowledge distillation (Hofst\u00e4tter et al., 2021; Ren et al., 2021; Zhang et al., 2021a). On top of these well-established techniques, it becomes increasingly popular to learn versatile embedding models, which are able to uniformly support a variety of application scenarios. So far, there have been many impactful methods in the direction, like Contriever (Izacard et al., 2022), LLM-Embedder (Zhang et al., 2023a), E5 (Wang et al., 2022), BGE (Xiao et al., 2023), SGPT (Muennighoff, 2022), and Open Text Embedding (Neelakantan et al., 2022), which significantly advance the usage of text embeddings for general tasks.\n\nOne major application of embedding models is neural retrieval (Lin et al., 2022). By measuring the semantic relationship with the text embeddings, the relevant answers to the input query can be retrieved based on the embedding similarity. The most common form of embedding-based retrieval method is dense retrieval (Karpukhin et al., 2020), where the text encoder's outputs are aggregated (e.g., via [CLS] or mean-pooling) to compute the embedding similarity. Another common alternative is known as multi-vecor retrieval (Khattab and Za -",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Firstly, we propose a novel self knowledge distillation framework, where the multiple retrieval functionalities can be jointly learned and mutually reinforced.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "066db941-5f7d-4765-831c-827638e4b0ca",
        "questions": "Who are the authors of the paper titled 'Multi-scale attentive interaction networks for Chinese medical question answer selection' published in IEEE Access in 2018?",
        "answers": "Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu",
        "context": "Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061-74071.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021b. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nXinyu Zhang, Kelechi Ogueji, Xueguang Ma, and Jimmy Lin. 2023b. Toward best practices for training multilingual dense retrieval models. ACM Trans. Inf. Syst., 42(2).\n\nXinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023c. MIRACL: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114-1131.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061-74071.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "066dee1e-a16b-447d-9bab-bdab0e1cae7a",
        "questions": "In which year was the paper 'Mr. TyDi: A multi-lingual benchmark for dense retrieval' presented at the 1st Workshop on Multilingual Representation Learning?",
        "answers": "2021",
        "context": "Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061-74071.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021b. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nXinyu Zhang, Kelechi Ogueji, Xueguang Ma, and Jimmy Lin. 2023b. Toward best practices for training multilingual dense retrieval models. ACM Trans. Inf. Syst., 42(2).\n\nXinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023c. MIRACL: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114-1131.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021b. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "066df3fe-8a07-4105-9547-0e5505adc80b",
        "questions": "Does the MIRACL dataset cover multiple languages, and if so, how many languages does it include?",
        "answers": "Yes, it covers 18 diverse languages.",
        "context": "Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018. Multi-scale attentive interaction networks for chinese medical question answer selection. IEEE Access, 6:74061-74071.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021b. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nXinyu Zhang, Kelechi Ogueji, Xueguang Ma, and Jimmy Lin. 2023b. Toward best practices for training multilingual dense retrieval models. ACM Trans. Inf. Syst., 42(2).\n\nXinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023c. MIRACL: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114-1131.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023c. MIRACL: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:1114-1131.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06714938-04fe-473d-a61c-ec19fff83cec",
        "questions": "What is the probability of applying the operation of shuffling the order of segments within entire texts during training?",
        "answers": "0.2%",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "During training, we applied this operation to passages with a probability of $0.2 \\%$.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0677063d-f5f2-4759-a025-44a61bcb34b5",
        "questions": "How many languages are covered by the 184 million text samples used in the pre-training of the XLM-RoBERTa model?",
        "answers": "105",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We sampled a total of 184 million text samples from these sources, covering 105 languages.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "067fece7-f1ff-46dc-b45a-a79bd855fb35",
        "questions": "What is the purpose of inserting a CLS token for every 256 tokens in the MCLS method?",
        "answers": "To jointly capture the semantics of long texts.",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06999e1e-2b02-437e-a2cd-159aebba80d2",
        "questions": "What is the nDCG@10 score for the Dense+Sparse method in the M3-Embedding evaluation on the NarrativeQA dataset?",
        "answers": "60.1",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & Max Length & Avg & ar & de & en & es & fr & hi & it & ja & ko & pt & ru & th & zh \\\\\n  \\multicolumn{16}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 8192 & 53.6 & 45.1 & 52.6 & 57.0 & 78.0 & 75.7 & 43.7 & 70.9 & 36.2 & 25.7 & 82.6 & 61.3 & 33.6 & 34.6 \\\\\n  mDPR & 512 & 23.5 & 15.6 & 17.1 & 23.9 & 34.1 & 39.6 & 14.6 & 35.4 & 23.7 & 16.5 & 43.3 & 28.8 & 3.4 & 9.5 \\\\\n  mContriever & 512 & 31.0 & 25.4 & 24.2 & 28.7 & 44.6 & 50.3 & 17.2 & 43.2 & 27.3 & 23.6 & 56.6 & 37.7 & 9.0 & 15.3 \\\\\n  $\\mathrm{mE5}$ large & 512 & 34.2 & 33.0 & 26.9 & 33.0 & 51.1 & 49.5 & 21.0 & 43.1 & 29.9 & 27.1 & 58.7 & 42.4 & 15.9 & 13.2 \\\\\n  $\\mathrm{E5}$ mistral-7b & 8192 & 42.6 & 29.6 & 40.6 & 43.3 & 70.2 & 60.5 & 23.2 & 55.3 & 41.6 & 32.7 & 69.5 & 52.4 & 18.2 & 16.8 \\\\\n  text-embedding-ada-002 & 8191 & 32.5 & 16.3 & 34.4 & 38.7 & 59.8 & 53.9 & 8.0 & 46.5 & 28.6 & 20.7 & 60.6 & 34.8 & 9.0 & 11.2 \\\\\n  jina-embeddings-v2-base-en & 8192 & & & - & 37.0 & & & & & & & & & & \\\\\n  \\multicolumn{16}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 8192 & 52.5 & 47.6 & 46.1 & 48.9 & 74.8 & 73.8 & 40.7 & 62.7 & 50.9 & 42.9 & 74.4 & 59.5 & 33.6 & 26.0 \\\\\n  Sparse & 8192 & 62.2 & 58.7 & 53.0 & 62.1 & 87.4 & 82.7 & 49.6 & 74.7 & 53.9 & 47.9 & 85.2 & 72.9 & 40.3 & 40.5 \\\\\n  Multi-vec & 8192 & 57.6 & 56.6 & 50.4 & 55.8 & 79.5 & 77.2 & 46.6 & 66.8 & 52.8 & 48.8 & 77.5 & 64.2 & 39.4 & 32.7 \\\\\n  Dense+Sparse & 8192 & 64.8 & 63.0 & 56.4 & 64.2 & 88.7 & 84.2 & 52.3 & 75.8 & 58.5 & 53.1 & 86.0 & 75.6 & 42.9 & 42.0 \\\\\n  All & 8192 & 65.0 & 64.7 & 57.9 & 63.8 & 86.8 & 83.9 & 52.2 & 75.5 & 60.1 & 55.7 & 85.4 & 73.8 & 44.7 & 40.0 \\\\\n  \\multicolumn{16}{|l|}{M3-w.o.long} \\\\\n  Dense-w.o.long & 8192 & 41.2 & 35.4 & 35.2 & 37.5 & 64.0 & 59.3 & 28.8 & 53.1 & 41.7 & 29.8 & 63.5 & 51.1 & 19.5 & 16.5 \\\\\n  Dense-w.o.long (MCLS) & 8192 & 45.0 & 37.9 & 43.3 & 41.2 & 67.7 & 64.6 & 32.0 & 55.8 & 43.4 & 33.1 & 67.8 & 52.8 & 27.2 & 18.2 \\\\\n \n\\end{tabular}\n\nTable 3: Evaluation of multilingual long-doc retrieval on the MLDR test set (measured by nDCG@10).\nlingual Long-Doc Retrieval), which is curated by the multilingual articles from Wikipedia, Wudao and mC 4 (see Table 7), and NarrativeQA (Ko\u010disk\u00fd et al., 2018; G\u00fcnther et al., 2023), which is only for English. In addition to the previous baselines, we further introduce JinaEmbeddingv2 (G\u00fcnther et al., 2023), text-embedding-ada-002 and text-embedding-3-large from OpenAI given their outstanding long-doc retrieval capability. For Dense+Sparse method, we set $w_{1}=0.2$, $w_{2}=0.8$ and $w_{3}=0$ in equation(1). For All method, we set $w_{1}=0.15, w_{2}=0.5$ and $w_{3}=0.35$ in equation(1).\n\nThe evaluation result on MLDR is presented in Table 3. Interestingly, M3 (Sparse) turns out to be a more effective method for long document retrieval, which achieves another about 10 points improvement over the dense method. Besides, the multivector retrieval is also impressive, which brings $5.1+$ points improvement over M3 (Dense). Finally, the combination of different retrieval methods leads to a remarkable average performance of 65.0.\n\nTo explore the reason for M3-Embedding's competitiveness in long-document retrieval, we perform the ablation study by removing the long document data from the fine-tuning stage (denoted as w.o. long). After this modification, the dense method, i.e. Dense-w.o.long, can still outperform the majority of baselines, which indicates that its empirical advantage has been well established during the pre-training stage. We also propose a simple strategy, MCLS, to address this situation (no data or no GPU resource for document-retrieval fine-tuning). Experimental results indicate that MCLS can significantly improve the performance of document retrieval without training $(41.2 \\rightarrow 45.0)$.\n\\begin{tabular}{lcc}\n  Model & Max Length & nDCG@ 10 \\\\\n  Baselines (Prior Work) & & \\\\\n  mDPR & 512 & 16.3 \\\\\nmContriever & 512 & 23.3 \\\\\nmE5 large & 512 & 24.2 \\\\\nE5 mistral-7b & 8192 & 49.9 \\\\\ntexte-embedding-ada-002 & 8191 & 41.1 \\\\\ntext-embedding-3-large & 8191 & 51.6 \\\\\njina-embeddings-v2-base-en & 8192 & 39.4 \\\\\n  M3-Embedding (Our Work) & & \\\\\n  Dense & 8192 & 48.7 \\\\\nSparse & 8192 & 57.5 \\\\\nMulti-vec & 8192 & 55.4 \\\\\nDense+Sparse & 8192 & 60.1 \\\\\nAll & 8192 & $\\mathbf{6 1 . 7}$ \\\\\n \n\\end{tabular}\n\nTable 4: Evaluation on NarrativeQA (nDCG@10).\n\nWe make further analysis with NarrativeQA (Table 4), where we can make a similar observation as MLDR. Besides, with the growth of sequence length, our method gradually expands its advantage over baseline methods (Figure 5), which reflects its proficiency in handling long inputs.\n\n4.4 Ablation study\n\nSelf-knowledge distillation. The ablation study is performed to analyze the impact of self-knowledge distillation (skd). Particularly, we disable the distillation processing and have each retrieval method trained independently (denoted as M3-w.o.skd). According to our evaluation on MIRACL (Table 5), the original method, i.e. M3-w.skd, is able to achieve better performances than the ablation method in all settings, i.e., Dense, Sparse, Multivec . Notably, the impact is more pronounced for sparse retrieval. Such a result also reflects the incompatibility between dense and sparse retrieval methods. With skd, the incompatibility can be largely overcome. (More detailed results are avail-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Dense+Sparse & 8192 & 60.1",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06a8e22f-d57c-4f0d-9db3-466d1ba6cc61",
        "questions": "Which method achieved the highest average performance in the MLDR test set evaluation, and what was the score?",
        "answers": "All method, 65.0",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & Max Length & Avg & ar & de & en & es & fr & hi & it & ja & ko & pt & ru & th & zh \\\\\n  \\multicolumn{16}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 8192 & 53.6 & 45.1 & 52.6 & 57.0 & 78.0 & 75.7 & 43.7 & 70.9 & 36.2 & 25.7 & 82.6 & 61.3 & 33.6 & 34.6 \\\\\n  mDPR & 512 & 23.5 & 15.6 & 17.1 & 23.9 & 34.1 & 39.6 & 14.6 & 35.4 & 23.7 & 16.5 & 43.3 & 28.8 & 3.4 & 9.5 \\\\\n  mContriever & 512 & 31.0 & 25.4 & 24.2 & 28.7 & 44.6 & 50.3 & 17.2 & 43.2 & 27.3 & 23.6 & 56.6 & 37.7 & 9.0 & 15.3 \\\\\n  $\\mathrm{mE5}$ large & 512 & 34.2 & 33.0 & 26.9 & 33.0 & 51.1 & 49.5 & 21.0 & 43.1 & 29.9 & 27.1 & 58.7 & 42.4 & 15.9 & 13.2 \\\\\n  $\\mathrm{E5}$ mistral-7b & 8192 & 42.6 & 29.6 & 40.6 & 43.3 & 70.2 & 60.5 & 23.2 & 55.3 & 41.6 & 32.7 & 69.5 & 52.4 & 18.2 & 16.8 \\\\\n  text-embedding-ada-002 & 8191 & 32.5 & 16.3 & 34.4 & 38.7 & 59.8 & 53.9 & 8.0 & 46.5 & 28.6 & 20.7 & 60.6 & 34.8 & 9.0 & 11.2 \\\\\n  jina-embeddings-v2-base-en & 8192 & & & - & 37.0 & & & & & & & & & & \\\\\n  \\multicolumn{16}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 8192 & 52.5 & 47.6 & 46.1 & 48.9 & 74.8 & 73.8 & 40.7 & 62.7 & 50.9 & 42.9 & 74.4 & 59.5 & 33.6 & 26.0 \\\\\n  Sparse & 8192 & 62.2 & 58.7 & 53.0 & 62.1 & 87.4 & 82.7 & 49.6 & 74.7 & 53.9 & 47.9 & 85.2 & 72.9 & 40.3 & 40.5 \\\\\n  Multi-vec & 8192 & 57.6 & 56.6 & 50.4 & 55.8 & 79.5 & 77.2 & 46.6 & 66.8 & 52.8 & 48.8 & 77.5 & 64.2 & 39.4 & 32.7 \\\\\n  Dense+Sparse & 8192 & 64.8 & 63.0 & 56.4 & 64.2 & 88.7 & 84.2 & 52.3 & 75.8 & 58.5 & 53.1 & 86.0 & 75.6 & 42.9 & 42.0 \\\\\n  All & 8192 & 65.0 & 64.7 & 57.9 & 63.8 & 86.8 & 83.9 & 52.2 & 75.5 & 60.1 & 55.7 & 85.4 & 73.8 & 44.7 & 40.0 \\\\\n  \\multicolumn{16}{|l|}{M3-w.o.long} \\\\\n  Dense-w.o.long & 8192 & 41.2 & 35.4 & 35.2 & 37.5 & 64.0 & 59.3 & 28.8 & 53.1 & 41.7 & 29.8 & 63.5 & 51.1 & 19.5 & 16.5 \\\\\n  Dense-w.o.long (MCLS) & 8192 & 45.0 & 37.9 & 43.3 & 41.2 & 67.7 & 64.6 & 32.0 & 55.8 & 43.4 & 33.1 & 67.8 & 52.8 & 27.2 & 18.2 \\\\\n \n\\end{tabular}\n\nTable 3: Evaluation of multilingual long-doc retrieval on the MLDR test set (measured by nDCG@10).\nlingual Long-Doc Retrieval), which is curated by the multilingual articles from Wikipedia, Wudao and mC 4 (see Table 7), and NarrativeQA (Ko\u010disk\u00fd et al., 2018; G\u00fcnther et al., 2023), which is only for English. In addition to the previous baselines, we further introduce JinaEmbeddingv2 (G\u00fcnther et al., 2023), text-embedding-ada-002 and text-embedding-3-large from OpenAI given their outstanding long-doc retrieval capability. For Dense+Sparse method, we set $w_{1}=0.2$, $w_{2}=0.8$ and $w_{3}=0$ in equation(1). For All method, we set $w_{1}=0.15, w_{2}=0.5$ and $w_{3}=0.35$ in equation(1).\n\nThe evaluation result on MLDR is presented in Table 3. Interestingly, M3 (Sparse) turns out to be a more effective method for long document retrieval, which achieves another about 10 points improvement over the dense method. Besides, the multivector retrieval is also impressive, which brings $5.1+$ points improvement over M3 (Dense). Finally, the combination of different retrieval methods leads to a remarkable average performance of 65.0.\n\nTo explore the reason for M3-Embedding's competitiveness in long-document retrieval, we perform the ablation study by removing the long document data from the fine-tuning stage (denoted as w.o. long). After this modification, the dense method, i.e. Dense-w.o.long, can still outperform the majority of baselines, which indicates that its empirical advantage has been well established during the pre-training stage. We also propose a simple strategy, MCLS, to address this situation (no data or no GPU resource for document-retrieval fine-tuning). Experimental results indicate that MCLS can significantly improve the performance of document retrieval without training $(41.2 \\rightarrow 45.0)$.\n\\begin{tabular}{lcc}\n  Model & Max Length & nDCG@ 10 \\\\\n  Baselines (Prior Work) & & \\\\\n  mDPR & 512 & 16.3 \\\\\nmContriever & 512 & 23.3 \\\\\nmE5 large & 512 & 24.2 \\\\\nE5 mistral-7b & 8192 & 49.9 \\\\\ntexte-embedding-ada-002 & 8191 & 41.1 \\\\\ntext-embedding-3-large & 8191 & 51.6 \\\\\njina-embeddings-v2-base-en & 8192 & 39.4 \\\\\n  M3-Embedding (Our Work) & & \\\\\n  Dense & 8192 & 48.7 \\\\\nSparse & 8192 & 57.5 \\\\\nMulti-vec & 8192 & 55.4 \\\\\nDense+Sparse & 8192 & 60.1 \\\\\nAll & 8192 & $\\mathbf{6 1 . 7}$ \\\\\n \n\\end{tabular}\n\nTable 4: Evaluation on NarrativeQA (nDCG@10).\n\nWe make further analysis with NarrativeQA (Table 4), where we can make a similar observation as MLDR. Besides, with the growth of sequence length, our method gradually expands its advantage over baseline methods (Figure 5), which reflects its proficiency in handling long inputs.\n\n4.4 Ablation study\n\nSelf-knowledge distillation. The ablation study is performed to analyze the impact of self-knowledge distillation (skd). Particularly, we disable the distillation processing and have each retrieval method trained independently (denoted as M3-w.o.skd). According to our evaluation on MIRACL (Table 5), the original method, i.e. M3-w.skd, is able to achieve better performances than the ablation method in all settings, i.e., Dense, Sparse, Multivec . Notably, the impact is more pronounced for sparse retrieval. Such a result also reflects the incompatibility between dense and sparse retrieval methods. With skd, the incompatibility can be largely overcome. (More detailed results are avail-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Finally, the combination of different retrieval methods leads to a remarkable average performance of 65.0.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06a9232b-e797-497a-a972-7e1c1ad84f72",
        "questions": "In the evaluation of multilingual long-doc retrieval on the MLDR test set, what is the nDCG@10 score for the M3-Embedding Sparse method for the Japanese language?",
        "answers": "42.9",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & Max Length & Avg & ar & de & en & es & fr & hi & it & ja & ko & pt & ru & th & zh \\\\\n  \\multicolumn{16}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 8192 & 53.6 & 45.1 & 52.6 & 57.0 & 78.0 & 75.7 & 43.7 & 70.9 & 36.2 & 25.7 & 82.6 & 61.3 & 33.6 & 34.6 \\\\\n  mDPR & 512 & 23.5 & 15.6 & 17.1 & 23.9 & 34.1 & 39.6 & 14.6 & 35.4 & 23.7 & 16.5 & 43.3 & 28.8 & 3.4 & 9.5 \\\\\n  mContriever & 512 & 31.0 & 25.4 & 24.2 & 28.7 & 44.6 & 50.3 & 17.2 & 43.2 & 27.3 & 23.6 & 56.6 & 37.7 & 9.0 & 15.3 \\\\\n  $\\mathrm{mE5}$ large & 512 & 34.2 & 33.0 & 26.9 & 33.0 & 51.1 & 49.5 & 21.0 & 43.1 & 29.9 & 27.1 & 58.7 & 42.4 & 15.9 & 13.2 \\\\\n  $\\mathrm{E5}$ mistral-7b & 8192 & 42.6 & 29.6 & 40.6 & 43.3 & 70.2 & 60.5 & 23.2 & 55.3 & 41.6 & 32.7 & 69.5 & 52.4 & 18.2 & 16.8 \\\\\n  text-embedding-ada-002 & 8191 & 32.5 & 16.3 & 34.4 & 38.7 & 59.8 & 53.9 & 8.0 & 46.5 & 28.6 & 20.7 & 60.6 & 34.8 & 9.0 & 11.2 \\\\\n  jina-embeddings-v2-base-en & 8192 & & & - & 37.0 & & & & & & & & & & \\\\\n  \\multicolumn{16}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 8192 & 52.5 & 47.6 & 46.1 & 48.9 & 74.8 & 73.8 & 40.7 & 62.7 & 50.9 & 42.9 & 74.4 & 59.5 & 33.6 & 26.0 \\\\\n  Sparse & 8192 & 62.2 & 58.7 & 53.0 & 62.1 & 87.4 & 82.7 & 49.6 & 74.7 & 53.9 & 47.9 & 85.2 & 72.9 & 40.3 & 40.5 \\\\\n  Multi-vec & 8192 & 57.6 & 56.6 & 50.4 & 55.8 & 79.5 & 77.2 & 46.6 & 66.8 & 52.8 & 48.8 & 77.5 & 64.2 & 39.4 & 32.7 \\\\\n  Dense+Sparse & 8192 & 64.8 & 63.0 & 56.4 & 64.2 & 88.7 & 84.2 & 52.3 & 75.8 & 58.5 & 53.1 & 86.0 & 75.6 & 42.9 & 42.0 \\\\\n  All & 8192 & 65.0 & 64.7 & 57.9 & 63.8 & 86.8 & 83.9 & 52.2 & 75.5 & 60.1 & 55.7 & 85.4 & 73.8 & 44.7 & 40.0 \\\\\n  \\multicolumn{16}{|l|}{M3-w.o.long} \\\\\n  Dense-w.o.long & 8192 & 41.2 & 35.4 & 35.2 & 37.5 & 64.0 & 59.3 & 28.8 & 53.1 & 41.7 & 29.8 & 63.5 & 51.1 & 19.5 & 16.5 \\\\\n  Dense-w.o.long (MCLS) & 8192 & 45.0 & 37.9 & 43.3 & 41.2 & 67.7 & 64.6 & 32.0 & 55.8 & 43.4 & 33.1 & 67.8 & 52.8 & 27.2 & 18.2 \\\\\n \n\\end{tabular}\n\nTable 3: Evaluation of multilingual long-doc retrieval on the MLDR test set (measured by nDCG@10).\nlingual Long-Doc Retrieval), which is curated by the multilingual articles from Wikipedia, Wudao and mC 4 (see Table 7), and NarrativeQA (Ko\u010disk\u00fd et al., 2018; G\u00fcnther et al., 2023), which is only for English. In addition to the previous baselines, we further introduce JinaEmbeddingv2 (G\u00fcnther et al., 2023), text-embedding-ada-002 and text-embedding-3-large from OpenAI given their outstanding long-doc retrieval capability. For Dense+Sparse method, we set $w_{1}=0.2$, $w_{2}=0.8$ and $w_{3}=0$ in equation(1). For All method, we set $w_{1}=0.15, w_{2}=0.5$ and $w_{3}=0.35$ in equation(1).\n\nThe evaluation result on MLDR is presented in Table 3. Interestingly, M3 (Sparse) turns out to be a more effective method for long document retrieval, which achieves another about 10 points improvement over the dense method. Besides, the multivector retrieval is also impressive, which brings $5.1+$ points improvement over M3 (Dense). Finally, the combination of different retrieval methods leads to a remarkable average performance of 65.0.\n\nTo explore the reason for M3-Embedding's competitiveness in long-document retrieval, we perform the ablation study by removing the long document data from the fine-tuning stage (denoted as w.o. long). After this modification, the dense method, i.e. Dense-w.o.long, can still outperform the majority of baselines, which indicates that its empirical advantage has been well established during the pre-training stage. We also propose a simple strategy, MCLS, to address this situation (no data or no GPU resource for document-retrieval fine-tuning). Experimental results indicate that MCLS can significantly improve the performance of document retrieval without training $(41.2 \\rightarrow 45.0)$.\n\\begin{tabular}{lcc}\n  Model & Max Length & nDCG@ 10 \\\\\n  Baselines (Prior Work) & & \\\\\n  mDPR & 512 & 16.3 \\\\\nmContriever & 512 & 23.3 \\\\\nmE5 large & 512 & 24.2 \\\\\nE5 mistral-7b & 8192 & 49.9 \\\\\ntexte-embedding-ada-002 & 8191 & 41.1 \\\\\ntext-embedding-3-large & 8191 & 51.6 \\\\\njina-embeddings-v2-base-en & 8192 & 39.4 \\\\\n  M3-Embedding (Our Work) & & \\\\\n  Dense & 8192 & 48.7 \\\\\nSparse & 8192 & 57.5 \\\\\nMulti-vec & 8192 & 55.4 \\\\\nDense+Sparse & 8192 & 60.1 \\\\\nAll & 8192 & $\\mathbf{6 1 . 7}$ \\\\\n \n\\end{tabular}\n\nTable 4: Evaluation on NarrativeQA (nDCG@10).\n\nWe make further analysis with NarrativeQA (Table 4), where we can make a similar observation as MLDR. Besides, with the growth of sequence length, our method gradually expands its advantage over baseline methods (Figure 5), which reflects its proficiency in handling long inputs.\n\n4.4 Ablation study\n\nSelf-knowledge distillation. The ablation study is performed to analyze the impact of self-knowledge distillation (skd). Particularly, we disable the distillation processing and have each retrieval method trained independently (denoted as M3-w.o.skd). According to our evaluation on MIRACL (Table 5), the original method, i.e. M3-w.skd, is able to achieve better performances than the ablation method in all settings, i.e., Dense, Sparse, Multivec . Notably, the impact is more pronounced for sparse retrieval. Such a result also reflects the incompatibility between dense and sparse retrieval methods. With skd, the incompatibility can be largely overcome. (More detailed results are avail-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Sparse & 8192 & 62.2 & 58.7 & 53.0 & 62.1 & 87.4 & 82.7 & 49.6 & 74.7 & 53.9 & 47.9 & 85.2 & 72.9 & 40.3 & 40.5",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06acc243-7243-4f0b-8f81-5b322a3a39cf",
        "questions": "What is the role of the integration score $s_{inter}$ in the training process described in the document?",
        "answers": "The integration score $s_{inter}$ is employed as the teacher in the knowledge distillation process.",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this place, we simply employ the integration score $s_{\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as: $$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06b42b6f-1f94-43c8-98a7-75945b8c4949",
        "questions": "What are the weights set for $w_1$, $w_2$, and $w_3$ during the training process to reduce the impact of poor $s_{lex}$ accuracy?",
        "answers": "The weights are set as $w_{1}=1, w_{2}=0.3, w_{3}=1$.",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and \\lambda_{3}=1$ during the training process.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06b91d3d-2efe-493b-9f00-2b5a7c1e3a6e",
        "questions": "By how much can the batch size be increased when processing text with a length of 8192 using the improved batching strategy?",
        "answers": "The batch size can be increased by more than 20 times.",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06bba00e-615a-4c4a-979d-92bc2bd11b15",
        "questions": "What is the title of the work by Zhuyun Dai and Jamie Callan presented at the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval in 2020?",
        "answers": "Context-aware term weighting for first stage passage retrieval",
        "context": "Lotufo, and Rodrigo Nogueira. 2021. mmarco: A multilingual version of ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897.\n\nPeter B\u00fchlmann. 2012. Bagging, Boosting and Ensemble Methods, pages 985-1022. Springer Berlin Heidelberg, Berlin, Heidelberg.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.\n\nZhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 1533-1536, New York, NY, USA. Association for Computing Machinery.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.\n\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. COIL: Revisit exact lexical match in information retrieval with contextualized inverted list. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3030-3042, Online. Association for Computational Linguistics.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923.\n\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages $218-223$.\n\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: a Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 37-46, Melbourne, Australia. Association for Computational Linguistics.\n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\n\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.\n\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China. Association for Computational Linguistics.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.\n\nOmar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43 rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 39-48, New York, NY, USA. Association for Computing Machinery.\n\nMi-Young Kim, Juliano Rabelo, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Zhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 1533-1536, New York, NY, USA. Association for Computing Machinery.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06c6cb66-8e7f-4afd-8cc7-2f1c6923e8da",
        "questions": "In which year was the paper 'The Pile: An 800 gb dataset of diverse text for language modeling' by Leo Gao and others published?",
        "answers": "2020",
        "context": "Lotufo, and Rodrigo Nogueira. 2021. mmarco: A multilingual version of ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897.\n\nPeter B\u00fchlmann. 2012. Bagging, Boosting and Ensemble Methods, pages 985-1022. Springer Berlin Heidelberg, Berlin, Heidelberg.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.\n\nZhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 1533-1536, New York, NY, USA. Association for Computing Machinery.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.\n\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. COIL: Revisit exact lexical match in information retrieval with contextualized inverted list. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3030-3042, Online. Association for Computational Linguistics.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923.\n\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages $218-223$.\n\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: a Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 37-46, Melbourne, Australia. Association for Computational Linguistics.\n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\n\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.\n\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China. Association for Computational Linguistics.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.\n\nOmar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43 rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 39-48, New York, NY, USA. Association for Computing Machinery.\n\nMi-Young Kim, Juliano Rabelo, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06d2ed7c-e6ac-4d8a-8160-ff927c7550f2",
        "questions": "Is the work 'Jina embeddings 2: 8192-token general-purpose text embeddings for long documents' by Michael G\u00fcnther and others a preprint on arXiv?",
        "answers": "Yes",
        "context": "Lotufo, and Rodrigo Nogueira. 2021. mmarco: A multilingual version of ms marco passage ranking dataset. arXiv preprint arXiv:2108.13897.\n\nPeter B\u00fchlmann. 2012. Bagging, Boosting and Ensemble Methods, pages 985-1022. Springer Berlin Heidelberg, Berlin, Heidelberg.\n\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.\n\nZhuyun Dai and Jamie Callan. 2020. Context-aware term weighting for first stage passage retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 1533-1536, New York, NY, USA. Association for Computing Machinery.\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.\n\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. COIL: Revisit exact lexical match in information retrieval with contextualized inverted list. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3030-3042, Online. Association for Computational Linguistics.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923.\n\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science, pages $218-223$.\n\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: a Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 37-46, Melbourne, Australia. Association for Computational Linguistics.\n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. SIGIR '21, page 113-122, New York, NY, USA. Association for Computing Machinery.\n\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.\n\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China. Association for Computational Linguistics.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.\n\nOmar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43 rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '20, page 39-48, New York, NY, USA. Association for Computing Machinery.\n\nMi-Young Kim, Juliano Rabelo, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Michael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06d9e13b-3838-4cfa-8e6b-13855e53d8da",
        "questions": "What is the total number of text pairs and cross-lingual correspondences included in the M3-Embedding data curation process?",
        "answers": "1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06db83ae-66ca-4e67-83d1-01f3b98cd16c",
        "questions": "Does M3-Embedding support retrieval of documents in a different language from the query language?",
        "answers": "Yes",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06de025e-03e9-42fa-8696-db0e0a30a255",
        "questions": "What are the three sources of data collection used in the M3-Embedding data curation process?",
        "answers": "The unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8).",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06e86e99-2c7d-449e-80b8-b40fd5f0098e",
        "questions": "What is the nDCG@10 score for the 'Dense' model in the 'M3-w.skd' configuration for the Hindi language?",
        "answers": "59.5",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06f187c2-1805-4deb-83b9-996fcfe36f57",
        "questions": "Which model configuration achieved the highest average nDCG@10 score in the 'M3-w.o.skd' setting?",
        "answers": "Multi-vec",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Multi-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "06f7302d-62bd-49ae-a072-6dd9e0738191",
        "questions": "Did the 'RetroMAE + Unsup + Fine-tune' configuration achieve a higher nDCG@10 score for the French language compared to the 'Fine-tune' configuration?",
        "answers": "Yes",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "RetroMAE + Unsup + Fine-tune & Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "070227a3-7a37-40a5-972e-78a53176ee40",
        "questions": "What is the highest maximum sequence length achieved without using split-batch in the tabular data for unsupervised data?",
        "answers": "262",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$\\times$ & 262 & 25 & 6",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0703c5f1-5b4f-4458-a523-923afe963e96",
        "questions": "In the tabular data comparing different methods on MIRACL dataset using different tokenizers, which method achieves the highest performance?",
        "answers": "M3(All) with 71.5",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "M3(All) & XLM-R & 71.5 & 75.5 & 65.0",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "070cd0e8-dd17-4fb5-9005-155c763f6720",
        "questions": "Across all datasets in the tabular data, which method shows the highest variability in performance when using the XLM-R tokenizer?",
        "answers": "M3(All) with variability range between 65.0 and 75.5",
        "context": "Figure 4: Language and sequence length distribution of unsupervised data\n\\begin{tabular}{cccc}\n  \\multirow{2}{*}{ Use Split-batch } & \\multicolumn{3}{c}{ Max Length } \\\\\n\\cline { 2 - 4 } & 1024 & 4096 & 8192 \\\\\n $\\times$ & 262 & 25 & 6 \\\\\n$\\sqrt{ }$ & 855 & 258 & 130 \\\\\n \n\\end{tabular}\n\nTable 10: Maximum batch size per device under different experimental settings.\n\n\nFigure 5: NarrativeQA with variant sequence length.\n- Using the Analyzer from Lucene ${ }^{9}$ can significantly enhance the effectiveness of BM25. Lucene analyzer includes multiple steps typically including tokenization, stemming, stopword removal, etc, achieving better results than directly using the tokenzier of XLMRoBERTa. Additionally, it's worth noting that the vocabulary size of the tokenizer from XLM-RoBERTa is limited, resulting in fewer\n\\begin{tabular}{lllll}\n  Method & \\multicolumn{5}{l}{ Tokenizer MIRACL } & MKQA & MLDR \\\\\n  BM25 & Analyzer & 38.5 & 40.9 & 64.1 \\\\\nBM25 & XLM-R & 31.9 & 39.9 & 53.6 \\\\\nM3(Sparse) & XLM-R & 53.9 & 45.3 & 62.2 \\\\\nM3(All) & XLM-R & 71.5 & 75.5 & 65.0 \\\\\n \n\\end{tabular}\n\nTable 11: Comparison with the BM25 methods using different tokenizers.\nunique tokens after encoding documents (for example, on the MLDR dataset, the tokenizer of XLM-RoBERTa produces 1056 unique terms per article, while Lucene's analyzer generates 1451 unique terms, which is over $37 \\%$ more and will increase retrieval latency).\n- M3 outperforms BM25 models using the same tokenizer on all datasets, indicating that the learned weights are significantly better than the weights calculated by BM25.\n- The sparse retrieval of M3 outperforms BM25 on MIRACL and MKQA datasets. In long document retrieval (MLDR), M3's sparse doesn't surpass BM25 but achieves competitive performance. This suggests that BM25 remains a highly competitive baseline model. Exploring tokenizers that perform better for sparse representation is a worthwhile topic for future research.\n\n\\footnotetext{\n9. https://github.com/apache/lucene/tree/main/lucene/analysis/common/src/java/org/apache/lucene/analysis\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "M3(All) & XLM-R & 71.5 & 75.5 & 65.0",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0717eb47-3104-47c5-b152-5077a7806577",
        "questions": "How many text samples were sampled from the Pile, Wudao, and mC4 datasets, and how many languages do these samples cover?",
        "answers": "184 million text samples covering 105 languages",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "We sampled a total of 184 million text samples from these sources, covering 105 languages.",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0719a856-b791-44d4-a51e-79030364180e",
        "questions": "During pre-training with massive unsupervised data, what are the maximum lengths set for the query and passage, and what is the learning rate?",
        "answers": "The maximum length of query is 512, passage is 8192, and the learning rate is $5 \times 10^{-5}$.",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "For the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \times 10^{-5}$...",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "071c6ee3-2c95-4683-afba-87f598ee53b7",
        "questions": "How many GPUs were used for pre-training, and how many steps were conducted for this pre-training?",
        "answers": "32 Al00 (40GB) GPUs for 20,000 steps",
        "context": "A Details of Datasets\n\nA. 1 Collected Data\n\nThe language and length distribution (the number of tokens) of the unsupervised data are illustrated in Figure 4.\n\nWe observed that for long texts (e.g., the news in cc-news), the initial sentences tend to be summarizing statements, and the model can rely solely on the information presented in these initial sentences to establish relevant relationships. To prevent the model from focusing solely on these starting sentences, we implemented a strategy of randomly shuffling the order of segments within entire texts. Specifically, we divided the text into three segments, shuffled their order randomly, and recombined them. This approach allows relevant text segments to appear randomly at any position within the long sequence. During training, we applied this operation to passages with a probability of $0.2 \\%$.\n\nA. 2 Synthetic Data\n\nThe prompt for GPT3.5 is \"You are a curious AI assistant, please generate one specific and valuable question based on the following text. The generated question should revolve around the core content of this text, and avoid using pronouns (e.g., \"this\"). Note that you should generate only one question, without including additional content:\". The details of generated dataset are shown in Table 7.\n\nB Implementation Details\n\nB. 1 Experimental Hyperparameters\n\nWe adopt a further pre-trained XLM-RoBERTa ${ }^{8}$ as the foundational model. We extend the max position to 8192 and update the model via the RetroMAE (Xiao et al., 2022) method. The data comprises Pile (Gao et al., 2020), Wudao (Yuan et al., 2021), and mC4 (Raffel et al., 2020) datasets. We sampled a total of 184 million text samples from these sources, covering 105 languages. The maximum sequence length is 8192 and the learning rate is $7 \\times 10^{-5}$. The batch size is set to 32 and we accumulate the gradient over 16 steps. Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.\n\nFor the pre-training with the massive unsupervised data, the max length of query and passage is set to 512 and 8192 , respectively. The learning rate is $5 \\times 10^{-5}$, the warmup ratio is 0.1 and the weight\n\n\\footnotetext{\n8. https://huggingface.co/FacebookAI/xlm-roberta-large\n}\ndecay is 0.01 . This training process takes 25,000 steps. For training data with different sequence length ranges (e.g., $0-500,500-1000$, etc.), we use different batch sizes. The details are represented in Table 9. The second stage is conducted on 96 A800(80GB) GPUs.\n\nIn the fine-tuning stage, we sample 7 negatives for each query. Refer to Table 9 for the batch size. In the initial phase, we employed approximately 6000 steps to perform warm-up on dense embedding, sparse embedding and multi-vectors. Subsequently, we conducted unified training with selfknowledge distillation. These experiments were carried out on 24 A800(80GB) GPUs.\n\nB. 2 MCLS Method\n\nThe fine-tuning using long text can be constrained due to the absence of long text data or computation resources. In this situation, we propose a simple but effective method: MCLS(Multiple CLS) to enhance the model's ability without fine-tuning on long text. The MCLS method aims to utilize multiple CLS tokens to jointly capture the semantics of long texts. Specifically, we insert a CLS token for every fixed number of tokens (in our experiments, we insert a \"[CLS]\" for each 256 tokens), and each CLS token can capture semantic information from its neighboring tokens. Ultimately, the final text embedding is obtained by averaging the last hidden states of all CLS tokens.\n\nB. 3 Split-batch Method\n```\nAlgorithm 1 Pseudocode of split-batch.\n# enable gradient-checkpointing\nM3.gradient_checkpointing_enable()\nfor batch_data in loader:\n    # split the large batch into multiple sub-batch\n    for sub__batch_data in batch_data:\n        sub_emb = M3 (sub_batch_d\u00e0ta)\n        # only collect the embs\n        embs.append(sub_emb)\n# concatenate the outputs to get final embeddings\nembs = cat(embs)\n```\n\nAlgorthm 1 provides the pseudo-code of the splitbatch strategy. For the current batch, we partition it into multiple smaller sub-batches. For each subbatch we utilize the model to generate embeddings, discarding all intermediate activations via gradient checkpointing during the forward pass. Finally, we gather the encoded results from all sub-batch, and obtain the embeddings for the current batch. It is crucial to enable the gradient-checkpointing",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Pre-training is conducted on $32 \\mathrm{Al00}$ (40GB) GPUs for 20,000 steps.",
        "evidence_page_no": 13,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "071ffe50-24e1-4d44-9fca-49e8c93fdc20",
        "questions": "Which retrieval method achieved the highest performance in the Japanese (ja) language in the M3-Embedding retrieval, excluding the combined Dense+Sparse and All methods?",
        "answers": "Dense",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE5}_{\\text {large }}$ & $\\mathrm{E} 5_{\\text {mistral-7b }}$ & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 18.9 & 48.2 & 58.2 & 68.7 & 59.6 & 65.6 & 71.1 & 23.5 & 71.4 & 71.1 & 71.5 \\\\\n  da & 49.3 & 67.4 & 73.9 & 77.4 & 77.8 & 73.6 & 77.2 & 55.4 & 77.5 & 77.4 & 77.6 \\\\\n  de & 35.4 & 65.8 & 71.7 & 76.9 & 77.0 & 73.6 & 76.2 & 43.3 & 76.3 & 76.4 & 76.3 \\\\\n  es & 43.4 & 66.8 & 72.6 & 76.4 & 77.4 & 73.9 & 76.4 & 50.6 & 76.6 & 76.7 & 76.9 \\\\\n  fi & 46.3 & 56.2 & 70.2 & 74.0 & 72.0 & 72.7 & 75.1 & 51.1 & 75.3 & 75.3 & 75.5 \\\\\n  fr & 45.3 & 68.2 & 72.8 & 75.5 & 78.0 & 74.1 & 76.2 & 53.9 & 76.4 & 76.6 & 76.6 \\\\\n  he & 26.9 & 49.7 & 63.8 & 69.6 & 47.2 & 58.1 & 72.4 & 31.1 & 72.9 & 72.5 & 73.0 \\\\\n  hu & 38.2 & 60.4 & 69.7 & 74.7 & 75.0 & 71.2 & 74.7 & 44.6 & 74.6 & 74.9 & 75.0 \\\\\n  it & 45.2 & 66.0 & 72.3 & 76.8 & 77.1 & 73.6 & 76.0 & 52.5 & 76.4 & 76.3 & 76.5 \\\\\n  ja & 24.5 & 60.3 & 64.8 & 71.5 & 65.1 & 71.9 & 75.0 & 31.3 & 75.1 & 75.0 & 75.2 \\\\\n  km & 27.8 & 29.5 & 26.8 & 28.1 & 34.3 & 33.9 & 68.6 & 30.1 & 69.1 & 68.8 & 69.2 \\\\\n  ko & 27.9 & 50.9 & 59.7 & 68.1 & 59.4 & 63.9 & 71.6 & 31.4 & 71.7 & 71.6 & 71.8 \\\\\n  ms & 55.9 & 65.5 & 74.1 & 76.3 & 77.2 & 73.3 & 77.2 & 62.4 & 77.4 & 77.4 & 77.4 \\\\\n  nl & 56.2 & 68.2 & 73.7 & 77.8 & 79.1 & 74.2 & 77.4 & 62.4 & 77.6 & 77.7 & 77.6 \\\\\n  no & 52.1 & 66.7 & 73.5 & 77.3 & 76.6 & 73.3 & 77.1 & 57.9 & 77.2 & 77.4 & 77.3 \\\\\n  pl & 40.8 & 63.3 & 71.6 & 76.7 & 77.1 & 72.7 & 76.3 & 46.1 & 76.5 & 76.3 & 76.6 \\\\\n  pt & 44.9 & 65.5 & 72.0 & 73.5 & 77.5 & 73.7 & 76.3 & 50.9 & 76.4 & 76.5 & 76.4 \\\\\n  ru & 33.2 & 62.7 & 69.8 & 76.8 & 75.5 & 72.0 & 76.2 & 36.9 & 76.4 & 76.2 & 76.5 \\\\\n  sv & 54.6 & 66.9 & 73.2 & 77.6 & 78.3 & 74.0 & 76.9 & 59.6 & 77.2 & 77.4 & 77.4 \\\\\n  th & 37.8 & 53.8 & 66.9 & 76.0 & 67.4 & 65.2 & 76.4 & 42.0 & 76.5 & 76.5 & 76.6 \\\\\n  tr & 45.8 & 59.1 & 71.1 & 74.3 & 73.0 & 71.8 & 75.6 & 51.8 & 75.9 & 76.0 & 76.0 \\\\\n  vi & 46.6 & 63.4 & 70.9 & 75.4 & 70.9 & 71.1 & 76.6 & 51.8 & 76.7 & 76.8 & 76.9 \\\\\n  zh_cn & 31.0 & 63.7 & 68.1 & 56.6 & 69.3 & 70.7 & 74.6 & 35.4 & 74.9 & 74.7 & 75.0 \\\\\n  zh_hk & 35.0 & 62.8 & 68.0 & 58.1 & 65.1 & 69.6 & 73.8 & 39.8 & 74.1 & 74.0 & 74.3 \\\\\n  zh_tw & 33.5 & 64.0 & 67.9 & 58.1 & 65.8 & 69.7 & 73.5 & 37.7 & 73.5 & 73.6 & 73.6 \\\\\n  Avg & 39.9 & 60.6 & 67.9 & 70.9 & 70.1 & 69.5 & 75.1 & 45.3 & 75.3 & 75.3 & 75.5 \\\\\n \n\\end{tabular}\n\nTable 2: Cross-lingual retrieval performance on MKQA (measured by Recall@100).\nfectively trained by M3-Embedding, as it outperforms the typical BM25 methods in all languages. We can also observe the additional improvement from multi-vector retrieval, which relies on finegrained interactions between query and passage's embeddings to compute the relevance score. Finally, the collaboration of dense and sparse method (Dense+Sparse) leads to a further improvement over each individual method, and the collaboration of all three methods (All) brings forth the best performance.\n\n4.2 Cross-Lingual Retrieval\n\nWe make evaluation for the cross-lingual retrieval performance with the MKQA benchmark (Longpre et al., 2021), which includes queries in 25 nonEnglish languages. For each query, it needs to retrieve the passages containing answers from the English Wikipedia corpus. In our experiment, we make use of the well-processed corpus offered by the BEIR ${ }^{6}$ (Thakur et al., 2021). Following the previous study (Izacard et al., 2022), we report Recall@100 as the primary metric (Recall@20 is reported as an auxiliary metric in the Appendix C.1). For Dense+Sparse method and All method, we set the same weights as in MIRACL dataset.\n\nThe experiment result is shown in Table 2. Sim-\n\n\\footnotetext{\n6. https://huggingface.co/datasets/BeIR/nq\n}\nilar to our observation in multi-lingual retrieval, M3-Embedding continues to produce a superior performance, where it notably outperforms other baseline methods purely with its dense retrieval functionality (Dense). The collaboration of different retrieval methods brings in further improvements, leading to the best empirical performance of cross-lingual retrieval. Besides, we can also observe the following interesting results which are unique to this benchmark. Firstly, the performance gaps are not as significant as MIRACL, where competitive baselines like $\\mathrm{E} 5_{\\text {mistral-7b }}$ is able to produce similar or even better results on some of the testing languages. However, the baselines are prone to bad performances in many other languages, especially the low-resource languages, such as ar, km, he, etc. In contrast, M3-Embedding maintains relatively stable performances in all languages, which can largely be attributed to its pre-training over comprehensive unsupervised data. Secondly, although M3Embedding (Sparse) is still better than BM25, it performs badly compared with other methods. This is because there are only very limited co-existed terms for cross-lingual retrieval as the query and passage are presented in different languages.\n\n4.3 Multilingual Long-Doc Retrieval\n\nWe evaluate the retrieval performance with longer sequences with two benchmarks: MLDR (Multi-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ja & 24.5 & 60.3 & 64.8 & 71.5 & 65.1 & 71.9 & 75.0 & 31.3 & 75.1 & 75.0 & 75.2",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0726d44a-2152-43c7-821a-a15ef58c43ce",
        "questions": "What is the average Recall@100 score across all baseline methods for cross-lingual retrieval in the languages tested, and how does it compare to the average Recall@100 score of the M3-Embedding methods?",
        "answers": "Baseline methods: 60.6; M3-Embedding methods: 75.3",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE5}_{\\text {large }}$ & $\\mathrm{E} 5_{\\text {mistral-7b }}$ & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 18.9 & 48.2 & 58.2 & 68.7 & 59.6 & 65.6 & 71.1 & 23.5 & 71.4 & 71.1 & 71.5 \\\\\n  da & 49.3 & 67.4 & 73.9 & 77.4 & 77.8 & 73.6 & 77.2 & 55.4 & 77.5 & 77.4 & 77.6 \\\\\n  de & 35.4 & 65.8 & 71.7 & 76.9 & 77.0 & 73.6 & 76.2 & 43.3 & 76.3 & 76.4 & 76.3 \\\\\n  es & 43.4 & 66.8 & 72.6 & 76.4 & 77.4 & 73.9 & 76.4 & 50.6 & 76.6 & 76.7 & 76.9 \\\\\n  fi & 46.3 & 56.2 & 70.2 & 74.0 & 72.0 & 72.7 & 75.1 & 51.1 & 75.3 & 75.3 & 75.5 \\\\\n  fr & 45.3 & 68.2 & 72.8 & 75.5 & 78.0 & 74.1 & 76.2 & 53.9 & 76.4 & 76.6 & 76.6 \\\\\n  he & 26.9 & 49.7 & 63.8 & 69.6 & 47.2 & 58.1 & 72.4 & 31.1 & 72.9 & 72.5 & 73.0 \\\\\n  hu & 38.2 & 60.4 & 69.7 & 74.7 & 75.0 & 71.2 & 74.7 & 44.6 & 74.6 & 74.9 & 75.0 \\\\\n  it & 45.2 & 66.0 & 72.3 & 76.8 & 77.1 & 73.6 & 76.0 & 52.5 & 76.4 & 76.3 & 76.5 \\\\\n  ja & 24.5 & 60.3 & 64.8 & 71.5 & 65.1 & 71.9 & 75.0 & 31.3 & 75.1 & 75.0 & 75.2 \\\\\n  km & 27.8 & 29.5 & 26.8 & 28.1 & 34.3 & 33.9 & 68.6 & 30.1 & 69.1 & 68.8 & 69.2 \\\\\n  ko & 27.9 & 50.9 & 59.7 & 68.1 & 59.4 & 63.9 & 71.6 & 31.4 & 71.7 & 71.6 & 71.8 \\\\\n  ms & 55.9 & 65.5 & 74.1 & 76.3 & 77.2 & 73.3 & 77.2 & 62.4 & 77.4 & 77.4 & 77.4 \\\\\n  nl & 56.2 & 68.2 & 73.7 & 77.8 & 79.1 & 74.2 & 77.4 & 62.4 & 77.6 & 77.7 & 77.6 \\\\\n  no & 52.1 & 66.7 & 73.5 & 77.3 & 76.6 & 73.3 & 77.1 & 57.9 & 77.2 & 77.4 & 77.3 \\\\\n  pl & 40.8 & 63.3 & 71.6 & 76.7 & 77.1 & 72.7 & 76.3 & 46.1 & 76.5 & 76.3 & 76.6 \\\\\n  pt & 44.9 & 65.5 & 72.0 & 73.5 & 77.5 & 73.7 & 76.3 & 50.9 & 76.4 & 76.5 & 76.4 \\\\\n  ru & 33.2 & 62.7 & 69.8 & 76.8 & 75.5 & 72.0 & 76.2 & 36.9 & 76.4 & 76.2 & 76.5 \\\\\n  sv & 54.6 & 66.9 & 73.2 & 77.6 & 78.3 & 74.0 & 76.9 & 59.6 & 77.2 & 77.4 & 77.4 \\\\\n  th & 37.8 & 53.8 & 66.9 & 76.0 & 67.4 & 65.2 & 76.4 & 42.0 & 76.5 & 76.5 & 76.6 \\\\\n  tr & 45.8 & 59.1 & 71.1 & 74.3 & 73.0 & 71.8 & 75.6 & 51.8 & 75.9 & 76.0 & 76.0 \\\\\n  vi & 46.6 & 63.4 & 70.9 & 75.4 & 70.9 & 71.1 & 76.6 & 51.8 & 76.7 & 76.8 & 76.9 \\\\\n  zh_cn & 31.0 & 63.7 & 68.1 & 56.6 & 69.3 & 70.7 & 74.6 & 35.4 & 74.9 & 74.7 & 75.0 \\\\\n  zh_hk & 35.0 & 62.8 & 68.0 & 58.1 & 65.1 & 69.6 & 73.8 & 39.8 & 74.1 & 74.0 & 74.3 \\\\\n  zh_tw & 33.5 & 64.0 & 67.9 & 58.1 & 65.8 & 69.7 & 73.5 & 37.7 & 73.5 & 73.6 & 73.6 \\\\\n  Avg & 39.9 & 60.6 & 67.9 & 70.9 & 70.1 & 69.5 & 75.1 & 45.3 & 75.3 & 75.3 & 75.5 \\\\\n \n\\end{tabular}\n\nTable 2: Cross-lingual retrieval performance on MKQA (measured by Recall@100).\nfectively trained by M3-Embedding, as it outperforms the typical BM25 methods in all languages. We can also observe the additional improvement from multi-vector retrieval, which relies on finegrained interactions between query and passage's embeddings to compute the relevance score. Finally, the collaboration of dense and sparse method (Dense+Sparse) leads to a further improvement over each individual method, and the collaboration of all three methods (All) brings forth the best performance.\n\n4.2 Cross-Lingual Retrieval\n\nWe make evaluation for the cross-lingual retrieval performance with the MKQA benchmark (Longpre et al., 2021), which includes queries in 25 nonEnglish languages. For each query, it needs to retrieve the passages containing answers from the English Wikipedia corpus. In our experiment, we make use of the well-processed corpus offered by the BEIR ${ }^{6}$ (Thakur et al., 2021). Following the previous study (Izacard et al., 2022), we report Recall@100 as the primary metric (Recall@20 is reported as an auxiliary metric in the Appendix C.1). For Dense+Sparse method and All method, we set the same weights as in MIRACL dataset.\n\nThe experiment result is shown in Table 2. Sim-\n\n\\footnotetext{\n6. https://huggingface.co/datasets/BeIR/nq\n}\nilar to our observation in multi-lingual retrieval, M3-Embedding continues to produce a superior performance, where it notably outperforms other baseline methods purely with its dense retrieval functionality (Dense). The collaboration of different retrieval methods brings in further improvements, leading to the best empirical performance of cross-lingual retrieval. Besides, we can also observe the following interesting results which are unique to this benchmark. Firstly, the performance gaps are not as significant as MIRACL, where competitive baselines like $\\mathrm{E} 5_{\\text {mistral-7b }}$ is able to produce similar or even better results on some of the testing languages. However, the baselines are prone to bad performances in many other languages, especially the low-resource languages, such as ar, km, he, etc. In contrast, M3-Embedding maintains relatively stable performances in all languages, which can largely be attributed to its pre-training over comprehensive unsupervised data. Secondly, although M3Embedding (Sparse) is still better than BM25, it performs badly compared with other methods. This is because there are only very limited co-existed terms for cross-lingual retrieval as the query and passage are presented in different languages.\n\n4.3 Multilingual Long-Doc Retrieval\n\nWe evaluate the retrieval performance with longer sequences with two benchmarks: MLDR (Multi-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Avg & 39.9 & 60.6 & 67.9 & 70.9 & 70.1 & 69.5 & 75.1 & 45.3 & 75.3 & 75.3 & 75.5",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "074712fc-9efd-4e7b-a0d0-4ce1d581f828",
        "questions": "In the comparison of retrieval methods for the Spanish (es) language, to what extent does the Dense+Sparse method outperform BM25?",
        "answers": "Dense+Sparse scored 76.7, while BM25 scored 43.4, resulting in a difference of 33.3.",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE5}_{\\text {large }}$ & $\\mathrm{E} 5_{\\text {mistral-7b }}$ & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 18.9 & 48.2 & 58.2 & 68.7 & 59.6 & 65.6 & 71.1 & 23.5 & 71.4 & 71.1 & 71.5 \\\\\n  da & 49.3 & 67.4 & 73.9 & 77.4 & 77.8 & 73.6 & 77.2 & 55.4 & 77.5 & 77.4 & 77.6 \\\\\n  de & 35.4 & 65.8 & 71.7 & 76.9 & 77.0 & 73.6 & 76.2 & 43.3 & 76.3 & 76.4 & 76.3 \\\\\n  es & 43.4 & 66.8 & 72.6 & 76.4 & 77.4 & 73.9 & 76.4 & 50.6 & 76.6 & 76.7 & 76.9 \\\\\n  fi & 46.3 & 56.2 & 70.2 & 74.0 & 72.0 & 72.7 & 75.1 & 51.1 & 75.3 & 75.3 & 75.5 \\\\\n  fr & 45.3 & 68.2 & 72.8 & 75.5 & 78.0 & 74.1 & 76.2 & 53.9 & 76.4 & 76.6 & 76.6 \\\\\n  he & 26.9 & 49.7 & 63.8 & 69.6 & 47.2 & 58.1 & 72.4 & 31.1 & 72.9 & 72.5 & 73.0 \\\\\n  hu & 38.2 & 60.4 & 69.7 & 74.7 & 75.0 & 71.2 & 74.7 & 44.6 & 74.6 & 74.9 & 75.0 \\\\\n  it & 45.2 & 66.0 & 72.3 & 76.8 & 77.1 & 73.6 & 76.0 & 52.5 & 76.4 & 76.3 & 76.5 \\\\\n  ja & 24.5 & 60.3 & 64.8 & 71.5 & 65.1 & 71.9 & 75.0 & 31.3 & 75.1 & 75.0 & 75.2 \\\\\n  km & 27.8 & 29.5 & 26.8 & 28.1 & 34.3 & 33.9 & 68.6 & 30.1 & 69.1 & 68.8 & 69.2 \\\\\n  ko & 27.9 & 50.9 & 59.7 & 68.1 & 59.4 & 63.9 & 71.6 & 31.4 & 71.7 & 71.6 & 71.8 \\\\\n  ms & 55.9 & 65.5 & 74.1 & 76.3 & 77.2 & 73.3 & 77.2 & 62.4 & 77.4 & 77.4 & 77.4 \\\\\n  nl & 56.2 & 68.2 & 73.7 & 77.8 & 79.1 & 74.2 & 77.4 & 62.4 & 77.6 & 77.7 & 77.6 \\\\\n  no & 52.1 & 66.7 & 73.5 & 77.3 & 76.6 & 73.3 & 77.1 & 57.9 & 77.2 & 77.4 & 77.3 \\\\\n  pl & 40.8 & 63.3 & 71.6 & 76.7 & 77.1 & 72.7 & 76.3 & 46.1 & 76.5 & 76.3 & 76.6 \\\\\n  pt & 44.9 & 65.5 & 72.0 & 73.5 & 77.5 & 73.7 & 76.3 & 50.9 & 76.4 & 76.5 & 76.4 \\\\\n  ru & 33.2 & 62.7 & 69.8 & 76.8 & 75.5 & 72.0 & 76.2 & 36.9 & 76.4 & 76.2 & 76.5 \\\\\n  sv & 54.6 & 66.9 & 73.2 & 77.6 & 78.3 & 74.0 & 76.9 & 59.6 & 77.2 & 77.4 & 77.4 \\\\\n  th & 37.8 & 53.8 & 66.9 & 76.0 & 67.4 & 65.2 & 76.4 & 42.0 & 76.5 & 76.5 & 76.6 \\\\\n  tr & 45.8 & 59.1 & 71.1 & 74.3 & 73.0 & 71.8 & 75.6 & 51.8 & 75.9 & 76.0 & 76.0 \\\\\n  vi & 46.6 & 63.4 & 70.9 & 75.4 & 70.9 & 71.1 & 76.6 & 51.8 & 76.7 & 76.8 & 76.9 \\\\\n  zh_cn & 31.0 & 63.7 & 68.1 & 56.6 & 69.3 & 70.7 & 74.6 & 35.4 & 74.9 & 74.7 & 75.0 \\\\\n  zh_hk & 35.0 & 62.8 & 68.0 & 58.1 & 65.1 & 69.6 & 73.8 & 39.8 & 74.1 & 74.0 & 74.3 \\\\\n  zh_tw & 33.5 & 64.0 & 67.9 & 58.1 & 65.8 & 69.7 & 73.5 & 37.7 & 73.5 & 73.6 & 73.6 \\\\\n  Avg & 39.9 & 60.6 & 67.9 & 70.9 & 70.1 & 69.5 & 75.1 & 45.3 & 75.3 & 75.3 & 75.5 \\\\\n \n\\end{tabular}\n\nTable 2: Cross-lingual retrieval performance on MKQA (measured by Recall@100).\nfectively trained by M3-Embedding, as it outperforms the typical BM25 methods in all languages. We can also observe the additional improvement from multi-vector retrieval, which relies on finegrained interactions between query and passage's embeddings to compute the relevance score. Finally, the collaboration of dense and sparse method (Dense+Sparse) leads to a further improvement over each individual method, and the collaboration of all three methods (All) brings forth the best performance.\n\n4.2 Cross-Lingual Retrieval\n\nWe make evaluation for the cross-lingual retrieval performance with the MKQA benchmark (Longpre et al., 2021), which includes queries in 25 nonEnglish languages. For each query, it needs to retrieve the passages containing answers from the English Wikipedia corpus. In our experiment, we make use of the well-processed corpus offered by the BEIR ${ }^{6}$ (Thakur et al., 2021). Following the previous study (Izacard et al., 2022), we report Recall@100 as the primary metric (Recall@20 is reported as an auxiliary metric in the Appendix C.1). For Dense+Sparse method and All method, we set the same weights as in MIRACL dataset.\n\nThe experiment result is shown in Table 2. Sim-\n\n\\footnotetext{\n6. https://huggingface.co/datasets/BeIR/nq\n}\nilar to our observation in multi-lingual retrieval, M3-Embedding continues to produce a superior performance, where it notably outperforms other baseline methods purely with its dense retrieval functionality (Dense). The collaboration of different retrieval methods brings in further improvements, leading to the best empirical performance of cross-lingual retrieval. Besides, we can also observe the following interesting results which are unique to this benchmark. Firstly, the performance gaps are not as significant as MIRACL, where competitive baselines like $\\mathrm{E} 5_{\\text {mistral-7b }}$ is able to produce similar or even better results on some of the testing languages. However, the baselines are prone to bad performances in many other languages, especially the low-resource languages, such as ar, km, he, etc. In contrast, M3-Embedding maintains relatively stable performances in all languages, which can largely be attributed to its pre-training over comprehensive unsupervised data. Secondly, although M3Embedding (Sparse) is still better than BM25, it performs badly compared with other methods. This is because there are only very limited co-existed terms for cross-lingual retrieval as the query and passage are presented in different languages.\n\n4.3 Multilingual Long-Doc Retrieval\n\nWe evaluate the retrieval performance with longer sequences with two benchmarks: MLDR (Multi-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "es & 43.4 & 66.8 & 72.6 & 76.4 & 77.4 & 73.9 & 76.4 & 50.6 & 76.6 & 76.7 & 76.9",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "074a2910-319e-43bc-96d7-027b5c3d2d4d",
        "questions": "How many text pairs and cross-lingual correspondences are included in the data curated by M3-Embedding for training?",
        "answers": "1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "074af7dc-29c4-48b3-971b-d58f50b7865f",
        "questions": "Which datasets are integrated for fine-tuning data in Chinese as part of M3-Embedding's data collection process?",
        "answers": "DuReader, mMARCO-ZH, $\\mathrm{T}^{2}$-Ranking, LawGPT, CMedQAv2, NLI$\\mathrm{zh}^{2}$, and LeCaRDv2",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT (Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023).",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "074b68af-45e8-46a0-94a5-a5d08fe0b3ed",
        "questions": "What sources are used in data synthesization for creating MultiLongDoc in the M3-Embedding's data curation?",
        "answers": "Wikipedia, Wudao, and mC4 datasets",
        "context": "haria, 2020; Humeau et al., 2020), which applies fine-grained interactions for the text encoder's outputs to compute the embedding similarity. Finally, the text embeddings can also be transformed into term weights, which facilitates sparse or lexical retrieval (Luan et al., 2021; Dai and Callan, 2020; Lin and Ma, 2021). Typically, the above retrieval methods are realized by different embedding models. To the best of our knowledge, no existing method is able to unify all these functionalities.\n\nDespite the substantial technical advancement, most of the existing text embeddings are developed only for English, where other languages are lagging behind. To mitigate this problem, continual efforts are presented from multiple directions. One is the development of pre-trained multi-lingual text encoders, such as mBERT (Pires et al., 2019), mT5 (Xue et al., 2021), XLM-R (Conneau et al., 2020). Another one is the curation of training and evaluation data for multi-lingual text embeddings, e.g., MIRACL (Zhang et al., 2023c), mMARCO (Bonifacio et al., 2021), Mr. TyDi (Zhang et al., 2021b), MKQA (Longpre et al., 2021). At the same time, the multi-lingual text embeddings are continually developed from the community, e.g., mDPR (Zhang et al., 2023b), mContriever (Izacard et al., 2022), mE5 (Wang et al., 2022), etc. However, the current progress is still far from enough given the notable gap with English models and the huge imbalance between different languages.\n\n3 M3-Embedding\n\nM3-Embedding realizes three-fold versatility. It supports a wide variety of languages and handles input data of different granularities. Besides, it unifies the common retrieval functionalities of text embeddings. Formally, given a query $q$ in an arbitrary language $x$, it is able to retrieve document $d$ in language $y$ from the corpus $D^{y}: d^{y} \\leftarrow \\mathrm{fn}^{*}\\left(q^{x}, D^{y}\\right)$. In this place, $\\mathrm{fn}^{*}(\\cdot)$ belongs to any of the functions: dense, lexical, or multi-vector retrieval; $y$ can be another language or the same language as $x$.\n\n3.1 Data Curation\n\nM3-Embedding calls for a large-scale and diverse multi-lingual dataset. In this work, we perform comprehensive data collection from three sources: the unsupervised data from unlabeled corpora, the fine-tuning data from labeled corpora, and the finetuning data via synthesization (shown as Table 8). The three data sources complement to each other,\nwhich are applied to different stages of the training process. Particularly, the unsupervised data is curated by extracting the rich-semantic structures, e.g., title-body, title-abstract, instruction-output, etc., within a wide variety of multi-lingual corpora, including Wikipedia, S2ORC (Lo et al., 2020), xP3 (Muennighoff et al., 2023), mC4 (Raffel et al., 2020), CC-News (Hamborg et al., 2017) and the well-curated data from MTP (Xiao et al., 2023). To learn the unified embedding space for cross-lingual semantic matching, the parallel sentences are introduced from two translation datasets, NLLB (NLLB Team et al., 2022) and CCMatrix (Schwenk et al., 2021). The raw data is filtered to remove potential bad contents and low-relevance samples. In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.\n\nBesides, we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. For English, we incorporate 8 datasets, including HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), NQ (Kwiatkowski et al., 2019), MS MARCO (Nguyen et al., 2016), COLIEE (Kim et al., 2023), PubMedQA (Jin et al., 2019), SQuAD (Rajpurkar et al., 2016), and NLI data from SimCSE (Gao et al., 2021b). For Chinese, we integrate 7 datasets, including DuReader (He et al., 2018), mMARCO-ZH (Bonifacio et al., 2021), $\\mathrm{T}^{2}$-Ranking (Xie et al., 2023), LawGPT(Liu et al., 2023), CMedQAv2 (Zhang et al., 2018), NLI$\\mathrm{zh}^{2}$, and LeCaRDv2 (Li et al., 2023). For other languages, we leverage the training data from Mr . Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).\n\nFinally, we generate synthetic data to mitigate the shortage of long document retrieval tasks and introduce extra multi-lingual fine-tuning data (denoted as MultiLongDoc). Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC 4 datasets and randomly choose paragraphs from them. Then we use GPT3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the fine-tuning data. Detailed specifications are presented in Appendix A.2.\n\n3.2 Hybrid Retrieval\n\nM3-Embedding unifies the common retrieval functionalities of the embedding model, i.e. dense retrieval, lexical (sparse) retrieval, and multi-vector\n\n\\footnotetext{\n2. https://huggingface.co/datasets/shibing624/nli-zh-all\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021), and mC 4 datasets and randomly choose paragraphs from them.",
        "evidence_page_no": 2,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07716ddf-e7a4-42f7-a7ef-50b1190fe075",
        "questions": "What is the nDCG@10 performance for the MIRACL dev set when using the M3-w.skd model with Sparse retrieval method?",
        "answers": "53.9",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Sparse & 53.9 \\",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07750580-bdc5-4359-a45b-2246d30ce37c",
        "questions": "Comparing the Dense retrieval method performance, is the RetroMAE + Unsup + Fine-tune training method more effective than the Fine-tune training method on the MIRACL dev set?",
        "answers": "Yes",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Model (Dense) & MIRACL \\\\\\n\\hline Fine-tune & 60.5 \\\\\\nRetroMAE + Fine-tune & 66.1 \\\\\\nRetroMAE + Unsup + Fine-tune & 69.2",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0777a678-3c89-4d01-883e-3b0c72e6ea7b",
        "questions": "What is the percentage increase in nDCG@10 when using M3-w.o.skd model with Multi-vec compared to Sparse retrieval methods on the MIRACL dev set?",
        "answers": "88.83%",
        "context": "\\begin{tabular}{ll|c}\n  Model & & MIRACL \\\\\n  \\multirow{3}{*}{ M3-w.skd } & Dense & 69.2 \\\\\n& Sparse & 53.9 \\\\\n& Multi-vec & 70.5 \\\\\n  & Dense & 68.7 \\\\\n\\multirow{2}{*}{ M3-w.o.skd } & Sparse & 36.7 \\\\\n& Multi-vec & 69.3 \\\\\n \n\\end{tabular}\n\nTable 5: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{l|c}\n  Model (Dense) & MIRACL \\\\\n  Fine-tune & 60.5 \\\\\nRetroMAE + Fine-tune & 66.1 \\\\\nRetroMAE + Unsup + Fine-tune & 69.2 \\\\\n \n\\end{tabular}\n\nTable 6: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).\nable in Appendix C.1.)\nImpact of multi-stage training. We also make explorations for the impacts from different training stages. Fine-tuning indicates the direct fine-tuning from XLM-RoBERTA (Conneau et al., 2020); RetroMAE+Fine-tuning refers to the fine-tuning on the pre-trained model from RetroMAE (Xiao et al., 2022). Meanwhile, RetroMAE+Unsup+Finetuning involves fine-tuning on a model that is trained with RetroMAE and then pre-trained on unsupervised data. The results are presented in Table 6. We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model. (More detailed results are available in Appendix C.1.)\n\n5 Conclusion\n\nIn this paper, we introduce M3-Embedding, which substantially advances the versatility of text embeddings in terms of supporting multi-lingual retrieval, handling input of diverse granularities, and unifying different retrieval functionalities. M3Embedding presents three technical contributions: self-knowledge distillation, efficient batching, and high-quality curation of data. The effectiveness of M3-Embedding is empirically verified, where it leads to superior performances on multi-lingual retrieval, cross-lingual retrieval, and multi-lingual long-document retrieval tasks.\n\nLimitations\n\nFirst of all, while our proposed M3-Embedding model achieves state-of-the-art performance on popular multi-lingual and cross-lingual bench-\nmarks such as MIRACL and MKQA, it is important to acknowledge that the generalizability of our approach to diverse datasets and real-world scenarios needs to be further investigated. Different datasets may have varying characteristics and challenges that could affect the performance of our model. Secondly, while M3-Embedding is designed to process inputs of different granularities, including long documents of up to 8192 tokens, we acknowledge that processing extremely long documents could pose challenges in terms of computational resources and model efficiency. The performance of our model on very long documents or documents exceeding the specified token limit needs to be further investigated. Furthermore, we claim support for more than 100 working languages in M3-Embedding. However, the potential variations in performance across different languages are not thoroughly discussed. Further analysis and evaluation on a broader range of languages are necessary to understand the robustness and effectiveness of our model across different language families and linguistic characteristics.\n\nEthics Consideration\n\nOur work proposes a new embedding model called M3-Embedding, which is distingulished for its versality in multi-linguality, multi-functionality and multi-granularity. Because our model will be publicly avaliable, it is influenced by the inherent impacts of open-source model. Moreover, we use the multilingual data including all kinds of languages in the training of M3-Embedding. However, due to the uneven distribution of training data for different languages, the model's performance may vary across languages, which could potentially be seen as discriminatory or unfair. We ensure that our work is conformant to the ACL Ethics Policy ${ }^{7}$.\n\nAcknowledgements\n\nWe would like to thank anonymous reviewers for their helpful feedback, and ACL 2024 and ACL Rolling Review organizers for their efforts. This research is supported by National Science and Technology Major Project (2023ZD0121504).\n\nReferences\n\nLuiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto\n\n\\footnotetext{\n7. https://www.aclweb.org/portal/content/acl-code-ethics\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Dense & Sparse & Multi-vec //\\\\\\n68.7 & 36.7 & 69.3",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "079092e4-537d-4c63-95a2-888492747f90",
        "questions": "What is the average length of documents for the French language in the MultiLongDoc dataset?",
        "answers": "9,659",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "fr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "0791002d-fc4f-4338-972b-8e79021fcf08",
        "questions": "In the training data specifications, what is the size of unsupervised data sourced from MTP for English and Chinese?",
        "answers": "291.1M",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "MTP & EN, ZH & 291.1M",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "079720d1-63aa-49e3-afdb-2d547d0dfcc6",
        "questions": "What pattern can be observed in the batch size for unsupervised training as the sequence length increases from 0 to 8192?",
        "answers": "The batch size decreases as the sequence length increases.",
        "context": "\\begin{tabular}{ccccccc}\n  Language & Source & \\#train & \\#dev & \\#test & \\#cropus & Avg. Length of Docs \\\\\n  ar & Wikipedia & 1,817 & 200 & 200 & 7,607 & 9,428 \\\\\nde & Wikipedia, mC4 & 1,847 & 200 & 200 & 10,000 & 9,039 \\\\\nen & Wikipedia & 10,000 & 200 & 800 & 200,000 & 3,308 \\\\\nes & Wikipedia, mC4 & 2,254 & 200 & 200 & 9,551 & 8,771 \\\\\nfr & Wikipedia & 1,608 & 200 & 200 & 10,000 & 9,659 \\\\\nhi & Wikipedia & 1,618 & 200 & 200 & 3,806 & 5,555 \\\\\nit & Wikipedia & 2,151 & 200 & 200 & 10,000 & 9,195 \\\\\nja & Wikipedia & 2,262 & 200 & 200 & 10,000 & 9,297 \\\\\nko & Wikipedia & 2,198 & 200 & 200 & 6,176 & 7,832 \\\\\npt & Wikipedia & 1,845 & 200 & 200 & 6,569 & 7,922 \\\\\nru & Wikipedia & 1,864 & 200 & 200 & 10,000 & 9,723 \\\\\nth & mC4 & 1,970 & 200 & 200 & 10,000 & 8,089 \\\\\nzh & Wikipedia, Wudao & 10,000 & 200 & 800 & 200,000 & 4,249 \\\\\n  Total & - & 41,434 & 2,600 & 3,800 & 493,709 & 4,737 \\\\\n \n\\end{tabular}\n\nTable 7: Specifications of MultiLongDoc dataset.\n\\begin{tabular}{|c|c|c|}\n  Data Source & Language & Size \\\\\n  \\multicolumn{3}{|c|}{Unsupervised Data} \\\\\n  MTP & EN, ZH & 291.1M \\\\\n  S2ORC, Wikipeida & EN & 48.3 M \\\\\n  $$\\begin{aligned}\n& \\text { xP3, mC4, } \\\\\n& \\text { CC-News }\n\\end{aligned}$$ & Multi-Lingual & 488.4 M \\\\\n  NLLB, CCMatrix & Cross-Lingual & 391.3M \\\\\n  CodeSearchNet & Text-Code & 344.1 K \\\\\n  Total & $-$ & $1.2 B$ \\\\\n  \\multicolumn{3}{|c|}{Fine-tuning Data} \\\\\n  MS MARCO, HotpotQA, NQ, NLI, etc. & EN & 1.1 M \\\\\n  DuReader, $\\mathrm{T}^{2}$-Ranking, NLI-zh, etc. & ZH & 386.6 K \\\\\n  \\begin{tabular}{l}\nMIRACL\uff0c \\\\\nMr.TyDi\n\\end{tabular} & Multi-Lingual & 88.9 K \\\\\n  MultiLongDoc & Multi-Lingual & 41.4 K \\\\\n \n\\end{tabular}\n\nTable 8: Specification of training data.\nstrategy; otherwise, the intermediate activations for each sub-batch will continuously accumulate, ultimately occupying the same amount of GPU memory as traditional methods.\n\nIn Table 10, we investigate the impact of splitbatch on batch size. It can be observed that, with the split-batch enabled, there is a significant increase in batch size. Simultaneously, the increase becomes more pronounced with longer text lengths, and in the case of a length of 8192 , enabling splitbatch results in a growth of batch size by over 20 times.\n\\begin{tabular}{lcc}\n  \\multirow{2}{*}{ Length Range } & \\multicolumn{2}{c}{ Batch Size } \\\\\n\\cline { 2 - 3 } & Unsupervised & Fine-tuning \\\\\n  $0-500$ & 67,200 & 1,152 \\\\\n$500-1000$ & 54,720 & 768 \\\\\n$1000-2000$ & 37,248 & 480 \\\\\n$2000-3000$ & 27,648 & 432 \\\\\n$3000-4000$ & 21,504 & 336 \\\\\n$4000-5000$ & 17,280 & 336 \\\\\n$5000-6000$ & 15,072 & 288 \\\\\n$6000-7000$ & 12,288 & 240 \\\\\n$7000-8192$ & 9,984 & 192 \\\\\n \n\\end{tabular}\n\nTable 9: Detailed total batch size used in training for data with different sequence length ranges.\n\nC More Results\n\nC. 1 Additional Resutls\n\nIn this section, we present additional evaluation results on the MIRACL and MKQA benchmarks. As shown in Table 12 and 13, M3-Embedding outperforms all baselines on average.\n\nThe detailed results of ablation studies of selfknowledge distillation and multi-stage training on the MIRACL dev set are shown in Table 14 and Table 15.\n\nC. 2 Different Tokenizer for BM25\n\nWe investigate the impact of different tokenizers on the BM25 method, and the results are shown in Table 11. We can observe that:",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Length Range & Unsupervised & Fine-tuning $0-500$ & 67,200 & 1,152 $500-1000$ & 54,720 & 768 $1000-2000$ & 37,248 & 480 $2000-3000$ & 27,648 & 432 $3000-4000$ & 21,504 & 336 $4000-5000$ & 17,280 & 336 $5000-6000$ & 15,072 & 288 $6000-7000$ & 12,288 & 240 $7000-8192$ & 9,984 & 192",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07991818-0e7b-4cac-8f1d-04cc8eb0f052",
        "questions": "What is the Recall@100 score for language 'zh' using the Multi-vec model on the MIRACL dataset?",
        "answers": "97.3",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "079aec91-13d7-4fa7-9f06-74c4bd4ad557",
        "questions": "Among the Baselines models, which one achieved the highest Recall@20 for the language 'es' on the MKQA dataset?",
        "answers": "E 5 mistral-7b",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "079b2d72-86de-4552-9152-7d3846c4f1c4",
        "questions": "Calculate the difference in Recall@20 scores between the Dense and Sparse models for the language 'ar' on the MKQA dataset.",
        "answers": "42.4",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 67.3 & 78.7 & 90.0 & 63.6 & 25.4 & 68.1 & 81.2 & 50.2 & 73.8 & 71.8 & 73.6 & 70.1 & 56.4 & 69 & 73.3 & 87.5 & 55.1 & 42.8 & 80.1 \\\\\n  mDPR & 79.0 & 84.1 & 81.9 & 76.8 & 86.4 & 89.8 & 78.8 & 91.5 & 77.6 & 57.3 & 82.5 & 73.7 & 79.7 & 61.6 & 76.2 & 67.8 & 94.4 & 89.8 & 71.5 \\\\\n  mContriever & 84.9 & 92.5 & 92.1 & 79.7 & 84.1 & 65.4 & 95.3 & 82.4 & 64.6 & 80.2 & 87.8 & 87.5 & 85.0 & 91 & 96.1 & 93.6 & 90.3 & 84.1 & 77.0 \\\\\n  $\\mathrm{mE5}$ large & 94.1 & 97.3 & 98.2 & 87.6 & 89.1 & 92.9 & 98.1 & 90.6 & 93.9 & 87.9 & 97.1 & 93.4 & 95.5 & 96.7 & 99.2 & 98.9 & 93.3 & 90.7 & 93.1 \\\\\n  E5 ${ }_{\\text {mistral-7b }}$ & 92.7 & 96.0 & 96.0 & 90.2 & 87.5 & 88.0 & 96.7 & 92.8 & 89.9 & 88.4 & 95.1 & 89.4 & 95.0 & 95.5 & 95.1 & 96.5 & 90.1 & 88.7 & 97.9 \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 95.5 & 97.6 & 98.7 & 90.7 & 91.1 & 94.0 & 97.9 & 93.8 & 94.4 & 90.5 & 97.5 & 95.5 & 95.9 & 97.2 & 99.4 & 99.1 & 96.9 & 90.9 & 98.7 \\\\\n  Sparse & 85.6 & 92.0 & 96.7 & 81.5 & 72.1 & 87.0 & 91.5 & 73.3 & 87.1 & 84.8 & 92.4 & 91.7 & 76.9 & 85.1 & 98.1 & 95.2 & 72.9 & 69.1 & 92.9 \\\\\n  Multi-vec & 96.3 & 97.8 & 98.9 & 91.7 & 92.4 & 94.9 & 98.2 & 96.1 & 95.1 & 92.5 & 98.0 & 95.9 & 96.6 & 97.3 & 99.4 & 99.2 & 97.3 & 92.4 & 99.2 \\\\\n  Dense+Sparse & 96.2 & 98.0 & 98.9 & 92.4 & 92.5 & 95.6 & 98.3 & 94.6 & 95.6 & 92.6 & 97.5 & 95.6 & 96.6 & 97.4 & 99.1 & 99.0 & 96.8 & 91.0 & 100.0 \\\\\n  All & 96.4 & 98.0 & 98.9 & 92.1 & 92.9 & 95.6 & 98.4 & 95.6 & 95.2 & 92.5 & 98.0 & 96.0 & 96.7 & 97.2 & 99.4 & 99.2 & 97.6 & 92.3 & 99.2 \\\\\n \n\\end{tabular}\n\nTable 12: Recall@ 100 on the dev set of the MIRACL dataset for multilingual retrieval in all 18 languages.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  & \\multicolumn{6}{|c|}{Baselines (Prior Work)} & \\multicolumn{5}{|c|}{M3-Embedding (Our Work)} \\\\\n  & BM25 & mDPR & mContriever & $\\mathrm{mE} 5_{\\text {large }}$ & E 5 mistral-7b & OpenAI-3 & Dense & Sparse & Multi-vec & Dense+Sparse & All \\\\\n  ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0 \\\\\n  da & 36.2 & 55.7 & 63.3 & 71.7 & 72.3 & 67.6 & 71.2 & 45.1 & 71.7 & 71.3 & 72.0 \\\\\n  de & 23.3 & 53.2 & 60.2 & 71.2 & 70.8 & 67.6 & 69.8 & 33.2 & 69.6 & 70.2 & 70.4 \\\\\n  es & 29.8 & 55.4 & 62.3 & 70.8 & 71.6 & 68.0 & 69.8 & 40.3 & 70.3 & 70.2 & 70.7 \\\\\n  fi & 33.2 & 42.8 & 58.7 & 67.7 & 63.6 & 65.5 & 67.8 & 41.2 & 68.3 & 68.4 & 68.9 \\\\\n  fr & 30.3 & 56.5 & 62.6 & 69.5 & 72.7 & 68.2 & 69.6 & 43.2 & 70.1 & 70.1 & 70.8 \\\\\n  he & 16.1 & 34.0 & 50.5 & 61.4 & 32.4 & 46.3 & 63.4 & 24.5 & 64.4 & 63.5 & 64.6 \\\\\n  hu & 26.1 & 46.1 & 57.1 & 68.0 & 68.3 & 64.0 & 67.1 & 34.5 & 67.3 & 67.7 & 67.9 \\\\\n  it & 31.5 & 53.8 & 62.0 & 71.2 & 71.3 & 67.6 & 69.7 & 41.5 & 69.9 & 69.9 & 70.3 \\\\\n  ja & 14.5 & 46.3 & 50.7 & 63.1 & 57.6 & 64.2 & 67.0 & 23.3 & 67.8 & 67.1 & 67.9 \\\\\n  km & 20.7 & 20.6 & 18.7 & 18.3 & 23.3 & 25.7 & 58.5 & 24.4 & 59.2 & 58.9 & 59.5 \\\\\n  ko & 18.3 & 36.8 & 44.9 & 58.9 & 49.4 & 53.9 & 61.9 & 24.3 & 63.2 & 62.1 & 63.3 \\\\\n  ms & 42.3 & 53.8 & 63.7 & 70.2 & 71.1 & 66.1 & 71.6 & 52.5 & 72.1 & 71.8 & 72.3 \\\\\n  nl & 42.5 & 56.9 & 63.9 & 73.0 & 74.5 & 68.8 & 71.3 & 52.9 & 71.8 & 71.7 & 72.3 \\\\\n  no & 38.5 & 55.2 & 63.0 & 71.1 & 70.8 & 67.0 & 70.7 & 47.0 & 71.4 & 71.1 & 71.6 \\\\\n  pl & 28.7 & 50.4 & 60.9 & 70.5 & 71.5 & 66.1 & 69.4 & 36.4 & 70.0 & 69.9 & 70.4 \\\\\n  pt & 31.8 & 52.5 & 61.0 & 66.8 & 71.6 & 67.7 & 69.3 & 40.2 & 70.0 & 69.8 & 70.6 \\\\\n  ru & 21.8 & 49.8 & 57.9 & 70.6 & 68.7 & 65.1 & 69.4 & 29.2 & 70.0 & 69.4 & 70.0 \\\\\n  sv & 41.1 & 54.9 & 62.7 & 72.0 & 73.3 & 67.8 & 70.5 & 49.8 & 71.3 & 71.5 & 71.5 \\\\\n  th & 28.4 & 40.9 & 54.4 & 69.7 & 57.1 & 55.2 & 69.6 & 34.7 & 70.5 & 69.8 & 70.8 \\\\\n  tr & 33.5 & 45.5 & 59.9 & 67.3 & 65.5 & 64.9 & 68.2 & 40.9 & 69.0 & 69.1 & 69.6 \\\\\n  vi & 33.6 & 51.3 & 59.9 & 68.7 & 62.3 & 63.5 & 69.6 & 42.2 & 70.5 & 70.2 & 70.9 \\\\\n  zh_cn & 19.4 & 50.1 & 55.9 & 44.3 & 61.2 & 62.7 & 66.4 & 26.9 & 66.7 & 66.6 & 67.3 \\\\\n  zh_hk & 23.9 & 50.2 & 55.5 & 46.4 & 55.9 & 61.4 & 65.8 & 31.2 & 66.4 & 65.9 & 66.7 \\\\\n  zh_tw & 22.5 & 50.6 & 55.2 & 45.9 & 56.5 & 61.6 & 64.8 & 29.8 & 65.3 & 64.9 & 65.6 \\\\\n  Avg & 28.1 & 47.9 & 56.3 & 63.5 & 62.4 & 62.1 & 67.8 & 36.3 & 68.4 & 68.1 & 68.8 \\\\\n \n\\end{tabular}\n\nTable 13: Recall@20 on MKQA dataset for cross-lingual retrieval in all 25 languages.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "ar & 13.4 & 33.8 & 43.8 & 59.7 & 47.6 & 55.1 & 61.9 & 19.5 & 62.6 & 61.9 & 63.0",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07a446fe-5e0b-44ea-8034-a0664417d108",
        "questions": "What is the nDCG@10 score for the 'bn' language using the M3-w.skd model with the Multi-vec approach?",
        "answers": "81.0",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Multi-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07afe1c5-1f37-40f4-9132-a7635a344fd7",
        "questions": "Compare the nDCG@10 scores of the Sparse and Dense models for 'te' when self-knowledge distillation is not used in the M3 approach. Which model performs better and by how much?",
        "answers": "Dense performs better by 22.2",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Dense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07b52a8b-0af0-4e69-853a-91353ee2151b",
        "questions": "What is the average score difference between the RetroMAE + Unsup + Fine-tune approach and the Fine-tune only approach across all languages in the Dense model?",
        "answers": "8.7",
        "context": "\\begin{tabular}{l|c|ccccccccccccccccccc}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  M3-w.skd \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\nSparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\nMulti-vec & 70.5 & 79.6 & 81.0 & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  M3-w.o.skd \\\\\nDense & 68.7 & 78.0 & 79.1 & 56.4 & 55.4 & 60.3 & 78.3 & 58.2 & 59.0 & 55.1 & 72.4 & 68.8 & 69.5 & 77.8 & 85.8 & 82.5 & 63.0 & 56.0 & 80.6 \\\\\nSparse & 36.7 & 48.2 & 51.9 & 24.3 & 20.3 & 26.0 & 48.6 & 16.8 & 30.1 & 32.0 & 33.0 & 43.1 & 27.2 & 45.2 & 63.6 & 52.2 & 22.6 & 16.5 & 59.2 \\\\\nMulti-vec & 69.3 & 78.7 & 80.2 & 57.6 & 56.7 & 60.5 & 79.0 & 58.4 & 59.3 & 57.5 & 74.0 & 70.3 & 70.2 & 78.6 & 86.9 & 82.1 & 61.9 & 56.7 & 78.2 \\\\\n \n\\end{tabular}\n\nTable 14: Ablation study of self-knowledge distillation on the MIRACL dev set (nDCG@10).\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Fine-tune} \\\\\n  Dense & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Fine-tune} \\\\\n  Dense & 66.1 & 75.9 & 77.9 & 54.5 & 54.0 & 58.3 & 76.6 & 55.1 & 57.0 & 53.9 & 70.1 & 66.9 & 66.9 & 74.8 & 86.1 & 79.5 & 61.9 & 52.7 & 67.5 \\\\\n  \\multicolumn{20}{|l|}{RetroMAE + Unsup + Fine-tune} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8 \\\\\n \n\\end{tabular}\n\nTable 15: Ablation study of multi-stage training on the MIRACL dev set (nDCG@10).",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Fine-tune & 60.5 & 71.0 & 72.5 & 47.6 & 46.7 & 51.8 & 72.3 & 50.9 & 48.9 & 48.9 & 65.7 & 60.5 & 60.9 & 71.9 & 81.3 & 74.7 & 54.4 & 48.7 & 60.6\nRetroMAE + Unsup + Fine-tune & 69.2 & 78.4 & 80.0 & 56.9 & 56.1 & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & 70.1 & 78.7 & 86.2 & 82.6 & 62.7 & 56.7 & 81.8",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07b62caf-fafd-460a-8544-0df00080c826",
        "questions": "What is the average nDCG@10 performance of the Multi-vec method in the MIRACL dev set?",
        "answers": "70.5",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07b9ae99-656b-4d12-bfc7-9d4117affe0c",
        "questions": "Which method achieved the highest nDCG@10 score for the Hindi language in the MIRACL dev set?",
        "answers": "Dense",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Dense & 69.2 & 78.4 & 80.0 & 56.9 & square & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & square & 78.7 & 86.2 & 82.6 & square 2.7 & 56. & 81.8",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07bbd20a-55e6-47c1-8b59-49a41a8a7384",
        "questions": "Considering the models Dense and All, which one has a higher nDCG@10 score for French language, and what are their respective scores?",
        "answers": "All with 61.2, Dense with 58.3",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  odel & Avg & ar & bn & en & es & fa & fi & fr & hi & id & ja & ko & ru & sw & te & th & zh & de & yo \\\\\n  \\multicolumn{20}{|l|}{Baselines (Prior Work)} \\\\\n  BM25 & 31.9 & 39.5 & 48.2 & 26.7 & 7.7 & 28 & 5.8 & 11.5 & 35.0 & 29 & & 37.1 & 25.6 & 35.1 & 38.3 & 49 & & 2.0 & 6.1 \\\\\n  mDPR & 41.8 & 49.9 & 44.3 & 39.4 & 47.8 & 48.0 & 47.2 & 43.5 & 38.3 & 27.2 & 43.9 & 41.9 & 40.7 & 29.9 & 35.6 & & & 49.0 & 39.6 \\\\\n  mContriever & 43.1 & 52.5 & 50.1 & 36.4 & 41.8 & 21.5 & 60.2 & 31.4 & 28.6 & 39.2 & 42.4 & 48.3 & 39.1 & 56.0 & 52.8 & 51.7 & 41.0 & 40.8 & 41.5 \\\\\n  $\\mathrm{mE5}_{\\text {large }}$ & 66.6 & 76.0 & 75.9 & 52.9 & 52.9 & 59.0 & 77.8 & 54.5 & 62.0 & 52.9 & 70.6 & 66.5 & 67.4 & 74.9 & 84.6 & 80.2 & 56. & 56. & 78.3 \\\\\n  E5mistral-7b & 63.4 & 73.3 & 70.3 & 57.3 & 52.2 & 52.1 & 74.7 & 55.2 & 52.1 & 52.7 & 66.8 & 61.8 & 67.7 & 68.4 & 73.9 & 74.0 & 54.0 & 54.1 & 79.7 \\\\\n  OpenAI-3 & 54.9 & - & - & - & & & & & & & & & & & & & - $\\square$ & & \\\\\n  \\multicolumn{20}{|l|}{M3-Embedding (Our Work)} \\\\\n  Dense & 69.2 & 78.4 & 80.0 & 56.9 & $\\square$ & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & $\\square$ & 78.7 & 86.2 & 82.6 & $\\square 2.7$ & 56. & 81.8 \\\\\n  Sparse & 53.9 & 67.1 & 68.9 & 43.8 & 38.6 & 45.1 & 65.4 & 35.3 & 48.2 & 48.9 & 56.1 & 61.5 & 44.5 & 57.9 & 79.1 & 70.9 & 36.1 & 32.5 & 70.0 \\\\\n  Multi-vec & 70.5 & 79.6 & & 59.3 & 57.8 & 62.0 & 80.1 & 59.4 & 61.5 & 58.3 & 74.5 & 71.2 & 71.2 & 79.1 & 87.9 & 83.0 & 63.7 & 58.0 & 82.4 \\\\\n  Dense+Sparse & 70.4 & 79.6 & 80.7 & 58.8 & 58.1 & 62.3 & 79.7 & 58.0 & 62.9 & 58.3 & 73.9 & 71.2 & 69.8 & 78.5 & 87.2 & 83.1 & 63.5 & 57.7 & 83.3 \\\\\n  All & 71.5 & 80.2 & 81.5 & 59.6 & 59.7 & 63.4 & 80.4 & 61.2 & 63.3 & 59.0 & 75.2 & 72.1 & 71.7 & 79.6 & 88.1 & 83.7 & 64.9 & 59.8 & 83.5 \\\\\n \n\\end{tabular}\n\nTable 1: Multi-lingual retrieval performance on the MIRACL dev set (measured by nDCG@10).\n\nFor users who are severely limited in computation or data resource, we present an even simpler method called MCLS (Multi-CLS), which simply inserts multiple CLS tokens to the long document during inference, and takes the average of all CLS embeddings as the ultimate embedding of the document. Despite simplicity, it is surprisingly effective in practice. (See Appendix B. 2 for more details.)\n\n4 Experiment\n\nIn this section, we investigate M3-Embedding's performance in terms of multi-lingual retrieval, crosslingual retrieval, and long-doc retrieval. We also explore the impact of its technical factors.\n\n4.1 Multi-Lingual Retrieval\n\nWe evaluate the multi-lingual retrieval performance with MIRACL (Zhang et al., 2023c), which consists of ad-hoc retrieval tasks in 18 languages. Each task is made up of query and passage presented in the same language. Following the official benchmark, we evaluate our method using Pyserini (Lin et al., 2021), and use nDCG@ 10 as the primary evaluation metric (Recall@100 is also measured and reported in Appendix C.1). Specifically, for the dense method (denoted as Dense), we first use it to generate the embeddings of the corpus and then build the dense index for searching top-1000 candidates with Faiss. For the sparse method (denoted as Sparse), we first use it to generate the weights of the corpus and then build the sparse index for searching top-1000 candidates with Lucene. For the multi-vector method (denoted as Multi-vec), considering its heavy cost, we use it as reranker to re-rank the top-200 candidates from dense method. For the hybrid retrieval of dense method and sparse method (denoted as\n\n$w_{3}=0$ in equation(1) to re-rank the union set of top-1000 candidates from Dense and top-1000 candidate from Sparse. For the hybrid retrieval of all three methods (denoted as $\\underline{A l l}$ ), we set $w_{1}=1$, $w_{2}=0.3$ and $w_{3}=1$ in equation(1) to re-rank the top-200 candidates from Dense.\n\nWe incorporate the following baselines in our experiment: the lexical retrieval method: BM25 (Robertson and Zaragoza, 2009); the dense retrieval methods: $\\mathrm{mDPR}^{3}$ (Zhang et al., 2023b), mContriever ${ }^{4}$ (Izacard et al., 2022), $\\mathrm{mE}_{\\text {large }}$ (Wang et al., 2022) and $E 5_{\\text {mistral-7b }}$ (Wang et al., 2023). To make the BM25 and M3 more comparable, in the experiment, we use the same tokenizer as M3 (i.e., the tokenizer of XLM-Roberta) for BM25. Using the same vocabulary from XLM-Roberta can also ensure that both approaches have the same retrieval latency. The results of BM25 with different tokenizers are shown in Appendix C.2. We also make a comparison with Text-Embedding-3-Large(abbreviated as OpenAI3 ), which was recently released by OpenAI ${ }^{5}$.\n\nWe can make the following observations according to the experiment result in Table 1. Firstly, M3Embedding already achieves a superior retrieval performance with only its dense retrieval functionality (Dense). It not only outperforms other baseline methods in the average performance, but also maintains a consistent empirical advantage in most of individual languages. Even compared with $\\mathrm{E} 5_{\\text {mistral-7b }}$, which leverages a much larger Mistral-7B model as the text encoder and specifically trained with English data, our method is able to produce a similar result in English and notably higher results in the other languages. Besides, the sparse retrieval functionality (Sparse) is also ef-\n\n\\footnotetext{\n3. https://huggingface.co/castorini/mdpr-tied-pft-msmarco\n4. https://huggingface.co/facebook/mcontriever-msmarco\n5. https://platform.openai.com/docs/guides/embeddings\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Dense & 69.2 & 78.4 & 80.0 & 56.9 & square & 60.9 & 78.6 & 58.3 & 59.5 & 56.1 & 72.8 & 69.9 & square & 78.7 & 86.2 & 82.6 & square 2.7 & 56. & 81.8",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07c0c00f-05ff-461b-91f0-0aad0dbab83e",
        "questions": "What is the equation that computes the relevance score in the Dense retrieval method?",
        "answers": "s_{\text {dense }} \\leftarrow\\left\\langle e_{p}, e_{q}\right\rangle",
        "context": "Figure 2: Multi-stage training process of M3-Embedding with self-knowledge distillation.\nretrieval. The formulation is presented as follows.\n- Dense retrieval. The input query $q$ is transformed into the hidden states $\\mathbf{H}_{\\mathbf{q}}$ based on a text encoder. We use the normalized hidden state of the special token \"[CLS]\" for the representation of the query: $e_{q}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{q}}[0]\\right)$. Similarly, we can get the embedding of passage $p$ as $e_{p}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{p}}[0]\\right)$. Thus, the relevance score between query and passage is measured by the inner product between the two embeddings $e_{q}$ and $e_{p}: s_{\\text {dense }} \\leftarrow\\left\\langle e_{p}, e_{q}\\right\\rangle$.\n- Lexical Retrieval. The output embeddings are also used to estimate the importance of each term to facilitate lexical retrieval. For each term $t$ within the query (a term is corresponding to a token in our work), the term weight is computed as $\\left.w_{q_{t}} \\leftarrow \\operatorname{Relu}\\left(\\mathbf{W}_{\\text {lex }}^{T} \\mathbf{H}_{\\mathbf{q}}[i]\\right)\\right)$, where $\\mathbf{W}_{\\text {lex }} \\in \\mathcal{R}^{d \\times 1}$ is the matrix mapping the hidden state to a float number. If a term $t$ appears multiple times in the query, we only retain its max weight. We use the same way to compute the weight of each term in the passage. Based on the estimation term weights, the relevance score between query and passage is computed by the joint importance of the co-existed terms (denoted as $q \\cap p$ ) within the query and passage: $s_{\\text {lex }} \\leftarrow \\sum_{t \\in q \\cap p}\\left(w_{q_{t}} * w_{p_{t}}\\right)$.\n- Multi-Vector Retrieval. As an extension of dense retrieval, the multi-vector method utilizes the entire output embeddings for the representation of query and passage: $E_{q}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{q}}\\right)$, $E_{p}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{p}}\\right)$, where $\\mathbf{W}_{m u l} \\in \\mathbb{R}^{d \\times d}$ is the learnable projection matrix. Following ColBert (Khattab and Zaharia, 2020), we use lateinteraction to compute the fine-grained relevance score: $s_{\\text {mul }} \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} \\max _{j=1}^{M} E_{q}[i] \\cdot E_{p}^{T}[j] ; N$ and $M$ are the lengths of query and passage.\nThanks to the multi-functionality of the embedding model, the retrieval process can be conducted\nin a hybrid process. First of all, the candidate results can be individually retrieved by each of the methods (the multi-vector method can be exempted from this step due to its heavy cost). Then, the final retrieval result is re-ranked based on the integrated relevance score:\n$$s_{\\text {rank }} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\nwhere the values of $w_{1}, w_{2}$ and $w_{3}$ depend on the downstream scenario.\n\n3.3 Self-Knowledge Distillation\n\nThe embedding model is trained to discriminate the positive samples from the negative ones. For each of the retrieval methods, it is expected to assign a higher score for the query's positive samples compared with the negative ones. Therefore, the training process is conducted to minimize the InfoNCE loss, whose general form is presented by the following loss function:\n$$\\mathcal{L}_{s(\\cdot)}=-\\log \\frac{\\exp \\left(s\\left(q, p^{*}\\right) / \\tau\\right)}{\\sum_{p \\in\\left\\{p^{*}, P^{\\prime}\\right\\}} \\exp (s(q, p) / \\tau)}$$\n\nHere, $p^{*}$ and $P^{\\prime}$ stand for the positive and negative samples to the query $q ; s(\\cdot)$ is any of the functions within $\\left\\{s_{\\text {dense }}(\\cdot), s_{\\text {lex }}(\\cdot), s_{\\text {mul }}(\\cdot)\\right\\}$.\n\nThe training objectives of different retrieval methods can be mutually conflicting with each their. Therefore, the native multi-objective training can be unfavorable to the embedding's quality. To facilitate the optimization of multiple retrieval functions, we propose to unify the training process on top of self-knowledge distillation. Particularly, based on the principle of ensemble learning (B\u00fchlmann, 2012), the predictions from different retrieval methods can be integrated as a more accurate relevance score given their heterogeneous",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Thus, the relevance score between query and passage is measured by the inner product between the two embeddings $e_{q}$ and $e_{p}: s_{\text {dense }} \\leftarrow\\left\\langle e_{p}, e_{q}\\right\\rangle$.",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07c57156-2d68-433b-92c2-8839db514bc0",
        "questions": "How is the relevance score computed in the Lexical retrieval method using term weights?",
        "answers": "s_{\text {lex }} \\leftarrow \\sum_{t \\in q \\cap p}\\left(w_{q_{t}} * w_{p_{t}}\right)",
        "context": "Figure 2: Multi-stage training process of M3-Embedding with self-knowledge distillation.\nretrieval. The formulation is presented as follows.\n- Dense retrieval. The input query $q$ is transformed into the hidden states $\\mathbf{H}_{\\mathbf{q}}$ based on a text encoder. We use the normalized hidden state of the special token \"[CLS]\" for the representation of the query: $e_{q}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{q}}[0]\\right)$. Similarly, we can get the embedding of passage $p$ as $e_{p}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{p}}[0]\\right)$. Thus, the relevance score between query and passage is measured by the inner product between the two embeddings $e_{q}$ and $e_{p}: s_{\\text {dense }} \\leftarrow\\left\\langle e_{p}, e_{q}\\right\\rangle$.\n- Lexical Retrieval. The output embeddings are also used to estimate the importance of each term to facilitate lexical retrieval. For each term $t$ within the query (a term is corresponding to a token in our work), the term weight is computed as $\\left.w_{q_{t}} \\leftarrow \\operatorname{Relu}\\left(\\mathbf{W}_{\\text {lex }}^{T} \\mathbf{H}_{\\mathbf{q}}[i]\\right)\\right)$, where $\\mathbf{W}_{\\text {lex }} \\in \\mathcal{R}^{d \\times 1}$ is the matrix mapping the hidden state to a float number. If a term $t$ appears multiple times in the query, we only retain its max weight. We use the same way to compute the weight of each term in the passage. Based on the estimation term weights, the relevance score between query and passage is computed by the joint importance of the co-existed terms (denoted as $q \\cap p$ ) within the query and passage: $s_{\\text {lex }} \\leftarrow \\sum_{t \\in q \\cap p}\\left(w_{q_{t}} * w_{p_{t}}\\right)$.\n- Multi-Vector Retrieval. As an extension of dense retrieval, the multi-vector method utilizes the entire output embeddings for the representation of query and passage: $E_{q}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{q}}\\right)$, $E_{p}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{p}}\\right)$, where $\\mathbf{W}_{m u l} \\in \\mathbb{R}^{d \\times d}$ is the learnable projection matrix. Following ColBert (Khattab and Zaharia, 2020), we use lateinteraction to compute the fine-grained relevance score: $s_{\\text {mul }} \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} \\max _{j=1}^{M} E_{q}[i] \\cdot E_{p}^{T}[j] ; N$ and $M$ are the lengths of query and passage.\nThanks to the multi-functionality of the embedding model, the retrieval process can be conducted\nin a hybrid process. First of all, the candidate results can be individually retrieved by each of the methods (the multi-vector method can be exempted from this step due to its heavy cost). Then, the final retrieval result is re-ranked based on the integrated relevance score:\n$$s_{\\text {rank }} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\nwhere the values of $w_{1}, w_{2}$ and $w_{3}$ depend on the downstream scenario.\n\n3.3 Self-Knowledge Distillation\n\nThe embedding model is trained to discriminate the positive samples from the negative ones. For each of the retrieval methods, it is expected to assign a higher score for the query's positive samples compared with the negative ones. Therefore, the training process is conducted to minimize the InfoNCE loss, whose general form is presented by the following loss function:\n$$\\mathcal{L}_{s(\\cdot)}=-\\log \\frac{\\exp \\left(s\\left(q, p^{*}\\right) / \\tau\\right)}{\\sum_{p \\in\\left\\{p^{*}, P^{\\prime}\\right\\}} \\exp (s(q, p) / \\tau)}$$\n\nHere, $p^{*}$ and $P^{\\prime}$ stand for the positive and negative samples to the query $q ; s(\\cdot)$ is any of the functions within $\\left\\{s_{\\text {dense }}(\\cdot), s_{\\text {lex }}(\\cdot), s_{\\text {mul }}(\\cdot)\\right\\}$.\n\nThe training objectives of different retrieval methods can be mutually conflicting with each their. Therefore, the native multi-objective training can be unfavorable to the embedding's quality. To facilitate the optimization of multiple retrieval functions, we propose to unify the training process on top of self-knowledge distillation. Particularly, based on the principle of ensemble learning (B\u00fchlmann, 2012), the predictions from different retrieval methods can be integrated as a more accurate relevance score given their heterogeneous",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Based on the estimation term weights, the relevance score between query and passage is computed by the joint importance of the co-existed terms (denoted as $q \\cap p$ ) within the query and passage: $s_{\text {lex }} \\leftarrow \\sum_{t \\in q \\cap p}\\left(w_{q_{t}} * w_{p_{t}}\\right)$.",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07d5e121-623a-45ae-ba31-a27cced4dd7c",
        "questions": "What is the formula for computing the fine-grained relevance score in the Multi-Vector Retrieval method?",
        "answers": "s_{\text {mul }} \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} \\max _{j=1}^{M} E_{q}[i] \\cdot E_{p}^{T}[j]",
        "context": "Figure 2: Multi-stage training process of M3-Embedding with self-knowledge distillation.\nretrieval. The formulation is presented as follows.\n- Dense retrieval. The input query $q$ is transformed into the hidden states $\\mathbf{H}_{\\mathbf{q}}$ based on a text encoder. We use the normalized hidden state of the special token \"[CLS]\" for the representation of the query: $e_{q}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{q}}[0]\\right)$. Similarly, we can get the embedding of passage $p$ as $e_{p}=\\operatorname{norm}\\left(\\mathbf{H}_{\\mathbf{p}}[0]\\right)$. Thus, the relevance score between query and passage is measured by the inner product between the two embeddings $e_{q}$ and $e_{p}: s_{\\text {dense }} \\leftarrow\\left\\langle e_{p}, e_{q}\\right\\rangle$.\n- Lexical Retrieval. The output embeddings are also used to estimate the importance of each term to facilitate lexical retrieval. For each term $t$ within the query (a term is corresponding to a token in our work), the term weight is computed as $\\left.w_{q_{t}} \\leftarrow \\operatorname{Relu}\\left(\\mathbf{W}_{\\text {lex }}^{T} \\mathbf{H}_{\\mathbf{q}}[i]\\right)\\right)$, where $\\mathbf{W}_{\\text {lex }} \\in \\mathcal{R}^{d \\times 1}$ is the matrix mapping the hidden state to a float number. If a term $t$ appears multiple times in the query, we only retain its max weight. We use the same way to compute the weight of each term in the passage. Based on the estimation term weights, the relevance score between query and passage is computed by the joint importance of the co-existed terms (denoted as $q \\cap p$ ) within the query and passage: $s_{\\text {lex }} \\leftarrow \\sum_{t \\in q \\cap p}\\left(w_{q_{t}} * w_{p_{t}}\\right)$.\n- Multi-Vector Retrieval. As an extension of dense retrieval, the multi-vector method utilizes the entire output embeddings for the representation of query and passage: $E_{q}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{q}}\\right)$, $E_{p}=\\operatorname{norm}\\left(\\mathbf{W}_{m u l}^{T} \\mathbf{H}_{\\mathbf{p}}\\right)$, where $\\mathbf{W}_{m u l} \\in \\mathbb{R}^{d \\times d}$ is the learnable projection matrix. Following ColBert (Khattab and Zaharia, 2020), we use lateinteraction to compute the fine-grained relevance score: $s_{\\text {mul }} \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} \\max _{j=1}^{M} E_{q}[i] \\cdot E_{p}^{T}[j] ; N$ and $M$ are the lengths of query and passage.\nThanks to the multi-functionality of the embedding model, the retrieval process can be conducted\nin a hybrid process. First of all, the candidate results can be individually retrieved by each of the methods (the multi-vector method can be exempted from this step due to its heavy cost). Then, the final retrieval result is re-ranked based on the integrated relevance score:\n$$s_{\\text {rank }} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\nwhere the values of $w_{1}, w_{2}$ and $w_{3}$ depend on the downstream scenario.\n\n3.3 Self-Knowledge Distillation\n\nThe embedding model is trained to discriminate the positive samples from the negative ones. For each of the retrieval methods, it is expected to assign a higher score for the query's positive samples compared with the negative ones. Therefore, the training process is conducted to minimize the InfoNCE loss, whose general form is presented by the following loss function:\n$$\\mathcal{L}_{s(\\cdot)}=-\\log \\frac{\\exp \\left(s\\left(q, p^{*}\\right) / \\tau\\right)}{\\sum_{p \\in\\left\\{p^{*}, P^{\\prime}\\right\\}} \\exp (s(q, p) / \\tau)}$$\n\nHere, $p^{*}$ and $P^{\\prime}$ stand for the positive and negative samples to the query $q ; s(\\cdot)$ is any of the functions within $\\left\\{s_{\\text {dense }}(\\cdot), s_{\\text {lex }}(\\cdot), s_{\\text {mul }}(\\cdot)\\right\\}$.\n\nThe training objectives of different retrieval methods can be mutually conflicting with each their. Therefore, the native multi-objective training can be unfavorable to the embedding's quality. To facilitate the optimization of multiple retrieval functions, we propose to unify the training process on top of self-knowledge distillation. Particularly, based on the principle of ensemble learning (B\u00fchlmann, 2012), the predictions from different retrieval methods can be integrated as a more accurate relevance score given their heterogeneous",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Following ColBert (Khattab and Zaharia, 2020), we use lateinteraction to compute the fine-grained relevance score: $s_{\text {mul }} \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} \\max _{j=1}^{M} E_{q}[i] \\cdot E_{p}^{T}[j] ; N$ and $M$ are the lengths of query and passage.",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07d8ab49-76e2-44c4-b1a5-621012fdd312",
        "questions": "What is the equation for the weighted sum of prediction scores in the simplest form of integration?",
        "answers": "$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "In the simplest form, the integration can just be the weighted sum of different prediction scores: $$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07e124e3-2292-44c0-8cbc-b225b7c8dc61",
        "questions": "How is the final loss function for self-knowledge distillation derived?",
        "answers": "$\\mathcal{L}_{\\text {final }} \\leftarrow (\\mathcal{L} + \\mathcal{L}^{\\prime}) / 2$",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Finally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2402.03216v4",
        "ID": "07f819db-e8d7-425b-8a8b-c03efafd02b8",
        "questions": "What is the equation for the loss function with integrated and normalized modified loss terms?",
        "answers": "$\\mathcal{L}^{\\prime} \\leftarrow (\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}) / 3$",
        "context": "nature. In the simplest form, the integration can just be the weighted sum of different prediction scores:\n$$s_{i n t e r} \\leftarrow w_{1} \\cdot s_{\\text {dense }}+w_{2} \\cdot s_{l e x}+w_{3} \\cdot s_{m u l}$$\n\nThen we compute the weighted sum of $\\mathcal{L}_{\\text {dense }}$, $\\mathcal{L}_{\\text {lex }}, \\mathcal{L}_{\\text {mul }}$ and $\\mathcal{L}_{\\text {inter }}$ as the loss without selfknowledge distillation:\n$$\\mathcal{L} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}+\\mathcal{L}_{\\text {inter }}\\right) / 4$$\n\nIn previous studies, the training quality of embedding model can benefit from knowledge distillation, which takes advantage of fine-grained soft labels from another ranking model (Hofst\u00e4tter et al., 2021). In this place, we simply employ the integration score $s_{\\text {inter }}$ as the teacher, where the loss function of each retrieval method is modified as:\n$$\\mathcal{L}_{*}^{\\prime} \\leftarrow-p\\left(s_{\\text {inter }}\\right) * \\log p\\left(s_{*}\\right)$$\n\nHere, $p(\\cdot)$ is the softmax activation; $s_{*}$ is any of the members within $s_{\\text {dense }}, s_{l e x}$, and $s_{m u l}$. We further integrate and normalize the modified loss function:\n$$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$\n\nFinally, we derive the final loss function for selfknowledge distillation with the linear combination of $\\mathcal{L}$ and $\\mathcal{L}^{\\prime}: \\mathcal{L}_{\\text {final }} \\leftarrow\\left(\\mathcal{L}+\\mathcal{L}^{\\prime}\\right) / 2$.\n\nThe training process constitutes a multi-stage workflow (Figure 2). In the first place, the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning. The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities. The random initialization of $\\mathbf{W}_{\\text {lex }}$ led to poor $s_{\\text {lex }}$ accuracy and high $\\mathcal{L}_{\\text {lex }}$ at the beginning of the training. In order to reduce the impact of this, we set $w_{1}=1, w_{2}=0.3, w_{3}=1, \\lambda_{1}=1, \\lambda_{2}=0.1$ and $\\lambda_{3}=1$ during the training process. Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020). (See Appendix B. 1 for more details.)\n\n3.4 Efficient Batching\n\nThe embedding model needs to learn from diverse and massive multi-lingual data to fully capture the\n\n\nFigure 3: Efficient Batching. (Data is grouped and sampled by length. Gradient-checkpointing and crossGPU broadcasting are enabled to save memory.)\ngeneral semantic of different languages. It also needs to keep the batch size as large as possible (introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings. Given the limitations on GPU's memory and computation power, people usually truncate the input data into short sequences for high throughput of training and a large batch size. However, the common practice is not a feasible option for M3-Embedding because it needs to learn from both short and long-sequence data to effectively handle the input of different granularities. In our work, we improve the training efficiency by optimizing the batching strategy, which enables high training throughput and large batch sizes.\n\nParticularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group. Due to the similar sequence lengths, it significantly reduces sequence padding (Figure 3, marked in red) and facilitates a more effective utilization of GPUs. Besides, when sampling the training data for different GPUs, the random seed is always fixed, which ensures the load balance and minimizes the waiting time in each training step. Besides, when handling longsequence training data, the mini-batch is further divided into sub-batches, which takes less memory footprint. We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016) and gather all generated embeddings. This method can significantly increase the batch size. For example, when processing text with a length of 8192 , the batch size can be increased by more than 20 times. (see Appendx B. 3 for more details.) Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, which notably expands the scale of in-bath negative samples.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "We further integrate and normalize the modified loss function: $$\\mathcal{L}^{\\prime} \\leftarrow\\left(\\lambda_{1} \\cdot \\mathcal{L}_{\\text {dense }}^{\\prime}+\\lambda_{2} \\cdot \\mathcal{L}_{\\text {lex }}^{\\prime}+\\lambda_{3} \\cdot \\mathcal{L}_{\\text {mul }}^{\\prime}\\right) / 3$$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "07fd4672-7c9d-48a5-a764-6631867eda2e",
        "questions": "What is the computational cost of the spatial-channel decoupled downsampling proposed for YOLOs?",
        "answers": "The computational cost is reduced to $\\mathcal{O}\\left(2 H W C^{2}+\frac{9}{2} H W C\right)$.",
        "context": "Figure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone and neck is numbered in the order of model forward process. The numerical rank $r$ is normalized to $r / C_{o}$ for y-axis and its threshold is set to $\\lambda_{\\max } / 2$, by default, where $C_{o}$ denotes the number of output channels and $\\lambda_{\\max }$ is the largest singular value. It can be observed that deep stages and large models exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial self-attention module (PSA).\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the model architecture exhibits non-negligible computational redundancy and constrained capability, which impedes its potential for achieving high efficiency and performance. Here, we aim to holistically perform model designs for YOLOs from both efficiency and accuracy perspectives.\n\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling layers, stages with basic building blocks, and the head. The stem incurs few computational cost and we thus perform efficiency driven model design for other three parts.\n(1) Lightweight classification head. The classification and regression heads usually share the same architecture in YOLOs. However, they exhibit notable disparities in computational overhead. For example, the FLOPs and parameter count of the classification head $(5.95 \\mathrm{G} / 1.51 \\mathrm{M})$ are $2.5 \\times$ and $2.4 \\times$ those of the regression head ( $2.34 \\mathrm{G} / 0.64 \\mathrm{M}$ ) in YOLOv8-S, respectively. However, after analyzing the impact of classification error and the regression error (seeing Tab. 6), we find that the regression head undertakes more significance for the performance of YOLOs. Consequently, we can reduce the overhead of classification head without worrying about hurting the performance greatly. Therefore, we simply adopt a lightweight architecture for the classification head, which consists of two depthwise separable convolutions $[24,8]$ with the kernel size of $3 \\times 3$ followed by a $1 \\times 1$ convolution.\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular $3 \\times 3$ standard convolutions with stride of 2, achieving spatial downsampling (from $H \\times W$ to $\\frac{H}{2} \\times \\frac{W}{2}$ ) and channel transformation (from $C$ to $2 C$ ) simultaneously. This introduces non-negligible computational cost of $\\mathcal{O}\\left(\\frac{9}{2} H W C^{2}\\right)$ and parameter count of $\\mathcal{O}\\left(18 C^{2}\\right)$. Instead, we propose to decouple the spatial reduction and channel increase operations, enabling more efficient downsampling. Specifically, we firstly leverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise convolution to perform spatial downsampling. This reduces the computational cost to $\\mathcal{O}\\left(2 H W C^{2}+\\right.$ $\\left.\\frac{9}{2} H W C\\right)$ and the parameter count to $\\mathcal{O}\\left(2 C^{2}+18 C\\right)$. Meanwhile, it maximizes information retention during downsampling, leading to competitive performance with latency reduction.\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [27, 59], e.g., the bottleneck block in YOLOv8 [20]. To thoroughly examine such homogeneous design for YOLOs, we utilize the intrinsic rank $[31,15]$ to analyze the redundancy ${ }^{2}$ of each stage. Specifically, we calculate the numerical rank of the last convolution in the last basic block in each stage, which counts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of YOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme which aims to decrease the complexity of stages that are shown to be redundant using compact architecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap depthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\n\n\\footnotetext{\n${ }^{2} \\mathrm{~A}$ lower rank implies greater redundancy, while a higher rank signifies more condensed information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "This reduces the computational cost to $\\mathcal{O}\\left(2 H W C^{2}+\right.$ $\\left.\frac{9}{2} H W C\right)$ and the parameter count to $\\mathcal{O}\\left(2 C^{2}+18 C\right)$.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "07fe6a4e-2a6f-4e47-9b61-f92b9cf600ec",
        "questions": "In YOLOv8-S, how many times greater are the FLOPs and parameter count of the classification head compared to the regression head?",
        "answers": "The FLOPs and parameter count of the classification head are 2.5 times and 2.4 times those of the regression head, respectively.",
        "context": "Figure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone and neck is numbered in the order of model forward process. The numerical rank $r$ is normalized to $r / C_{o}$ for y-axis and its threshold is set to $\\lambda_{\\max } / 2$, by default, where $C_{o}$ denotes the number of output channels and $\\lambda_{\\max }$ is the largest singular value. It can be observed that deep stages and large models exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial self-attention module (PSA).\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the model architecture exhibits non-negligible computational redundancy and constrained capability, which impedes its potential for achieving high efficiency and performance. Here, we aim to holistically perform model designs for YOLOs from both efficiency and accuracy perspectives.\n\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling layers, stages with basic building blocks, and the head. The stem incurs few computational cost and we thus perform efficiency driven model design for other three parts.\n(1) Lightweight classification head. The classification and regression heads usually share the same architecture in YOLOs. However, they exhibit notable disparities in computational overhead. For example, the FLOPs and parameter count of the classification head $(5.95 \\mathrm{G} / 1.51 \\mathrm{M})$ are $2.5 \\times$ and $2.4 \\times$ those of the regression head ( $2.34 \\mathrm{G} / 0.64 \\mathrm{M}$ ) in YOLOv8-S, respectively. However, after analyzing the impact of classification error and the regression error (seeing Tab. 6), we find that the regression head undertakes more significance for the performance of YOLOs. Consequently, we can reduce the overhead of classification head without worrying about hurting the performance greatly. Therefore, we simply adopt a lightweight architecture for the classification head, which consists of two depthwise separable convolutions $[24,8]$ with the kernel size of $3 \\times 3$ followed by a $1 \\times 1$ convolution.\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular $3 \\times 3$ standard convolutions with stride of 2, achieving spatial downsampling (from $H \\times W$ to $\\frac{H}{2} \\times \\frac{W}{2}$ ) and channel transformation (from $C$ to $2 C$ ) simultaneously. This introduces non-negligible computational cost of $\\mathcal{O}\\left(\\frac{9}{2} H W C^{2}\\right)$ and parameter count of $\\mathcal{O}\\left(18 C^{2}\\right)$. Instead, we propose to decouple the spatial reduction and channel increase operations, enabling more efficient downsampling. Specifically, we firstly leverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise convolution to perform spatial downsampling. This reduces the computational cost to $\\mathcal{O}\\left(2 H W C^{2}+\\right.$ $\\left.\\frac{9}{2} H W C\\right)$ and the parameter count to $\\mathcal{O}\\left(2 C^{2}+18 C\\right)$. Meanwhile, it maximizes information retention during downsampling, leading to competitive performance with latency reduction.\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [27, 59], e.g., the bottleneck block in YOLOv8 [20]. To thoroughly examine such homogeneous design for YOLOs, we utilize the intrinsic rank $[31,15]$ to analyze the redundancy ${ }^{2}$ of each stage. Specifically, we calculate the numerical rank of the last convolution in the last basic block in each stage, which counts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of YOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme which aims to decrease the complexity of stages that are shown to be redundant using compact architecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap depthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\n\n\\footnotetext{\n${ }^{2} \\mathrm{~A}$ lower rank implies greater redundancy, while a higher rank signifies more condensed information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For example, the FLOPs and parameter count of the classification head $(5.95 \\mathrm{G} / 1.51 \\mathrm{M})$ are $2.5 \times$ and $2.4 \times$ those of the regression head ( $2.34 \\mathrm{G} / 0.64 \\mathrm{M}$ ) in YOLOv8-S, respectively.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08055d83-2d81-4097-b80f-142f87bfef1a",
        "questions": "Does the intrinsic rank analysis suggest that using the same block design for all stages in YOLOv8 is optimal for capacity-efficiency trade-off?",
        "answers": "No",
        "context": "Figure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone and neck is numbered in the order of model forward process. The numerical rank $r$ is normalized to $r / C_{o}$ for y-axis and its threshold is set to $\\lambda_{\\max } / 2$, by default, where $C_{o}$ denotes the number of output channels and $\\lambda_{\\max }$ is the largest singular value. It can be observed that deep stages and large models exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial self-attention module (PSA).\nthe comprehensive inspection for various components in YOLOs is still lacking. Consequently, the model architecture exhibits non-negligible computational redundancy and constrained capability, which impedes its potential for achieving high efficiency and performance. Here, we aim to holistically perform model designs for YOLOs from both efficiency and accuracy perspectives.\n\nEfficiency driven model design. The components in YOLO consist of the stem, downsampling layers, stages with basic building blocks, and the head. The stem incurs few computational cost and we thus perform efficiency driven model design for other three parts.\n(1) Lightweight classification head. The classification and regression heads usually share the same architecture in YOLOs. However, they exhibit notable disparities in computational overhead. For example, the FLOPs and parameter count of the classification head $(5.95 \\mathrm{G} / 1.51 \\mathrm{M})$ are $2.5 \\times$ and $2.4 \\times$ those of the regression head ( $2.34 \\mathrm{G} / 0.64 \\mathrm{M}$ ) in YOLOv8-S, respectively. However, after analyzing the impact of classification error and the regression error (seeing Tab. 6), we find that the regression head undertakes more significance for the performance of YOLOs. Consequently, we can reduce the overhead of classification head without worrying about hurting the performance greatly. Therefore, we simply adopt a lightweight architecture for the classification head, which consists of two depthwise separable convolutions $[24,8]$ with the kernel size of $3 \\times 3$ followed by a $1 \\times 1$ convolution.\n(2) Spatial-channel decoupled downsampling. YOLOs typically leverage regular $3 \\times 3$ standard convolutions with stride of 2, achieving spatial downsampling (from $H \\times W$ to $\\frac{H}{2} \\times \\frac{W}{2}$ ) and channel transformation (from $C$ to $2 C$ ) simultaneously. This introduces non-negligible computational cost of $\\mathcal{O}\\left(\\frac{9}{2} H W C^{2}\\right)$ and parameter count of $\\mathcal{O}\\left(18 C^{2}\\right)$. Instead, we propose to decouple the spatial reduction and channel increase operations, enabling more efficient downsampling. Specifically, we firstly leverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise convolution to perform spatial downsampling. This reduces the computational cost to $\\mathcal{O}\\left(2 H W C^{2}+\\right.$ $\\left.\\frac{9}{2} H W C\\right)$ and the parameter count to $\\mathcal{O}\\left(2 C^{2}+18 C\\right)$. Meanwhile, it maximizes information retention during downsampling, leading to competitive performance with latency reduction.\n(3) Rank-guided block design. YOLOs usually employ the same basic building block for all stages [27, 59], e.g., the bottleneck block in YOLOv8 [20]. To thoroughly examine such homogeneous design for YOLOs, we utilize the intrinsic rank $[31,15]$ to analyze the redundancy ${ }^{2}$ of each stage. Specifically, we calculate the numerical rank of the last convolution in the last basic block in each stage, which counts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of YOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme which aims to decrease the complexity of stages that are shown to be redundant using compact architecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap depthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel\n\n\\footnotetext{\n${ }^{2} \\mathrm{~A}$ lower rank implies greater redundancy, while a higher rank signifies more condensed information.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "080ba0b1-bb47-4997-94b3-e0f143173432",
        "questions": "What is the main advantage of YOLOs in real-time object detection?",
        "answers": "Adept balance between performance and efficiency",
        "context": "1 Introduction\n\n\nReal-time object detection has always been a focal point of research in the area of computer vision, which aims to accurately predict the categories and positions of objects in an image under low latency. It is widely adopted in various practical applications, including autonomous driving [3], robot navigation [11], and object tracking [66], etc. In recent years, researchers have concentrated on devising CNN-based object detectors to achieve real-time detection [18, 22, 43, 44, 45, 51, 12]. Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency $[2,19,27,19,20,59,54,64,7,65,16,27]$. The detection pipeline of YOLOs consists of two parts: the model forward process and the NMS post-processing. However, both of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\n\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby one ground-truth object corresponds to multiple positive samples. Despite yielding superior performance, this approach necessitates NMS to select the best positive prediction during inference. This slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS, thereby preventing YOLOs from achieving optimal end-to-end deployment [71]. One line to tackle this issue is to adopt the recently introduced end-to-end DETR architectures [4, 74, 67, 28, 34, 40, 61]. For example, RT-DETR [71] presents an efficient hybrid encoder and uncertainty-minimal query selection, propelling DETRs into the realm of real-time applications. Nevertheless, the inherent complexity in deploying DETRs impedes its ability to attain the optimal balance between accuracy and speed. Another line is to explore end-to-end detection for CNN -based detectors, which typically leverages one-to-one assignment strategies to suppress the redundant predictions [5, 49, 60, 73, 16]. However, they usually introduce additional inference overhead or achieve suboptimal performance.\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which exhibits an important impact on the accuracy and speed [45, 16, 65, 7]. To achieve more efficient and effective model architectures, researchers have explored different design strategies. Various primary computational units are presented for the backbone to enhance the feature extraction ability, including DarkNet [43, 44, 45], CSPNet [2], EfficientRep [27] and ELAN [56, 58], etc. For the neck, PAN [35], BiC [27], GD [54] and RepGFPN [65], etc., are explored to enhance the multi-scale feature fusion. Besides, model scaling strategies [56, 55] and re-parameterization [10, 27] techniques are also investigated. While these efforts have achieved notable advancements, a comprehensive inspection for various components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As a result, there still exists considerable computational redundancy within YOLOs, leading to inefficient parameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability also leads to inferior performance, leaving ample room for accuracy improvements.\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of YOLOs. We target both the post-processing and the model architecture throughout the detection pipeline. To this end, we first tackle the problem of redundant predictions in the post-processing by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label assignments and consistent matching metric. It allows the model to enjoy rich and harmonious supervision during training while eliminating the need for NMS during inference, leading to competitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy driven model design strategy for the model architecture by performing the comprehensive inspection for various components in YOLOs. For efficiency, we propose the lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested computational redundancy and achieve more efficient architecture. For accuracy, we explore the large-kernel convolution and present the effective partial self-attention module to enhance the model capability, harnessing the potential for performance improvements under low cost.\n\nBased on these approaches, we succeed in achieving a new family of real-time end-to-end detectors with different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard benchmarks for object detection, i.e., COCO [33], demonstrate that our YOLOv10 can significantly outperform previous state-of-the-art models in terms of computation-accuracy trade-offs across various model scales. As shown in Fig. 1, our YOLOv10-S / X are $1.8 \\times / 1.3 \\times$ faster than RT-DETRR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B achieves a $46 \\%$ reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly efficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and 0.5 AP , with $1.8 \\times$ and $2.3 \\times$ smaller number of parameters, respectively. YOLOv10-M achieves the",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency $[2,19,27,19,20,59,54,64,7,65,16,27]$.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "081116e5-a4c0-4de0-9b94-82ed19691d41",
        "questions": "How much faster is YOLOv10-S compared to RT-DETRR18 under similar performance conditions?",
        "answers": "1.8 times faster",
        "context": "1 Introduction\n\n\nReal-time object detection has always been a focal point of research in the area of computer vision, which aims to accurately predict the categories and positions of objects in an image under low latency. It is widely adopted in various practical applications, including autonomous driving [3], robot navigation [11], and object tracking [66], etc. In recent years, researchers have concentrated on devising CNN-based object detectors to achieve real-time detection [18, 22, 43, 44, 45, 51, 12]. Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency $[2,19,27,19,20,59,54,64,7,65,16,27]$. The detection pipeline of YOLOs consists of two parts: the model forward process and the NMS post-processing. However, both of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\n\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby one ground-truth object corresponds to multiple positive samples. Despite yielding superior performance, this approach necessitates NMS to select the best positive prediction during inference. This slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS, thereby preventing YOLOs from achieving optimal end-to-end deployment [71]. One line to tackle this issue is to adopt the recently introduced end-to-end DETR architectures [4, 74, 67, 28, 34, 40, 61]. For example, RT-DETR [71] presents an efficient hybrid encoder and uncertainty-minimal query selection, propelling DETRs into the realm of real-time applications. Nevertheless, the inherent complexity in deploying DETRs impedes its ability to attain the optimal balance between accuracy and speed. Another line is to explore end-to-end detection for CNN -based detectors, which typically leverages one-to-one assignment strategies to suppress the redundant predictions [5, 49, 60, 73, 16]. However, they usually introduce additional inference overhead or achieve suboptimal performance.\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which exhibits an important impact on the accuracy and speed [45, 16, 65, 7]. To achieve more efficient and effective model architectures, researchers have explored different design strategies. Various primary computational units are presented for the backbone to enhance the feature extraction ability, including DarkNet [43, 44, 45], CSPNet [2], EfficientRep [27] and ELAN [56, 58], etc. For the neck, PAN [35], BiC [27], GD [54] and RepGFPN [65], etc., are explored to enhance the multi-scale feature fusion. Besides, model scaling strategies [56, 55] and re-parameterization [10, 27] techniques are also investigated. While these efforts have achieved notable advancements, a comprehensive inspection for various components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As a result, there still exists considerable computational redundancy within YOLOs, leading to inefficient parameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability also leads to inferior performance, leaving ample room for accuracy improvements.\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of YOLOs. We target both the post-processing and the model architecture throughout the detection pipeline. To this end, we first tackle the problem of redundant predictions in the post-processing by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label assignments and consistent matching metric. It allows the model to enjoy rich and harmonious supervision during training while eliminating the need for NMS during inference, leading to competitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy driven model design strategy for the model architecture by performing the comprehensive inspection for various components in YOLOs. For efficiency, we propose the lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested computational redundancy and achieve more efficient architecture. For accuracy, we explore the large-kernel convolution and present the effective partial self-attention module to enhance the model capability, harnessing the potential for performance improvements under low cost.\n\nBased on these approaches, we succeed in achieving a new family of real-time end-to-end detectors with different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard benchmarks for object detection, i.e., COCO [33], demonstrate that our YOLOv10 can significantly outperform previous state-of-the-art models in terms of computation-accuracy trade-offs across various model scales. As shown in Fig. 1, our YOLOv10-S / X are $1.8 \\times / 1.3 \\times$ faster than RT-DETRR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B achieves a $46 \\%$ reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly efficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and 0.5 AP , with $1.8 \\times$ and $2.3 \\times$ smaller number of parameters, respectively. YOLOv10-M achieves the",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "As shown in Fig. 1, our YOLOv10-S / X are $1.8 \\times / 1.3 \\times$ faster than RT-DETRR18 / R101, respectively, under the similar performance.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "081797ca-99ac-4892-b26e-1c2f2fcfd8b7",
        "questions": "Does the proposed YOLOv10 model family include a version that achieves a 46% reduction in latency compared to YOLOv9-C while maintaining the same performance?",
        "answers": "Yes",
        "context": "1 Introduction\n\n\nReal-time object detection has always been a focal point of research in the area of computer vision, which aims to accurately predict the categories and positions of objects in an image under low latency. It is widely adopted in various practical applications, including autonomous driving [3], robot navigation [11], and object tracking [66], etc. In recent years, researchers have concentrated on devising CNN-based object detectors to achieve real-time detection [18, 22, 43, 44, 45, 51, 12]. Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency $[2,19,27,19,20,59,54,64,7,65,16,27]$. The detection pipeline of YOLOs consists of two parts: the model forward process and the NMS post-processing. However, both of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.\n\nSpecifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby one ground-truth object corresponds to multiple positive samples. Despite yielding superior performance, this approach necessitates NMS to select the best positive prediction during inference. This slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS, thereby preventing YOLOs from achieving optimal end-to-end deployment [71]. One line to tackle this issue is to adopt the recently introduced end-to-end DETR architectures [4, 74, 67, 28, 34, 40, 61]. For example, RT-DETR [71] presents an efficient hybrid encoder and uncertainty-minimal query selection, propelling DETRs into the realm of real-time applications. Nevertheless, the inherent complexity in deploying DETRs impedes its ability to attain the optimal balance between accuracy and speed. Another line is to explore end-to-end detection for CNN -based detectors, which typically leverages one-to-one assignment strategies to suppress the redundant predictions [5, 49, 60, 73, 16]. However, they usually introduce additional inference overhead or achieve suboptimal performance.\nFurthermore, the model architecture design remains a fundamental challenge for YOLOs, which exhibits an important impact on the accuracy and speed [45, 16, 65, 7]. To achieve more efficient and effective model architectures, researchers have explored different design strategies. Various primary computational units are presented for the backbone to enhance the feature extraction ability, including DarkNet [43, 44, 45], CSPNet [2], EfficientRep [27] and ELAN [56, 58], etc. For the neck, PAN [35], BiC [27], GD [54] and RepGFPN [65], etc., are explored to enhance the multi-scale feature fusion. Besides, model scaling strategies [56, 55] and re-parameterization [10, 27] techniques are also investigated. While these efforts have achieved notable advancements, a comprehensive inspection for various components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As a result, there still exists considerable computational redundancy within YOLOs, leading to inefficient parameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability also leads to inferior performance, leaving ample room for accuracy improvements.\nIn this work, we aim to address these issues and further advance the accuracy-speed boundaries of YOLOs. We target both the post-processing and the model architecture throughout the detection pipeline. To this end, we first tackle the problem of redundant predictions in the post-processing by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label assignments and consistent matching metric. It allows the model to enjoy rich and harmonious supervision during training while eliminating the need for NMS during inference, leading to competitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy driven model design strategy for the model architecture by performing the comprehensive inspection for various components in YOLOs. For efficiency, we propose the lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested computational redundancy and achieve more efficient architecture. For accuracy, we explore the large-kernel convolution and present the effective partial self-attention module to enhance the model capability, harnessing the potential for performance improvements under low cost.\n\nBased on these approaches, we succeed in achieving a new family of real-time end-to-end detectors with different model scales, i.e., YOLOv10-N / S / M / B / L / X. Extensive experiments on standard benchmarks for object detection, i.e., COCO [33], demonstrate that our YOLOv10 can significantly outperform previous state-of-the-art models in terms of computation-accuracy trade-offs across various model scales. As shown in Fig. 1, our YOLOv10-S / X are $1.8 \\times / 1.3 \\times$ faster than RT-DETRR18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B achieves a $46 \\%$ reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly efficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and 0.5 AP , with $1.8 \\times$ and $2.3 \\times$ smaller number of parameters, respectively. YOLOv10-M achieves the",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Compared with YOLOv9-C, YOLOv10-B achieves a $46 \\%$ reduction in latency with the same performance.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "081f7d48-bf27-4b0e-8900-2e4fcbb4b8dc",
        "questions": "What is the percentage reduction in parameters for the new AP model compared to YOLOv9-M?",
        "answers": "23%",
        "context": "similar AP compared with YOLOv9-M / YOLO-MS, with $23 \\%$ / $31 \\%$ fewer parameters, respectively. We hope that our work can inspire further studies and advancements in the field.\n\n\n2 Related Work\n\n\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low latency, which is crucial for real-world applications. Over the past years, substantial efforts have been directed towards developing efficient detectors [18, 51, 43, 32, 72, 69, 30, 29, 39]. Particularly, the YOLO series [43, 44, 45, 2, 19, 27, 56, 20, 59] stand out as the mainstream ones. YOLOv1, YOLOv2, and YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck, and head [43, 44, 45]. YOLOv4 [2] and YOLOv5 [19] introduce the CSPNet [57] design to replace DarkNet [42], coupled with data augmentation strategies, enhanced PAN, and a greater variety of model scales, etc. YOLOv6 [27] presents BiC and SimCSPSPPF for neck and backbone, respectively, with anchor-aided training and self-distillation strategy. YOLOv7 [56] introduces E-ELAN for rich gradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [20] presents C2f building block for effective feature extraction and fusion. Gold-YOLO [54] provides the advanced GD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [59] proposes GELAN to improve the architecture and PGI to augment the training process.\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from traditional pipelines, offering streamlined architectures [48]. DETR [4] introduces the transformer architecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating hand-crafted components and post-processing. Since then, various DETR variants have been proposed to enhance its performance and efficiency [40, 61, 50, 28, 34]. Deformable-DETR [74] leverages multi-scale deformable attention module to accelerate the convergence speed. DINO [67] integrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs. RT-DETR [71] further designs the efficient hybrid encoder and proposes the uncertainty-minimal query selection to improve both the accuracy and latency. Another line to achieve end-to-end object detection is based CNN detectors. Learnable NMS [23] and relation networks [25] present another network to remove duplicated predictions for detectors. OneNet [49] and DeFCN [60] propose one-to-one matching strategies to enable end-to-end object detection with fully convolutional networks. $\\mathrm{FCOS}_{\\mathrm{pss}}[73]$ introduces a positive sample selector to choose the optimal sample for prediction.\n\n3 Methodology\n\n3.1 Consistent Dual Assignments for NMS-free Training\n\nDuring training, YOLOs $[20,59,27,64]$ usually leverage TAL [14] to allocate multiple positive samples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals, facilitating the optimization and achieving superior performance. However, it necessitates YOLOs to rely on the NMS post-processing, which causes the suboptimal inference efficiency for deployment. While previous works $[49,60,73,5]$ explore one-to-one matching to suppress the redundant predictions, they usually introduce additional inference overhead or yield suboptimal performance. In this work, we present a NMS-free training strategy for YOLOs with dual label assignments and consistent matching metric, achieving both high efficiency and competitive performance.\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one prediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak supervision, which causes suboptimal accuracy and convergence speed [75]. Fortunately, this deficiency can be compensated for by the one-to-many assignment [5]. To achieve this, we introduce dual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown in Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure and adopts the same optimization objectives as the original one-to-many branch but leverages the one-to-one matching to obtain label assignments. During training, two heads are jointly optimized with the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-to-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which achieves the same performance as Hungarian matching [4] with less extra training time.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "similar AP compared with YOLOv9-M / YOLO-MS, with $23 \\%$ / $31 \\%$ fewer parameters, respectively.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08268550-d171-4798-91a0-375a2d0b4f43",
        "questions": "Which YOLO version introduces the C2f building block for effective feature extraction and fusion?",
        "answers": "YOLOv8",
        "context": "similar AP compared with YOLOv9-M / YOLO-MS, with $23 \\%$ / $31 \\%$ fewer parameters, respectively. We hope that our work can inspire further studies and advancements in the field.\n\n\n2 Related Work\n\n\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low latency, which is crucial for real-world applications. Over the past years, substantial efforts have been directed towards developing efficient detectors [18, 51, 43, 32, 72, 69, 30, 29, 39]. Particularly, the YOLO series [43, 44, 45, 2, 19, 27, 56, 20, 59] stand out as the mainstream ones. YOLOv1, YOLOv2, and YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck, and head [43, 44, 45]. YOLOv4 [2] and YOLOv5 [19] introduce the CSPNet [57] design to replace DarkNet [42], coupled with data augmentation strategies, enhanced PAN, and a greater variety of model scales, etc. YOLOv6 [27] presents BiC and SimCSPSPPF for neck and backbone, respectively, with anchor-aided training and self-distillation strategy. YOLOv7 [56] introduces E-ELAN for rich gradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [20] presents C2f building block for effective feature extraction and fusion. Gold-YOLO [54] provides the advanced GD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [59] proposes GELAN to improve the architecture and PGI to augment the training process.\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from traditional pipelines, offering streamlined architectures [48]. DETR [4] introduces the transformer architecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating hand-crafted components and post-processing. Since then, various DETR variants have been proposed to enhance its performance and efficiency [40, 61, 50, 28, 34]. Deformable-DETR [74] leverages multi-scale deformable attention module to accelerate the convergence speed. DINO [67] integrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs. RT-DETR [71] further designs the efficient hybrid encoder and proposes the uncertainty-minimal query selection to improve both the accuracy and latency. Another line to achieve end-to-end object detection is based CNN detectors. Learnable NMS [23] and relation networks [25] present another network to remove duplicated predictions for detectors. OneNet [49] and DeFCN [60] propose one-to-one matching strategies to enable end-to-end object detection with fully convolutional networks. $\\mathrm{FCOS}_{\\mathrm{pss}}[73]$ introduces a positive sample selector to choose the optimal sample for prediction.\n\n3 Methodology\n\n3.1 Consistent Dual Assignments for NMS-free Training\n\nDuring training, YOLOs $[20,59,27,64]$ usually leverage TAL [14] to allocate multiple positive samples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals, facilitating the optimization and achieving superior performance. However, it necessitates YOLOs to rely on the NMS post-processing, which causes the suboptimal inference efficiency for deployment. While previous works $[49,60,73,5]$ explore one-to-one matching to suppress the redundant predictions, they usually introduce additional inference overhead or yield suboptimal performance. In this work, we present a NMS-free training strategy for YOLOs with dual label assignments and consistent matching metric, achieving both high efficiency and competitive performance.\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one prediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak supervision, which causes suboptimal accuracy and convergence speed [75]. Fortunately, this deficiency can be compensated for by the one-to-many assignment [5]. To achieve this, we introduce dual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown in Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure and adopts the same optimization objectives as the original one-to-many branch but leverages the one-to-one matching to obtain label assignments. During training, two heads are jointly optimized with the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-to-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which achieves the same performance as Hungarian matching [4] with less extra training time.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "YOLOv8 [20] presents C2f building block for effective feature extraction and fusion.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "082aba19-4bc7-4aee-ae11-af49775c20e9",
        "questions": "Does the dual label assignment strategy for YOLOs require NMS post-processing during inference?",
        "answers": "No",
        "context": "similar AP compared with YOLOv9-M / YOLO-MS, with $23 \\%$ / $31 \\%$ fewer parameters, respectively. We hope that our work can inspire further studies and advancements in the field.\n\n\n2 Related Work\n\n\nReal-time object detectors. Real-time object detection aims to classify and locate objects under low latency, which is crucial for real-world applications. Over the past years, substantial efforts have been directed towards developing efficient detectors [18, 51, 43, 32, 72, 69, 30, 29, 39]. Particularly, the YOLO series [43, 44, 45, 2, 19, 27, 56, 20, 59] stand out as the mainstream ones. YOLOv1, YOLOv2, and YOLOv3 identify the typical detection architecture consisting of three parts, i.e., backbone, neck, and head [43, 44, 45]. YOLOv4 [2] and YOLOv5 [19] introduce the CSPNet [57] design to replace DarkNet [42], coupled with data augmentation strategies, enhanced PAN, and a greater variety of model scales, etc. YOLOv6 [27] presents BiC and SimCSPSPPF for neck and backbone, respectively, with anchor-aided training and self-distillation strategy. YOLOv7 [56] introduces E-ELAN for rich gradient flow path and explores several trainable bag-of-freebies methods. YOLOv8 [20] presents C2f building block for effective feature extraction and fusion. Gold-YOLO [54] provides the advanced GD mechanism to boost the multi-scale feature fusion capability. YOLOv9 [59] proposes GELAN to improve the architecture and PGI to augment the training process.\nEnd-to-end object detectors. End-to-end object detection has emerged as a paradigm shift from traditional pipelines, offering streamlined architectures [48]. DETR [4] introduces the transformer architecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating hand-crafted components and post-processing. Since then, various DETR variants have been proposed to enhance its performance and efficiency [40, 61, 50, 28, 34]. Deformable-DETR [74] leverages multi-scale deformable attention module to accelerate the convergence speed. DINO [67] integrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs. RT-DETR [71] further designs the efficient hybrid encoder and proposes the uncertainty-minimal query selection to improve both the accuracy and latency. Another line to achieve end-to-end object detection is based CNN detectors. Learnable NMS [23] and relation networks [25] present another network to remove duplicated predictions for detectors. OneNet [49] and DeFCN [60] propose one-to-one matching strategies to enable end-to-end object detection with fully convolutional networks. $\\mathrm{FCOS}_{\\mathrm{pss}}[73]$ introduces a positive sample selector to choose the optimal sample for prediction.\n\n3 Methodology\n\n3.1 Consistent Dual Assignments for NMS-free Training\n\nDuring training, YOLOs $[20,59,27,64]$ usually leverage TAL [14] to allocate multiple positive samples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals, facilitating the optimization and achieving superior performance. However, it necessitates YOLOs to rely on the NMS post-processing, which causes the suboptimal inference efficiency for deployment. While previous works $[49,60,73,5]$ explore one-to-one matching to suppress the redundant predictions, they usually introduce additional inference overhead or yield suboptimal performance. In this work, we present a NMS-free training strategy for YOLOs with dual label assignments and consistent matching metric, achieving both high efficiency and competitive performance.\nDual label assignments. Unlike one-to-many assignment, one-to-one matching assigns only one prediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak supervision, which causes suboptimal accuracy and convergence speed [75]. Fortunately, this deficiency can be compensated for by the one-to-many assignment [5]. To achieve this, we introduce dual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown in Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure and adopts the same optimization objectives as the original one-to-many branch but leverages the one-to-one matching to obtain label assignments. During training, two heads are jointly optimized with the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-to-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which achieves the same performance as Hungarian matching [4] with less extra training time.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "During inference, we discard the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08359e8a-9268-4b5e-a8ad-335586e35301",
        "questions": "What optimizer is used for training all YOLOv10 models, and how many epochs are they trained for?",
        "answers": "SGD optimizer, 500 epochs",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Following [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "083851f3-d845-4428-af7a-08bc65b57e18",
        "questions": "What is the initial learning rate for the YOLOv10 models, and how does it change over time?",
        "answers": "The initial learning rate is $1 \times 10^{-2}$ and it decays linearly to $1 \times 10^{-4}$.",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The initial learning rate is $1 \times 10^{-2}$ and it decays linearly to $1 \times 10^{-4}$.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "083d5ca3-616a-47da-b05d-8006f3c380d8",
        "questions": "What is the expansion factor used for the FFN in the PSA module of YOLOv10 models?",
        "answers": "2",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08439ba1-2afd-45c3-a748-4ffbbfd9de84",
        "questions": "Who are the authors of the paper titled 'Path aggregation network for instance segmentation' presented at the IEEE conference on computer vision and pattern recognition in 2018?",
        "answers": "Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia",
        "context": "[35] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $8759-8768,2018$.\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.\n[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022.\n[38] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016.\n[39] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors. arXiv preprint arXiv:2212.07784, 2022.\n[40] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3651-3660, 2021.\n[41] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of statistics and its application, 6:405-431, 2019.\n[42] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/ darknet/, 2013-2016.\n[43] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[44] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n[45] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018.\n[47] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430-8439, 2019.\n[48] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2325-2333, 2016.\n[49] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What makes for end-to-end object detection? In International Conference on Machine Learning, pages 9934-9944. PMLR, 2021.\n[50] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14454-14463, 2021.\n[51] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922-1933, 2020 .",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $8759-8768,2018$.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "084993fb-3fb1-408e-a5e2-f858789dba64",
        "questions": "In which year was the paper 'Statistical aspects of wasserstein distances' by Victor M Panaretos and Yoav Zemel published in the Annual Review of Statistics and Its Application?",
        "answers": "2019",
        "context": "[35] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $8759-8768,2018$.\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.\n[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022.\n[38] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016.\n[39] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors. arXiv preprint arXiv:2212.07784, 2022.\n[40] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3651-3660, 2021.\n[41] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of statistics and its application, 6:405-431, 2019.\n[42] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/ darknet/, 2013-2016.\n[43] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[44] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n[45] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018.\n[47] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430-8439, 2019.\n[48] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2325-2333, 2016.\n[49] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What makes for end-to-end object detection? In International Conference on Machine Learning, pages 9934-9944. PMLR, 2021.\n[50] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14454-14463, 2021.\n[51] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922-1933, 2020 .",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of statistics and its application, 6:405-431, 2019.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "084e0003-5046-4dca-8b7d-71e98eb00e64",
        "questions": "Is the paper 'Sparse r-cnn: End-to-end object detection with learnable proposals' authored by Peize Sun and others presented at the IEEE/CVF conference on computer vision and pattern recognition in 2021?",
        "answers": "Yes",
        "context": "[35] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $8759-8768,2018$.\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021.\n[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022.\n[38] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016.\n[39] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors. arXiv preprint arXiv:2212.07784, 2022.\n[40] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3651-3660, 2021.\n[41] Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of statistics and its application, 6:405-431, 2019.\n[42] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/ darknet/, 2013-2016.\n[43] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[44] Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n[45] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018.\n[47] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430-8439, 2019.\n[48] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2325-2333, 2016.\n[49] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan, Changhu Wang, and Ping Luo. What makes for end-to-end object detection? In International Conference on Machine Learning, pages 9934-9944. PMLR, 2021.\n[50] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14454-14463, 2021.\n[51] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1922-1933, 2020 .",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14454-14463, 2021.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "084edeb0-e53a-4fe5-846a-87592dbca078",
        "questions": "What is the latency of the IRB model as shown in the document?",
        "answers": "2.30",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "IRB & 43.7 & 2.30",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "085d7943-eb63-47ef-9540-654de644d4f4",
        "questions": "How much AP improvement does the large-kernel convolution bring for YOLOv10-S according to the document?",
        "answers": "0.4% AP",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0870e15c-2fcd-41b5-924b-ace4307bf326",
        "questions": "Does the document indicate that the large-kernel convolution is employed for YOLOv10-M?",
        "answers": "No",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0876daaa-01ce-4fc6-b2d1-16fe8f08963c",
        "questions": "Which publication by Ross Girshick focuses on a method for object detection and was presented at the IEEE international conference on computer vision in 2015?",
        "answers": "Fast r-cnn",
        "context": "[18] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages $1440-1448,2015$.\n[19] Jocher Glenn. Yolov5 release v7.0. https: // github. com/ultralytics/yolov5/tree/ v7. $0,2022$.\n[20] Jocher Glenn. Yolov8. https://github. com/ultralytics/ultralytics/tree/ main, 2023.\n[21] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259$12269,2021$.\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n[23] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $4507-4515,2017$.\n[24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n[25] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3588-3597, 2018.\n[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.\n[27] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading. arXiv preprint arXiv:2301.05586, 2023.\n[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13619-13627, 2022.\n[29] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11632-11641, 2021.\n[30] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002-21012, 2020.\n[31] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design for gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.\n[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017.\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n[34] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022 .",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages $1440-1448,2015$.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "087d9b9c-202c-440d-bbfe-de43b337ed27",
        "questions": "In which year was the paper 'Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection' presented at the IEEE/CVF conference on computer vision and pattern recognition?",
        "answers": "2021",
        "context": "[18] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages $1440-1448,2015$.\n[19] Jocher Glenn. Yolov5 release v7.0. https: // github. com/ultralytics/yolov5/tree/ v7. $0,2022$.\n[20] Jocher Glenn. Yolov8. https://github. com/ultralytics/ultralytics/tree/ main, 2023.\n[21] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259$12269,2021$.\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n[23] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $4507-4515,2017$.\n[24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n[25] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3588-3597, 2018.\n[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.\n[27] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading. arXiv preprint arXiv:2301.05586, 2023.\n[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13619-13627, 2022.\n[29] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11632-11641, 2021.\n[30] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002-21012, 2020.\n[31] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design for gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.\n[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017.\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n[34] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022 .",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11632-11641, 2021.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0884fd16-636a-4f8c-a7e3-f52c0cbf404d",
        "questions": "Is the publication 'Dab-detr: Dynamic anchor boxes are better queries for detr' available as an arXiv preprint?",
        "answers": "Yes",
        "context": "[18] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages $1440-1448,2015$.\n[19] Jocher Glenn. Yolov5 release v7.0. https: // github. com/ultralytics/yolov5/tree/ v7. $0,2022$.\n[20] Jocher Glenn. Yolov8. https://github. com/ultralytics/ultralytics/tree/ main, 2023.\n[21] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259$12269,2021$.\n[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n[23] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $4507-4515,2017$.\n[24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\n[25] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3588-3597, 2018.\n[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456. pmlr, 2015.\n[27] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading. arXiv preprint arXiv:2301.05586, 2023.\n[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13619-13627, 2022.\n[29] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11632-11641, 2021.\n[30] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002-21012, 2020.\n[31] Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Neural architecture design for gpu-efficient networks. arXiv preprint arXiv:2006.14090, 2020.\n[32] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017.\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.\n[34] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022 .",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "088808b9-fc23-48aa-9df1-ec81bc54eab3",
        "questions": "What is the latency in milliseconds for the forward process of the YOLOv10-N model?",
        "answers": "1.79",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "088affd2-f3f8-46ad-985c-7b8c67b340cf",
        "questions": "How many fewer parameters does YOLOv10-L have compared to Gold-YOLO-L?",
        "answers": "68%",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08900b58-d75d-4659-a079-45e39329393d",
        "questions": "By how much does the NMS-free training with consistent dual assignments reduce the end-to-end latency of YOLOv10-S?",
        "answers": "4.63 ms",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0895e48e-6e22-438d-afb5-fe36642d0f77",
        "questions": "Which paper introduced the concept of 'Attention is all you need' and who are the authors?",
        "answers": "The paper 'Attention is all you need' was introduced by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.",
        "context": "[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[53] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. arXiv preprint arXiv:2307.09283, 2023.\n[54] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in Neural Information Processing Systems, 36, 2024.\n[55] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 13029-13038, 2021.\n[56] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-offreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7464-7475, 2023.\n[57] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390-391, 2020.\n[58] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies through gradient path analysis. arXiv preprint arXiv:2211.04800, 2022.\n[59] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.\n[60] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15849-15858, 2021.\n[61] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages $2567-2575,2022$.\n[62] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22-31, 2021.\n[63] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with attention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875, 2023.\n[64] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo. arXiv preprint arXiv:2203.16250, 2022.\n[65] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo: A report on real-time object detection design. arXiv preprint arXiv:2211.15444, 2022.\n[66] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages $659-675$. Springer, 2022.\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\n[68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "089b30a0-809f-4923-87a1-798fe6addd9e",
        "questions": "In which year was the paper 'Scaled-yolov4: Scaling cross stage partial network' presented at the IEEE/CVF conference on computer vision and pattern recognition?",
        "answers": "2021",
        "context": "[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[53] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. arXiv preprint arXiv:2307.09283, 2023.\n[54] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in Neural Information Processing Systems, 36, 2024.\n[55] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 13029-13038, 2021.\n[56] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-offreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7464-7475, 2023.\n[57] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390-391, 2020.\n[58] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies through gradient path analysis. arXiv preprint arXiv:2211.04800, 2022.\n[59] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.\n[60] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15849-15858, 2021.\n[61] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages $2567-2575,2022$.\n[62] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22-31, 2021.\n[63] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with attention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875, 2023.\n[64] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo. arXiv preprint arXiv:2203.16250, 2022.\n[65] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo: A report on real-time object detection design. arXiv preprint arXiv:2211.15444, 2022.\n[66] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages $659-675$. Springer, 2022.\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\n[68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 13029-13038, 2021.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "089f059c-f6e6-4035-b612-12fe40dac72e",
        "questions": "Is the paper 'Dino: Detr with improved denoising anchor boxes for end-to-end object detection' available as an arXiv preprint?",
        "answers": "Yes",
        "context": "[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[53] Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. arXiv preprint arXiv:2307.09283, 2023.\n[54] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. Advances in Neural Information Processing Systems, 36, 2024.\n[55] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 13029-13038, 2021.\n[56] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-offreebies sets new state-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7464-7475, 2023.\n[57] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 390-391, 2020.\n[58] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh. Designing network design strategies through gradient path analysis. arXiv preprint arXiv:2211.04800, 2022.\n[59] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv preprint arXiv:2402.13616, 2024.\n[60] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15849-15858, 2021.\n[61] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages $2567-2575,2022$.\n[62] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 22-31, 2021.\n[63] Haiyang Xu, Zhichao Zhou, Dongliang He, Fu Li, and Jingdong Wang. Vision transformer with attention map hallucination and ffn compaction. arXiv preprint arXiv:2306.10875, 2023.\n[64] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, et al. Pp-yoloe: An evolved version of yolo. arXiv preprint arXiv:2203.16250, 2022.\n[65] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, and Xiuyu Sun. Damo-yolo: A report on real-time object detection design. arXiv preprint arXiv:2211.15444, 2022.\n[66] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages $659-675$. Springer, 2022.\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\n[68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08a30261-b675-4bef-ac60-1ff0b0350134",
        "questions": "What is the performance gap in percentage points between one-to-many training with NMS and NMS-free training for YOLOv10-N?",
        "answers": "1.0",
        "context": "Figure 4: Visualization results under complex and challenging scenarios.\n\nLimitation. Due to the limited computational resources, we do not investigate the pretraining of YOLOv10 on large-scale datasets, e.g., Objects365 [47]. Besides, although we can achieve competitive end-to-end performance using the one-to-one head under NMS-free training, there still exists a performance gap compared with the original one-to-many training using NMS, especially noticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of one-to-many training with NMS outperforms that of NMS-free training by $1.0 \\% \\mathrm{AP}$ and $0.5 \\% \\mathrm{AP}$, respectively. We will explore ways to further reduce the gap and achieve higher performance for YOLOv10 in the future work.\n\nBroader impact. The YOLOs can be widely applied in various real-world applications, including medical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these fields and improve the efficiency. However, we acknowledge the potential for malicious use of our models. We will make every effort to prevent this.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For example, in YOLOv10-N and YOLOv10-S, the performance of one-to-many training with NMS outperforms that of NMS-free training by $1.0 \\% \\mathrm{AP}$ and $0.5 \\% \\mathrm{AP}$, respectively.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08a3b1e3-64e2-4c6f-bb48-146954fe81bb",
        "questions": "In which fields does the document suggest YOLOv10 could be applied to improve efficiency?",
        "answers": "medical image analyses and autonomous driving",
        "context": "Figure 4: Visualization results under complex and challenging scenarios.\n\nLimitation. Due to the limited computational resources, we do not investigate the pretraining of YOLOv10 on large-scale datasets, e.g., Objects365 [47]. Besides, although we can achieve competitive end-to-end performance using the one-to-one head under NMS-free training, there still exists a performance gap compared with the original one-to-many training using NMS, especially noticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of one-to-many training with NMS outperforms that of NMS-free training by $1.0 \\% \\mathrm{AP}$ and $0.5 \\% \\mathrm{AP}$, respectively. We will explore ways to further reduce the gap and achieve higher performance for YOLOv10 in the future work.\n\nBroader impact. The YOLOs can be widely applied in various real-world applications, including medical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these fields and improve the efficiency. However, we acknowledge the potential for malicious use of our models. We will make every effort to prevent this.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The YOLOs can be widely applied in various real-world applications, including medical image analyses and autonomous driving, etc.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08b2ace5-f544-412b-8fb5-f417de2c4c68",
        "questions": "Does the document mention any potential negative impacts of YOLOv10?",
        "answers": "Yes",
        "context": "Figure 4: Visualization results under complex and challenging scenarios.\n\nLimitation. Due to the limited computational resources, we do not investigate the pretraining of YOLOv10 on large-scale datasets, e.g., Objects365 [47]. Besides, although we can achieve competitive end-to-end performance using the one-to-one head under NMS-free training, there still exists a performance gap compared with the original one-to-many training using NMS, especially noticeable in small models. For example, in YOLOv10-N and YOLOv10-S, the performance of one-to-many training with NMS outperforms that of NMS-free training by $1.0 \\% \\mathrm{AP}$ and $0.5 \\% \\mathrm{AP}$, respectively. We will explore ways to further reduce the gap and achieve higher performance for YOLOv10 in the future work.\n\nBroader impact. The YOLOs can be widely applied in various real-world applications, including medical image analyses and autonomous driving, etc. We hope that our YOLOv10 can assist in these fields and improve the efficiency. However, we acknowledge the potential for malicious use of our models. We will make every effort to prevent this.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "However, we acknowledge the potential for malicious use of our models.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08b8427a-8a34-46d4-8dc7-8aa722ee475c",
        "questions": "What is the default value of the hyperparameter beta for the one-to-many matching metric in YOLOv8-S?",
        "answers": "6",
        "context": "Figure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one assignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6$ by default [20]. For consistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=6$. For inconsistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=2$.\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches leverage a metric to quantitatively assess the level of concordance between predictions and instances. To achieve prediction aware matching for both branches, we employ a uniform matching metric, i.e.,\n$$m(\\alpha, \\beta)=s \\cdot p^{\\alpha} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta}$$\nwhere $p$ is the classification score, $\\hat{b}$ and $b$ denote the bounding box of prediction and instance, respectively. $s$ represents the spatial prior indicating whether the anchor point of prediction is within the instance $[20,59,27,64] . \\alpha$ and $\\beta$ are two important hyperparameters that balance the impact of the semantic prediction task and the location regression task. We denote the one-to-many and one-to-one metrics as $m_{o 2 m}=m\\left(\\alpha_{o 2 m}, \\beta_{o 2 m}\\right)$ and $m_{o 2 o}=m\\left(\\alpha_{o 2 o}, \\beta_{o 2 o}\\right)$, respectively. These metrics influence the label assignments and supervision information for the two heads.\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that of one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many head's optimization. As a result, the one-to-one head can provide improved quality of samples during inference, leading to better performance. To this end, we first analyze the supervision gap between the two heads. Due to the randomness during training, we initiate our examination in the beginning with two heads initialized with the same values and producing the same predictions, i.e., one-to-one head and one-to-many head generate the same $p$ and IoU for each prediction-instance pair. We note that the regression targets of two branches do not conflict, as matched predictions share the same targets and unmatched predictions are ignored. The supervision gap thus lies in the different classification targets. Given an instance, we denote its largest IoU with predictions as $u^{*}$, and the largest one-to-many and one-to-one matching scores as $m_{o 2 m}^{*}$ and $m_{o 2 o}^{*}$, respectively. Suppose that one-to-many branch yields the positive samples $\\Omega$ and one-to-one branch selects $i$-th prediction with the metric $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can then derive the classification target $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$ for task aligned loss as in [20, 59, 27, 64, 14]. The supervision gap between two branches can thus be derived by the 1-Wasserstein distance [41] of different classification objectives, i.e.,\n$$A=t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}$$\n\nWe can observe that the gap decreases as $t_{o 2 m, i}$ increases, i.e., $i$ ranks higher within $\\Omega$. It reaches the minimum when $t_{o 2 m, i}=u^{*}$, i.e., $i$ is the best positive sample in $\\Omega$, as shown in Fig. 2.(a). To achieve this, we present the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, which implies $m_{o 2 o}=m_{o 2 m}^{r}$. Therefore, the best positive sample for one-to-many head is also the best for one-to-one head. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we take $r=1$, by default, i.e., $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$. To verify the improved supervision alignment, we count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results after training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric. For a more comprehensive understanding of the mathematical proof, please refer to the appendix.\n\n\n3.2 Holistic Efficiency-Accuracy Driven Model Design\n\n\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the efficiency-accuracy trade-offs [45, 7, 27]. Although previous works explore various design strategies,",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Frequency of one-to-one assignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6$ by default [20].",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08ba3f66-b596-4a46-8fd0-fc309e875edb",
        "questions": "What is the relationship between the one-to-one and one-to-many matching metrics when the consistent matching metric is applied?",
        "answers": "m_{o 2 o}=m_{o 2 m}^{r}",
        "context": "Figure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one assignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6$ by default [20]. For consistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=6$. For inconsistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=2$.\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches leverage a metric to quantitatively assess the level of concordance between predictions and instances. To achieve prediction aware matching for both branches, we employ a uniform matching metric, i.e.,\n$$m(\\alpha, \\beta)=s \\cdot p^{\\alpha} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta}$$\nwhere $p$ is the classification score, $\\hat{b}$ and $b$ denote the bounding box of prediction and instance, respectively. $s$ represents the spatial prior indicating whether the anchor point of prediction is within the instance $[20,59,27,64] . \\alpha$ and $\\beta$ are two important hyperparameters that balance the impact of the semantic prediction task and the location regression task. We denote the one-to-many and one-to-one metrics as $m_{o 2 m}=m\\left(\\alpha_{o 2 m}, \\beta_{o 2 m}\\right)$ and $m_{o 2 o}=m\\left(\\alpha_{o 2 o}, \\beta_{o 2 o}\\right)$, respectively. These metrics influence the label assignments and supervision information for the two heads.\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that of one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many head's optimization. As a result, the one-to-one head can provide improved quality of samples during inference, leading to better performance. To this end, we first analyze the supervision gap between the two heads. Due to the randomness during training, we initiate our examination in the beginning with two heads initialized with the same values and producing the same predictions, i.e., one-to-one head and one-to-many head generate the same $p$ and IoU for each prediction-instance pair. We note that the regression targets of two branches do not conflict, as matched predictions share the same targets and unmatched predictions are ignored. The supervision gap thus lies in the different classification targets. Given an instance, we denote its largest IoU with predictions as $u^{*}$, and the largest one-to-many and one-to-one matching scores as $m_{o 2 m}^{*}$ and $m_{o 2 o}^{*}$, respectively. Suppose that one-to-many branch yields the positive samples $\\Omega$ and one-to-one branch selects $i$-th prediction with the metric $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can then derive the classification target $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$ for task aligned loss as in [20, 59, 27, 64, 14]. The supervision gap between two branches can thus be derived by the 1-Wasserstein distance [41] of different classification objectives, i.e.,\n$$A=t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}$$\n\nWe can observe that the gap decreases as $t_{o 2 m, i}$ increases, i.e., $i$ ranks higher within $\\Omega$. It reaches the minimum when $t_{o 2 m, i}=u^{*}$, i.e., $i$ is the best positive sample in $\\Omega$, as shown in Fig. 2.(a). To achieve this, we present the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, which implies $m_{o 2 o}=m_{o 2 m}^{r}$. Therefore, the best positive sample for one-to-many head is also the best for one-to-one head. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we take $r=1$, by default, i.e., $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$. To verify the improved supervision alignment, we count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results after training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric. For a more comprehensive understanding of the mathematical proof, please refer to the appendix.\n\n\n3.2 Holistic Efficiency-Accuracy Driven Model Design\n\n\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the efficiency-accuracy trade-offs [45, 7, 27]. Although previous works explore various design strategies,",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "To achieve this, we present the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, which implies $m_{o 2 o}=m_{o 2 m}^{r}$.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08bd4713-f044-472f-88d4-005ef427a696",
        "questions": "Does the supervision gap between the one-to-one and one-to-many branches decrease when the classification target for the one-to-many branch increases?",
        "answers": "Yes",
        "context": "Figure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one assignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6$ by default [20]. For consistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=6$. For inconsistency, $\\alpha_{o 2 o}=0.5 ; \\beta_{o 2 o}=2$.\nConsistent matching metric. During assignments, both one-to-one and one-to-many approaches leverage a metric to quantitatively assess the level of concordance between predictions and instances. To achieve prediction aware matching for both branches, we employ a uniform matching metric, i.e.,\n$$m(\\alpha, \\beta)=s \\cdot p^{\\alpha} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta}$$\nwhere $p$ is the classification score, $\\hat{b}$ and $b$ denote the bounding box of prediction and instance, respectively. $s$ represents the spatial prior indicating whether the anchor point of prediction is within the instance $[20,59,27,64] . \\alpha$ and $\\beta$ are two important hyperparameters that balance the impact of the semantic prediction task and the location regression task. We denote the one-to-many and one-to-one metrics as $m_{o 2 m}=m\\left(\\alpha_{o 2 m}, \\beta_{o 2 m}\\right)$ and $m_{o 2 o}=m\\left(\\alpha_{o 2 o}, \\beta_{o 2 o}\\right)$, respectively. These metrics influence the label assignments and supervision information for the two heads.\nIn dual label assignments, the one-to-many branch provides much richer supervisory signals than one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that of one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many head's optimization. As a result, the one-to-one head can provide improved quality of samples during inference, leading to better performance. To this end, we first analyze the supervision gap between the two heads. Due to the randomness during training, we initiate our examination in the beginning with two heads initialized with the same values and producing the same predictions, i.e., one-to-one head and one-to-many head generate the same $p$ and IoU for each prediction-instance pair. We note that the regression targets of two branches do not conflict, as matched predictions share the same targets and unmatched predictions are ignored. The supervision gap thus lies in the different classification targets. Given an instance, we denote its largest IoU with predictions as $u^{*}$, and the largest one-to-many and one-to-one matching scores as $m_{o 2 m}^{*}$ and $m_{o 2 o}^{*}$, respectively. Suppose that one-to-many branch yields the positive samples $\\Omega$ and one-to-one branch selects $i$-th prediction with the metric $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can then derive the classification target $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$ for task aligned loss as in [20, 59, 27, 64, 14]. The supervision gap between two branches can thus be derived by the 1-Wasserstein distance [41] of different classification objectives, i.e.,\n$$A=t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}$$\n\nWe can observe that the gap decreases as $t_{o 2 m, i}$ increases, i.e., $i$ ranks higher within $\\Omega$. It reaches the minimum when $t_{o 2 m, i}=u^{*}$, i.e., $i$ is the best positive sample in $\\Omega$, as shown in Fig. 2.(a). To achieve this, we present the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, which implies $m_{o 2 o}=m_{o 2 m}^{r}$. Therefore, the best positive sample for one-to-many head is also the best for one-to-one head. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we take $r=1$, by default, i.e., $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$. To verify the improved supervision alignment, we count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results after training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric. For a more comprehensive understanding of the mathematical proof, please refer to the appendix.\n\n\n3.2 Holistic Efficiency-Accuracy Driven Model Design\n\n\nIn addition to the post-processing, the model architectures of YOLOs also pose great challenges to the efficiency-accuracy trade-offs [45, 7, 27]. Although previous works explore various design strategies,",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We can observe that the gap decreases as $t_{o 2 m, i}$ increases, i.e., $i$ ranks higher within $\\Omega$.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08bee994-0814-44b9-b7fb-38722edee8e6",
        "questions": "In which year was the paper 'Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection' published?",
        "answers": "2020",
        "context": "[69] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020 .\n[70] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12083-12093, 2022.\n[71] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023.\n[72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 12993-13000, 2020.\n[73] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms. IEEE Transactions on Multimedia, 2023.\n[74] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n[75] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748-6758, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08ccf199-2ed9-43d0-b846-ce9a623d110d",
        "questions": "Who are the authors of the paper titled 'Topformer: Token pyramid transformer for mobile semantic segmentation'?",
        "answers": "Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen",
        "context": "[69] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020 .\n[70] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12083-12093, 2022.\n[71] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023.\n[72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 12993-13000, 2020.\n[73] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms. IEEE Transactions on Multimedia, 2023.\n[74] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n[75] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748-6758, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12083-12093, 2022.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08db1e3e-915c-4259-bab2-d8ab2818e27a",
        "questions": "Is the paper 'Object detection made simpler by eliminating heuristic nms' published in IEEE Transactions on Multimedia?",
        "answers": "Yes",
        "context": "[69] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020 .\n[70] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12083-12093, 2022.\n[71] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. arXiv preprint arXiv:2304.08069, 2023.\n[72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better learning for bounding box regression. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 12993-13000, 2020.\n[73] Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms. IEEE Transactions on Multimedia, 2023.\n[74] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n[75] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748-6758, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Qiang Zhou and Chaohui Yu. Object detection made simpler by eliminating heuristic nms. IEEE Transactions on Multimedia, 2023.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08df6b19-7c3a-4037-a0d5-c16e961139cf",
        "questions": "What is the AP performance of the YOLOv10-X model on the COCO dataset for large objects?",
        "answers": "70.9%",
        "context": "```\nAlgorithm 1: Rank-guided block design\nInput: Intrinsic ranks $R$ for all stages $S$; Original Network $\\Theta ; \\operatorname{CIB} \\theta_{c i b}$;\nOutput: New network $\\Theta^{*}$ with CIB for certain stages.\n$t \\leftarrow 0 ;$\n$\\Theta_{0} \\leftarrow \\Theta ; \\Theta^{*} \\leftarrow \\Theta_{0} ;$\n$a p_{0} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{0}\\right)\\right) ; \\quad / / \\mathrm{T}:$ training the network; AP:evaluating the AP performance.\nwhile $S \\neq \\emptyset$ do\n    $\\boldsymbol{s}_{t} \\leftarrow \\operatorname{argmin}_{s \\in S} R$;\n    $\\Theta_{t+1} \\leftarrow$ Replace $\\left(\\Theta_{t}, \\theta_{c i b}, \\boldsymbol{s}_{t}\\right)$; // Replace the block in Stage $s_{t}$ of $\\Theta_{t}$ with CIB $\\theta_{c i b}$.\n    $a p_{t+1} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{t+1}\\right)\\right) ;$\n    if $a p_{t+1} \\geq a p_{0}$ then\n        $\\Theta^{*} \\leftarrow \\Theta_{t+1} ; S \\leftarrow S \\backslash\\left\\{s_{t}\\right\\}$\n    else\n        return $\\Theta^{*} ;$\n    end\nend\nreturn $\\Theta^{*}$;\n```\n\nTable 15: Detailed performance of YOLOv10 on COCO.\n\\begin{tabular}{lcccccc}\n  Model & $\\mathrm{AP}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{50}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{75}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {small }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {medium }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {large }}^{\\text {val }}(\\%)$ \\\\\n  YOLOv10-N & 38.5 & 53.8 & 41.7 & 18.9 & 42.4 & 54.6 \\\\\nYOLOv10-S & 46.3 & 63.0 & 50.4 & 26.8 & 51.0 & 63.8 \\\\\nYOLOv10-M & 51.1 & 68.1 & 55.8 & 33.8 & 56.5 & 67.0 \\\\\nYOLOv10-B & 52.5 & 69.6 & 57.2 & 35.1 & 57.8 & 68.5 \\\\\nYOLOv10-L & 53.2 & 70.1 & 58.1 & 35.8 & 58.5 & 69.4 \\\\\nYOLOv10-X & 54.4 & 71.3 & 59.3 & 37.0 & 59.8 & 70.9 \\\\\n \n\\end{tabular}\n0.05 ms latency overhead. Besides, for YOLOv10-M (\\#6 in Tab. 2), which has a larger model scale and more redundancy, our efficiency driven model design results in a considerable $12.5 \\%$ latency reduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a notable $0.8 \\%$ AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48 ms . These results well demonstrate the effectiveness of our design strategy across different model scales.\n\n\nA. 6 Visualization Results\n\n\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It can be observed that YOLOv10 can achieve precise detection under various difficult conditions, such as low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely packed objects, such as bottle, cup, and person. These results indicate its superior performance.\n\nA. 7 Contribution, Limitation, and Broader Impact\n\nContribution. In summary, our contributions are three folds as follows:\n1. We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label assignments way is designed to provide rich supervision by one-to-many branch during training and high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious supervision between two branches, we innovatively propose the consistent matching metric, which can well reduce the theoretical supervision gap and lead to improved performance.\n2. We propose a holistic efficiency-accuracy driven model design strategy for the model architecture of YOLOs. We present novel lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, which greatly reduce the computational redundancy and achieve high efficiency. We further introduce the large-kernel convolution and innovative partial self-attention module, which effectively enhance the performance under low cost.\n3. Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object detector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art performance and efficiency trade-offs compared with other advanced detectors.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "YOLOv10-X & 54.4 & 71.3 & 59.3 & 37.0 & 59.8 & 70.9",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08e0e8df-688b-49e7-8449-6aef90a27b0e",
        "questions": "What is the latency reduction achieved by the efficiency-driven model design for YOLOv10-M?",
        "answers": "12.5%",
        "context": "```\nAlgorithm 1: Rank-guided block design\nInput: Intrinsic ranks $R$ for all stages $S$; Original Network $\\Theta ; \\operatorname{CIB} \\theta_{c i b}$;\nOutput: New network $\\Theta^{*}$ with CIB for certain stages.\n$t \\leftarrow 0 ;$\n$\\Theta_{0} \\leftarrow \\Theta ; \\Theta^{*} \\leftarrow \\Theta_{0} ;$\n$a p_{0} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{0}\\right)\\right) ; \\quad / / \\mathrm{T}:$ training the network; AP:evaluating the AP performance.\nwhile $S \\neq \\emptyset$ do\n    $\\boldsymbol{s}_{t} \\leftarrow \\operatorname{argmin}_{s \\in S} R$;\n    $\\Theta_{t+1} \\leftarrow$ Replace $\\left(\\Theta_{t}, \\theta_{c i b}, \\boldsymbol{s}_{t}\\right)$; // Replace the block in Stage $s_{t}$ of $\\Theta_{t}$ with CIB $\\theta_{c i b}$.\n    $a p_{t+1} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{t+1}\\right)\\right) ;$\n    if $a p_{t+1} \\geq a p_{0}$ then\n        $\\Theta^{*} \\leftarrow \\Theta_{t+1} ; S \\leftarrow S \\backslash\\left\\{s_{t}\\right\\}$\n    else\n        return $\\Theta^{*} ;$\n    end\nend\nreturn $\\Theta^{*}$;\n```\n\nTable 15: Detailed performance of YOLOv10 on COCO.\n\\begin{tabular}{lcccccc}\n  Model & $\\mathrm{AP}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{50}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{75}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {small }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {medium }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {large }}^{\\text {val }}(\\%)$ \\\\\n  YOLOv10-N & 38.5 & 53.8 & 41.7 & 18.9 & 42.4 & 54.6 \\\\\nYOLOv10-S & 46.3 & 63.0 & 50.4 & 26.8 & 51.0 & 63.8 \\\\\nYOLOv10-M & 51.1 & 68.1 & 55.8 & 33.8 & 56.5 & 67.0 \\\\\nYOLOv10-B & 52.5 & 69.6 & 57.2 & 35.1 & 57.8 & 68.5 \\\\\nYOLOv10-L & 53.2 & 70.1 & 58.1 & 35.8 & 58.5 & 69.4 \\\\\nYOLOv10-X & 54.4 & 71.3 & 59.3 & 37.0 & 59.8 & 70.9 \\\\\n \n\\end{tabular}\n0.05 ms latency overhead. Besides, for YOLOv10-M (\\#6 in Tab. 2), which has a larger model scale and more redundancy, our efficiency driven model design results in a considerable $12.5 \\%$ latency reduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a notable $0.8 \\%$ AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48 ms . These results well demonstrate the effectiveness of our design strategy across different model scales.\n\n\nA. 6 Visualization Results\n\n\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It can be observed that YOLOv10 can achieve precise detection under various difficult conditions, such as low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely packed objects, such as bottle, cup, and person. These results indicate its superior performance.\n\nA. 7 Contribution, Limitation, and Broader Impact\n\nContribution. In summary, our contributions are three folds as follows:\n1. We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label assignments way is designed to provide rich supervision by one-to-many branch during training and high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious supervision between two branches, we innovatively propose the consistent matching metric, which can well reduce the theoretical supervision gap and lead to improved performance.\n2. We propose a holistic efficiency-accuracy driven model design strategy for the model architecture of YOLOs. We present novel lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, which greatly reduce the computational redundancy and achieve high efficiency. We further introduce the large-kernel convolution and innovative partial self-attention module, which effectively enhance the performance under low cost.\n3. Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object detector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art performance and efficiency trade-offs compared with other advanced detectors.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "our efficiency driven model design results in a considerable $12.5 \\%$ latency reduction, as shown in Tab. 2.",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08e2745c-f106-4c85-b87c-252130c4e2e6",
        "questions": "Does the rank-guided block design contribute to the efficiency-accuracy driven model design strategy for YOLOs?",
        "answers": "Yes",
        "context": "```\nAlgorithm 1: Rank-guided block design\nInput: Intrinsic ranks $R$ for all stages $S$; Original Network $\\Theta ; \\operatorname{CIB} \\theta_{c i b}$;\nOutput: New network $\\Theta^{*}$ with CIB for certain stages.\n$t \\leftarrow 0 ;$\n$\\Theta_{0} \\leftarrow \\Theta ; \\Theta^{*} \\leftarrow \\Theta_{0} ;$\n$a p_{0} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{0}\\right)\\right) ; \\quad / / \\mathrm{T}:$ training the network; AP:evaluating the AP performance.\nwhile $S \\neq \\emptyset$ do\n    $\\boldsymbol{s}_{t} \\leftarrow \\operatorname{argmin}_{s \\in S} R$;\n    $\\Theta_{t+1} \\leftarrow$ Replace $\\left(\\Theta_{t}, \\theta_{c i b}, \\boldsymbol{s}_{t}\\right)$; // Replace the block in Stage $s_{t}$ of $\\Theta_{t}$ with CIB $\\theta_{c i b}$.\n    $a p_{t+1} \\leftarrow \\mathrm{AP}\\left(\\mathrm{T}\\left(\\Theta_{t+1}\\right)\\right) ;$\n    if $a p_{t+1} \\geq a p_{0}$ then\n        $\\Theta^{*} \\leftarrow \\Theta_{t+1} ; S \\leftarrow S \\backslash\\left\\{s_{t}\\right\\}$\n    else\n        return $\\Theta^{*} ;$\n    end\nend\nreturn $\\Theta^{*}$;\n```\n\nTable 15: Detailed performance of YOLOv10 on COCO.\n\\begin{tabular}{lcccccc}\n  Model & $\\mathrm{AP}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{50}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{75}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {small }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {medium }}^{\\text {val }}(\\%)$ & $\\mathrm{AP}_{\\text {large }}^{\\text {val }}(\\%)$ \\\\\n  YOLOv10-N & 38.5 & 53.8 & 41.7 & 18.9 & 42.4 & 54.6 \\\\\nYOLOv10-S & 46.3 & 63.0 & 50.4 & 26.8 & 51.0 & 63.8 \\\\\nYOLOv10-M & 51.1 & 68.1 & 55.8 & 33.8 & 56.5 & 67.0 \\\\\nYOLOv10-B & 52.5 & 69.6 & 57.2 & 35.1 & 57.8 & 68.5 \\\\\nYOLOv10-L & 53.2 & 70.1 & 58.1 & 35.8 & 58.5 & 69.4 \\\\\nYOLOv10-X & 54.4 & 71.3 & 59.3 & 37.0 & 59.8 & 70.9 \\\\\n \n\\end{tabular}\n0.05 ms latency overhead. Besides, for YOLOv10-M (\\#6 in Tab. 2), which has a larger model scale and more redundancy, our efficiency driven model design results in a considerable $12.5 \\%$ latency reduction, as shown in Tab. 2. When combined with accuracy driven model design, we observe a notable $0.8 \\%$ AP improvement for YOLOv10-M, along with a favorable latency reduction of 0.48 ms . These results well demonstrate the effectiveness of our design strategy across different model scales.\n\n\nA. 6 Visualization Results\n\n\nFig. 4 presents the visualization results of our YOLOv10 in the complex and challenging scenarios. It can be observed that YOLOv10 can achieve precise detection under various difficult conditions, such as low light, rotation, etc. It also demonstrates a strong capability in detecting diverse and densely packed objects, such as bottle, cup, and person. These results indicate its superior performance.\n\nA. 7 Contribution, Limitation, and Broader Impact\n\nContribution. In summary, our contributions are three folds as follows:\n1. We present a novel consistent dual assignments strategy for NMS-free YOLOs. A dual label assignments way is designed to provide rich supervision by one-to-many branch during training and high efficiency by one-to-one branch during inference. Besides, to ensure the harmonious supervision between two branches, we innovatively propose the consistent matching metric, which can well reduce the theoretical supervision gap and lead to improved performance.\n2. We propose a holistic efficiency-accuracy driven model design strategy for the model architecture of YOLOs. We present novel lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, which greatly reduce the computational redundancy and achieve high efficiency. We further introduce the large-kernel convolution and innovative partial self-attention module, which effectively enhance the performance under low cost.\n3. Based on the above approaches, we introduce YOLOv10, a new real-time end-to-end object detector. Extensive experiments demonstrate that our YOLOv10 achieves the state-of-the-art performance and efficiency trade-offs compared with other advanced detectors.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We present novel lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, which greatly reduce the computational redundancy and achieve high efficiency.",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08f2bc96-d0a7-46b0-9c3e-5d9b0640151e",
        "questions": "Which paper discusses the use of the YOLO algorithm for mobile robot navigation using RGBD images?",
        "answers": "Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm",
        "context": "References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.\n[3] Daniel Bogdoll, Maximilian Nitsche, and J Marius Z\u00f6llner. Anomaly detection in autonomous driving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4488-4499, 2022.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213-229. Springer, 2020.\n[5] Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end fully convolutional object detection. arXiv preprint arXiv:2211.13859, 2022.\n[6] Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing your trained detrs with box refinement. arXiv preprint arXiv:2307.11828, 2023.\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. Yolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480, 2023.\n[8] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251-1258, 2017.\n[9] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11963-11975, 2022.\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733-13742, 2021.\n[11] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290-1305, 2019.\n[12] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6569-6578, 2019.\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021.\n[14] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned one-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3490-3499. IEEE Computer Society, 2021.\n[15] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank diminishing in deep neural networks. Advances in Neural Information Processing Systems, $35: 33054-33065,2022$.\n[16] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\n[17] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2918-2928, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290-1305, 2019.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "08ffe28c-5926-4d4d-8c75-c775c02ff5f2",
        "questions": "In which year was the paper 'Repvgg: Making vgg-style convnets great again' presented at the IEEE/CVF conference on computer vision and pattern recognition?",
        "answers": "2021",
        "context": "References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.\n[3] Daniel Bogdoll, Maximilian Nitsche, and J Marius Z\u00f6llner. Anomaly detection in autonomous driving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4488-4499, 2022.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213-229. Springer, 2020.\n[5] Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end fully convolutional object detection. arXiv preprint arXiv:2211.13859, 2022.\n[6] Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing your trained detrs with box refinement. arXiv preprint arXiv:2307.11828, 2023.\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. Yolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480, 2023.\n[8] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251-1258, 2017.\n[9] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11963-11975, 2022.\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733-13742, 2021.\n[11] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290-1305, 2019.\n[12] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6569-6578, 2019.\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021.\n[14] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned one-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3490-3499. IEEE Computer Society, 2021.\n[15] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank diminishing in deep neural networks. Advances in Neural Information Processing Systems, $35: 33054-33065,2022$.\n[16] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\n[17] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2918-2928, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733-13742, 2021.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0907ca13-1511-43f9-846e-1738790b1990",
        "questions": "Does the paper 'Yolo-ms: rethinking multi-scale representation learning for real-time object detection' appear in the references list with a publication year of 2023?",
        "answers": "Yes",
        "context": "References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.\n[3] Daniel Bogdoll, Maximilian Nitsche, and J Marius Z\u00f6llner. Anomaly detection in autonomous driving: A survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4488-4499, 2022.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213-229. Springer, 2020.\n[5] Yiqun Chen, Qiang Chen, Qinghao Hu, and Jian Cheng. Date: Dual assignment for end-to-end fully convolutional object detection. arXiv preprint arXiv:2211.13859, 2022.\n[6] Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, and Jian Cheng. Enhancing your trained detrs with box refinement. arXiv preprint arXiv:2307.11828, 2023.\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. Yolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480, 2023.\n[8] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1251-1258, 2017.\n[9] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11963-11975, 2022.\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733-13742, 2021.\n[11] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):1290-1305, 2019.\n[12] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6569-6578, 2019.\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021.\n[14] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned one-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3490-3499. IEEE Computer Society, 2021.\n[15] Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank diminishing in deep neural networks. Advances in Neural Information Processing Systems, $35: 33054-33065,2022$.\n[16] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\n[17] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2918-2928, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin Hou, and Ming-Ming Cheng. Yolo-ms: rethinking multi-scale representation learning for real-time object detection. arXiv preprint arXiv:2308.05480, 2023.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0909d045-4cde-4dae-b6a0-17da86a284e8",
        "questions": "What is the latency in milliseconds for the YOLOv10-S model with NMS-free and efficiency features enabled?",
        "answers": "2.31",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "090d126a-af7e-41c1-bde6-0d956338d2c6",
        "questions": "What is the improvement in AP percentage achieved by the spatial-channel decoupled downsampling strategy compared to the baseline method in YOLOv10-S?",
        "answers": "0.7%",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09122343-1611-4688-a843-acb103a17a29",
        "questions": "Does the consistent matching metric eliminate the need for exhaustive hyper-parameter tuning in the YOLOv8-S model?",
        "answers": "Yes",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09176692-6bc8-40e3-af9c-64ef9806ba8f",
        "questions": "What is the Latency (in ms) for the YOLOv10-S model that has both NMS-free and Efficiency features enabled but not Accuracy, and how does it compare to when all three features are enabled?",
        "answers": "2.31 ms vs 2.49 ms",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Comparison",
        "evidence_source": "table",
        "evidence_context": "3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\ 4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "091c5153-2564-4c18-a192-083fd999d865",
        "questions": "How many parameters (in millions) does YOLOv10-M have when both NMS-free and Efficiency features are enabled?",
        "answers": "14.1",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "091d5dc7-e584-42a4-bfbb-6e84b04e67ee",
        "questions": "For the baseline model in Table 3, what is the $\\mathrm{AP}^{\text {val}}$ and Latency when training with only the one-to-one (o2o) branch?",
        "answers": "44.9 AP and 7.07 ms latency",
        "context": "Table 2: Ablation study with YOLOv10-S and YOLOv10-M on COCO.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\# & Model & NMS-free. & Efficiency. & Accuracy. & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}$ (\\%) & Latency(ms) \\\\\n  1 & & & & & 11.2 & 28.6 & 44.9 & 7.07 \\\\\n  2 & & $\\checkmark$ & & & 11.2 & 28.6 & 44.3 & 2.44 \\\\\n  3 & YOLOv10-S & $\\checkmark$ & $\\checkmark$ & & 6.2 & 20.8 & 44.5 & 2.31 \\\\\n  4 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 7.2 & 21.6 & 46.3 & 2.49 \\\\\n  5 & & & & & 25.9 & 78.9 & 50.6 & 9.50 \\\\\n  6 & & & & & 25.9 & 78.9 & 50.3 & 5.22 \\\\\n  7 & YOLOV10-M & $\\checkmark$ & $\\checkmark$ & & 14.1 & 58.1 & 50.4 & 4.57 \\\\\n  8 & & $\\checkmark$ & $\\checkmark$ & $\\checkmark$ & 15.4 & 59.1 & 51.1 & 4.74 \\\\\n \n\\end{tabular}\n\nTable 3: Dual assign. Table 4: Matching metric. Table 5: Efficiency. for YOLOv10-S/M.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\\alpha_{o 2 o} \\beta_{o 2 o} \\mathrm{AP}^{\\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\\\\n  $\\checkmark$ & 43.4 & 2.44 & 0.5 & 4.0 & 44.2 & 0.25 & 6.0 & 43.5 & & +cls. & 9.9/23.2 & 23.5/67.7 & 44.2/50.2 & 2.39/5.07 \\\\\n  $\\checkmark$ \u6dc2 & 44.3 & 2.44 & 0.5 & 6.0 & 44.3 & & 6.0 & 43.9 & & +downs. & 8.0/19.7 & 22.2/65.0 & 44.4/50.4 & 2.36/4.97 \\\\\n  & & & 0.5 & 8.0 & 44.0 & 1.0 & 12.0 & 44.3 & & +block. & 6.2/14.1 & 20.8/58.1 & 44.5/50.4 & 2.31/4.57 \\\\\n \n\\end{tabular}\n\n\nAnalyses for NMS-free training.\n\n- Dual label assignments. We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many ( 22 m ) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, i.e., $\\# 1$ in Tab. 2. Specifically, we introduce baselines for training with only o 2 m branch and only o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.\n- Consistent matching metric. We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, i.e., \\#1 in Tab. 2, under different $\\alpha_{o 2 o}$ and $\\beta_{o 2 o}$. As shown in Tab. 4, the proposed consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$, can achieve the optimal performance, where $\\alpha_{o 2 m}=0.5$ and $\\beta_{o 2 m}=6.0$ in the one-to-many head [20]. Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.\nAnalyses for efficiency driven model design. We conduct experiments to gradually incorporate the efficiency driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model without efficiency-accuracy driven model design, i.e., \\#2/\\#6 in Tab. 2. As shown in Tab. 5, each design component, including lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, contributes to the reduction of parameters count, FLOPs, and latency. Importantly, these improvements are achieved while maintaining competitive performance.\n- Lightweight classification head. We analyze the impact of category and localization errors of predictions on the performance, based on the YOLOv10-S of \\#1 and \\#2 in Tab. 5, like [6]. Specifically, we match the predictions to the instances by the one-to-one assignment. Then, we substitute the predicted category score with instance labels, resulting in $\\mathrm{AP}_{w / o c}^{v a l}$ with no classification errors. Similarly, we replace the predicted locations with those of instances, yielding $\\mathrm{AP}_{w / o r}^{v a l}$ with no regression errors. As shown in Tab. 6, $\\mathrm{AP}_{w / o r}^{v a l}$ is much higher than $\\mathrm{AP}_{w / o}^{v a l}$, revealing that eliminating the regression errors achieves greater improvement. The performance bottleneck thus lies more in the regression task. Therefore, adopting the lightweight classification head can allow higher efficiency without compromising the performance.\n- Spatial-channel decoupled downsampling. We decouple the downsampling operations for efficiency, where the channel dimensions are first increased by pointwise convolution (PW) and the resolution is then reduced by depthwise convolution (DW) for maximal information retention. We compare it with the baseline way of spatial reduction by DW followed by channel modulation by PW, based on the YOLOv10-S of \\#3 in Tab. 5. As shown in Tab. 7, our downsampling strategy achieves the $0.7 \\% \\mathrm{AP}$ improvement by enjoying less information loss during downsampling.\n- Compact inverted block (CIB). We introduce CIB as the compact basic building block. We verify its effectiveness based on the YOLOv10-S of \\#4 in the Tab. 5. Specifically, we introduce the inverted residual block [46] (IRB) as the baseline, which achieves the suboptimal $43.7 \\% \\mathrm{AP}$, as shown in Tab. 8. We then append a $3 \\times 3$ depthwise convolution (DW) after it, denoted as \"IRB-DW\", which",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "o2m o2o & AP & Latency & \\multicolumn{3}{|l|}{$\u0007lpha_{o 2 o} \beta_{o 2 o} \\mathrm{AP}^{v a l}$} & \\multicolumn{3}{|l|}{$\u0007lpha_{o 2 o} \beta_{o 2 o} \\mathrm{AP}^{\text {val }}$} & & \\# Model & \\#Param & FLOPs & $\\mathrm{AP}^{\text {val }}$ & Latency \\ $\\checkmark$ & 44.9 & 7.07 & 0.5 & 2.0 & 42.7 & 0.25 & 3.0 & 44.3 & & base. & 11.2/25.9 & 28.6/78.9 & 44.3/50.3 & 2.44/5.22 \\",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "091eca72-0278-4963-b9be-384524584b39",
        "questions": "What is the final learning rate used for training YOLOv10 models?",
        "answers": "0.0001",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "final learning rate & $10^{-4}$",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "092770d2-86c5-4c45-b9dd-6e7597a03443",
        "questions": "How many warm-up epochs are employed while training YOLOv10 models?",
        "answers": "3",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "warm-up epochs & 3",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0934fe22-440c-4ed5-81a8-ccc35c33da30",
        "questions": "What is the translation augmentation value used for YOLOv10 models?",
        "answers": "0.1",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "translation augmentation & 0.1",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09352795-6ef0-4999-bae8-cb671361eca3",
        "questions": "What is the percentage decrease in latency achieved by YOLOv10-B compared to YOLOv9-C for the same performance?",
        "answers": "46%",
        "context": "YOLOv10: Real-Time End-to-End Object Detection\n\n\n\nAo Wang Hui Chen* Lihao Liu Kai Chen Zijia Lin Jungong Han Guiguang Ding* \\\\ Tsinghua University\n\n\n\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs. We measure the end-to-end latency using the official pre-trained models.\n\n\nOver the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is $1.8 \\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying $2.8 \\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has $46 \\%$ less latency and $25 \\%$ fewer parameters for the same performance. Code: https://github.com/THU-MIG/yolov10.\n\n\n\\footnotetext{\n*Corresponding Author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Compared with YOLOv9-C, YOLOv10-B has $46 \\%$ less latency and $25 \\%$ fewer parameters for the same performance.",
        "evidence_page_no": 0,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "093585a3-96ce-4513-8d60-1dd36c4b30d2",
        "questions": "How much faster is YOLOv10-S compared to RT-DETR-R18 under similar Average Precision (AP) on COCO?",
        "answers": "1.8 times",
        "context": "YOLOv10: Real-Time End-to-End Object Detection\n\n\n\nAo Wang Hui Chen* Lihao Liu Kai Chen Zijia Lin Jungong Han Guiguang Ding* \\\\ Tsinghua University\n\n\n\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs. We measure the end-to-end latency using the official pre-trained models.\n\n\nOver the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is $1.8 \\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying $2.8 \\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has $46 \\%$ less latency and $25 \\%$ fewer parameters for the same performance. Code: https://github.com/THU-MIG/yolov10.\n\n\n\\footnotetext{\n*Corresponding Author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "For example, our YOLOv10-S is $1.8 \times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying $2.8 \times$ smaller number of parameters and FLOPs.",
        "evidence_page_no": 0,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "093c656e-b5fa-419e-8c38-38a24b44b4f9",
        "questions": "What percentage fewer parameters does YOLOv10-B have in comparison to YOLOv9-C for the same performance?",
        "answers": "25%",
        "context": "YOLOv10: Real-Time End-to-End Object Detection\n\n\n\nAo Wang Hui Chen* Lihao Liu Kai Chen Zijia Lin Jungong Han Guiguang Ding* \\\\ Tsinghua University\n\n\n\nFigure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs. We measure the end-to-end latency using the official pre-trained models.\n\n\nOver the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is $1.8 \\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying $2.8 \\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has $46 \\%$ less latency and $25 \\%$ fewer parameters for the same performance. Code: https://github.com/THU-MIG/yolov10.\n\n\n\\footnotetext{\n*Corresponding Author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Compared with YOLOv9-C, YOLOv10-B has $46 \\%$ less latency and $25 \\%$ fewer parameters for the same performance.",
        "evidence_page_no": 0,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09449a97-fa7a-44b0-b4d1-630a3268e92b",
        "questions": "What is the latency of the YOLOv10-X model, and how does it compare to RT-DETR-R101 in milliseconds?",
        "answers": "The latency of YOLOv10-X is 10.70 ms, while the latency of RT-DETR-R101 is 13.71 ms.",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "094e8d7b-68d6-49ec-af0c-09a520632407",
        "questions": "Which model has the lowest number of parameters between YOLOv8-N and YOLOv10-N, and what are their respective parameter counts in millions?",
        "answers": "YOLOv10-N has the lowest number of parameters with 2.3 million compared to YOLOv8-N which has 3.2 million.",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0956b326-6af9-47d5-b619-2485c72b9b25",
        "questions": "By how much is the AP performance of YOLOv10-L (Ours) higher than Gold-YOLO-L, and what is the latency difference between them?",
        "answers": "YOLOv10-L has an AP performance of 1.4% higher and a latency difference of 3.37 ms lower compared to Gold-YOLO-L.",
        "context": "Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency ${ }^{f}$ denotes the latency in the forward process of model without post-processing. $\\dagger$ means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  Model & \\#Param.(M) & FLOPs(G) & $\\mathrm{AP}^{\\text {val }}(\\%)$ & Latency(ms) & Latency $^{f}(\\mathrm{~ms})$ \\\\\n  YOLOv6-3.0-N [27] & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\\\\n  Gold-YOLO-N [54] & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\\\\n  YOLOv8-N [20] & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\\\\n  YOLOv10-N (Ours) & 2.3 & 6.7 & $38.5 / 39.5^{\\dagger}$ & 1.84 & 1.79 \\\\\n  YOLOv6-3.0-S [27] & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\\\\n  Gold-YOLO-S [54] & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\\\\n  YOLO-MS-XS [7] & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\\\\n  YOLO-MS-S [7] & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\\\\n  YOLOv8-S [20] & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\\\\n  YOLOv9-S [59] & 7.1 & 26.4 & 46.7 & - & - \\\\\n  RT-DETR-R18 [71] & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\\\\n  YOLOv10-S (Ours) & 7.2 & 21.6 & $46.3 / 46.8^{\\dagger}$ & 2.49 & 2.39 \\\\\n  YOLOv6-3.0-M [27] & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\\\\n  Gold-YOLO-M [54] & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\\\\n  YOLO-MS [7] & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\\\\n  YOLOv8-M [20] & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\\\\n  YOLOv9-M [59] & 20.0 & 76.3 & 51.1 & - & - \\\\\n  RT-DETR-R34 [71] & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\\\\n  RT-DETR-R50m [71] & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\\\\n  YOLOv10-M (Ours) & 15.4 & 59.1 & $51.1 / 51.3^{\\dagger}$ & 4.74 & 4.63 \\\\\n  YOLOv6-3.0-L [27] & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\\\\n  Gold-YOLO-L [54] & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\\\\n  YOLOv9-C [59] & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\\\\n  YOLOv10-B (Ours) & 19.1 & 92.0 & $52.5 / 52.7^{\\dagger}$ & 5.74 & 5.67 \\\\\n  YOLOv8-L [20] & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\\\\n  RT-DETR-R50 [71] & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\\\\n  YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21 \\\\\n  YOLOv8-X [20] & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\\\\n  RT-DETR-R101 [71] & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\\\\n  YOLOv10-X (Ours) & 29.5 & 160.4 & $54.4 / 54.4^{\\dagger}$ & 10.70 & 10.60 \\\\\n \n\\end{tabular}\n\nAP, with $51 \\% / 61 \\%$ fewer parameters and $41 \\% / 52 \\%$ less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46\\% / $62 \\%$ latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows $68 \\%$ fewer parameters and $32 \\%$ lower latency, along with a significant improvement of $1.4 \\%$ AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves $1.8 \\times$ and $1.3 \\times$ faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.\nWe also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency ${ }^{f}$ ) in this situation, following [56, 20, 54]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.\n\n\n4.3 Model Analyses\n\n\nAblation study. We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63 ms , while maintaining competitive performance of $44.3 \\%$ AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFlOPs , with a considerable latency reduction of 0.65 ms for $\\mathrm{YOLOv} 10-\\mathrm{M}$, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for $\\mathrm{YOLOv10-S}$ and $\\mathrm{YOLOv10-M}$, alone with only 0.18 ms and 0.17 ms latency overhead, respectively, which well demonstrates its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "YOLOv10-L (Ours) & 24.4 & 120.3 & $53.2 / 53.4^{\\dagger}$ & 7.28 & 7.21",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09584d18-42a2-48aa-a0ab-508b75f9277c",
        "questions": "What is the Average Precision (AP) value and latency for the 'ours' model according to the Rank-guided block design results?",
        "answers": "44.5 and 2.31 ms",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ours & 44.5 & 2.31",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "095859f7-610a-42d9-84a0-1681277e68dc",
        "questions": "In the table showing stages with CIB, what is the AP value when stages 8 and 4 are utilized?",
        "answers": "44.5",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$8,4, & 44.5",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "095fb252-e683-4156-87f3-11b5b2759630",
        "questions": "Based on Table 7, which model variant achieves the highest AP value without using the reparameterization branch during validation, and what is that value?",
        "answers": "64.5",
        "context": "Table 6: cls. results. Table 7: Results of d.s.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  & base. & + cls. & Model & $\\mathrm{AP}^{\\text {val }}$ & Latency \\\\\n  $\\mathrm{AP}^{\\text {val }}$ & 44.3 & 44.2 & base. & 43.7 & 2.33 \\\\\n  $\\mathrm{AP}_{w / o ~ c}^{v a l}$ & 59.9 & 59.9 & ours & 44.4 & 2.36 \\\\\n  $\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & & \\\\\n \n\\end{tabular}\n\nTable 8: Results of CIB. Table 9: Rank-guided.\n\\begin{tabular}{llc}\n  Model & AP $^{\\text {val }}$ & Latency \\\\\n  IRB & 43.7 & 2.30 \\\\\nIRB-DW & 44.2 & 2.30 \\\\\nours & 44.5 & 2.31 \\\\\n \n\\end{tabular}\n\\begin{tabular}{|c|c|}\n  Stages with CIB & $\\mathrm{AP}^{v a l}$ \\\\\n  empty & 44.4 \\\\\n  8 & 44.5 \\\\\n  8,4, & 44.5 \\\\\n  $8,4,7$ & 44.3 \\\\\n \n\\end{tabular}\n\nTable 10: Accuracy. for S/M. Table 11: L.k. results. Table 12: L.k. usage. Table 13: PSA results.\n\nbrings $0.5 \\% \\mathrm{AP}$ improvement. Compared with \"IRB-DW\", our CIB further achieves $0.3 \\% \\mathrm{AP}$ improvement by prepending another DW with minimal overhead, indicating its superiority.\n- Rank-guided block design. We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of \\#3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7 . In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.\nAnalyses for accuracy driven model design. We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, i.e., \\#3/\\#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of $0.4 \\% \\mathrm{AP}$ and $1.4 \\% \\mathrm{AP}$ for YOLOv10-S under minimal latency increase of 0.03 ms and 0.15 ms , respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).\n- Large-kernel convolution. We first investigate the effect of different kernel sizes based on the YOLOv10-S of \\#2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of $7 \\times 7$, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves $0.1 \\% \\mathrm{AP}$ degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, i.e., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, i.e., YOLOv10-N / S.\n- Partial self-attention (PSA). We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10S of \\#3 in Tab. 10. Specifically, we introduce the transformer block, i.e., MHSA followed by FFN, as the baseline, denoted as \"Trans.\". As shown in Tab. 13, compared with it, PSA brings $0.3 \\%$ AP improvement with 0.05 ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [62, 9] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different $N_{\\text {PSA }}$. As shown in Tab. 13, increasing $N_{\\text {PSA }}$ to 2 obtains $0.2 \\%$ AP improvement but with 0.1 ms latency overhead. Therefore, we set $N_{\\text {PSA }}$ to 1 , by default, to enhance the model capability while maintaining high efficiency.\n\n\n5 Conclusion\n\n\nIn this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMSfree training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency tradeoffs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$\\mathrm{AP}_{w / o r}^{v a l}$ & 64.5 & 64.2 & & &",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09650fe0-d5df-4c49-9942-69b4dd60375a",
        "questions": "What is the formula used to derive the supervision gap between two branches in task alignment learning?",
        "answers": "$A =\\left|\\left(1-t_{o 2 o, i}\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\right)\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash \\{i\\}} t_{o 2 m, k} =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash \\{i\\}} t_{o 2 m, k}$",
        "context": "A Appendix\n\n\nA. 1 Implementation Details\n\nFollowing [20, 56, 59], all YOLOv10 models are trained from scratch using the SGD optimizer for 500 epochs. The SGD momentum and weight decay are set to 0.937 and $5 \\times 10^{-4}$, respectively. The initial learning rate is $1 \\times 10^{-2}$ and it decays linearly to $1 \\times 10^{-4}$. For data augmentation, we adopt the Mosaic [2, 19], Mixup [68] and copy-paste augmentation [17], etc., like [20, 59]. Tab. 14 presents the detailed hyper-parameters. All models are trained on 8 NVIDIA 3090 GPUs. Besides, we increase the width scale factor of YOLOv10-M to 1.0 to obtain YOLOv10-B. For PSA, we employ it after the SPPF module [20] and adopt the expansion factor of 2 for FFN. For CIB, we also adopt the expansion ratio of 2 for the inverted bottleneck block structure. Following [59, 56], we report the standard mean average precision (AP) across different object scales and IoU thresholds on the COCO dataset [33].\nMoreover, we follow [71] to establish the end-to-end speed benchmark. Since the execution time of NMS is affected by the input, we thus measure the latency on the COCO val set, like [71]. We adopt the same NMS hyperparameters used by the detectors during their validation. The TensorRT efficientNMSPlugin is appended for post-processing and the I/O overhead is omitted. We report the average latency across all images.\n\nTable 14: Hyper-parameters of YOLOv10.\n\\begin{tabular}{cc}\n  hyper-parameter & YOLOv10-N/S/M/B/L/X \\\\\n  epochs & 500 \\\\\noptimizer & SGD \\\\\nmomentum & 0.937 \\\\\nweight decay & $5 \\times 10^{-4}$ \\\\\nwarm-up epochs & 3 \\\\\nwarm-up momentum & 0.8 \\\\\nwarm-up bias learning rate & 0.1 \\\\\ninitial learning rate & $10^{-2}$ \\\\\nfinal learning rate & $10^{-4}$ \\\\\nlearning rate schedule & linear decay \\\\\nbox loss gain & 7.5 \\\\\nclass loss gain & 0.5 \\\\\nDFL loss gain & 1.5 \\\\\nHSV saturation augmentation & 0.7 \\\\\nHSV value augmentation & 0.4 \\\\\nHSV hue augmentation & 0.015 \\\\\ntranslation augmentation & 0.1 \\\\\nscale augmentation & $0.5 / 0.5 / 0.9 / 0.9 / 0.9 / 0.9$ \\\\\nmosaic augmentation & 1.0 \\\\\nMixup augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.15 / 0.15$ \\\\\ncopy-paste augmentation & $0.0 / 0.0 / 0.1 / 0.1 / 0.3 / 0.3$ \\\\\nclose mosaic epochs & 10 \\\\\n \n\\end{tabular}\n\nA. 2 Details of Consistent Matching Metric\n\nWe provide the detailed derivation of consistent matching metric here.\nAs mentioned in the paper, we suppose that the one-to-many positive samples is $\\Omega$ and the one-toone branch selects $i$-th prediction. We can then leverage the normalized metric [14] to obtain the classification target for task alignment learning [20,14, 59, 27, 64], i.e., $t_{o 2 m, j}=u^{*} \\cdot \\frac{m_{o 2 m, j}}{m_{o 2 m}^{*}} \\leq u^{*}$ for $j \\in \\Omega$ and $t_{o 2 o, i}=u^{*} \\cdot \\frac{m_{o 2 o, i}}{m_{o 2 o}^{*}}=u^{*}$. We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "We can thus derive the supervision gap between two branches by the 1-Wasserstein distance [41] of the different classification targets, i.e.,\n$$\\begin{aligned}\nA & =\\left|\\left(1-t_{o 2 o, i}\\right)-\\left(1-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right)\\right|+\\sum_{k \\in \\Omega \\backslash\\{i\\}}\\left|1-\\left(1-t_{o 2 m, k}\\right)\\right| \\\n& =\\left|t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\\right|+\\sum_{k \\in \\Omega \\backslash \\{i\\}} t_{o 2 m, k} \\\n& =t_{o 2 o, i}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash \\{i\\}} t_{o 2 m, k}\n\\end{aligned}$$",
        "evidence_page_no": 14,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "096b03e6-21a4-4722-a253-00a248d44211",
        "questions": "What is the value of A when $i \\notin \\Omega$ in the given equation?",
        "answers": "$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}$",
        "context": "where $\\mathbb{I}(\\cdot)$ is the indicator function. We denote the classification targets of the predictions in $\\Omega$ as $\\left\\{\\hat{t}_{1}, \\hat{t}_{2}, \\ldots, \\hat{t}_{|\\Omega|}\\right\\}$ in descending order, with $\\hat{t}_{1} \\geq \\hat{t}_{2} \\geq \\ldots \\geq \\hat{t}_{|\\Omega|}$. We can then replace $t_{o 2 o, i}$ with $u^{*}$ and obtain:\n$$\\begin{aligned}\nA & =u^{*}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =u^{*}+\\sum_{k \\in \\Omega} t_{o 2 m, k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i} \\\\\n& =u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\n\\end{aligned}$$\n\nWe further discuss the supervision gap in two scenarios, i.e.,\n1. Supposing $i \\notin \\Omega$, we can obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}$$\n2. Supposing $i \\in \\Omega$, we denote $t_{o 2 m, i}=\\hat{t}_{n}$ and obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\hat{t}_{n}$$\n\nDue to $\\hat{t}_{n} \\geq 0$, the second case can lead to smaller supervision gap. Besides, we can observe that $A$ decreases as $\\hat{t}_{n}$ increases, indicating that $n$ decreases and the ranking of $i$ within $\\Omega$ improves. Due to $\\hat{t}_{n} \\leq \\hat{t}_{1}, A$ thus achieves the minimum when $\\hat{t}_{n}=\\hat{t}_{1}$, i.e., $i$ is the best positive sample in $\\Omega$ with $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $t_{o 2 m, i}=u^{*} \\cdot \\frac{m_{o 2 m, i}}{m_{o 2 m}^{*}}=u^{*}$.\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching metric. We suppose $\\alpha_{o 2 m}>0$ and $\\beta_{o 2 m}>0$, which are common in [20, 59, 27, 14, 64]. Similarly, we assume $\\alpha_{o 2 o}>0$ and $\\beta_{o 2 o}>0$. We can obtain $r_{1}=\\frac{\\alpha_{o 2 o}}{\\alpha_{o 2 m}}>0$ and $r_{2}=\\frac{\\beta_{o 2 o}}{\\beta_{o 2 m}}>0$, and then derive $m_{o 2 o}$ by\n$$\\begin{aligned}\nm_{o 2 o} & =s \\cdot p^{\\alpha_{o 2 o}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 o}} \\\\\n& =s \\cdot p^{r_{1} \\cdot \\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{r_{2} \\cdot \\beta_{o 2 m}} \\\\\n& =s \\cdot\\left(p^{\\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 m}}\\right)^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}}\n\\end{aligned}$$\n\nTo achieve $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can make $m_{o 2 o}$ monotonically increase with $m_{o 2 m}$ by assigning $\\left(r_{2}-r_{1}\\right)=0$, i.e.,\n$$\\begin{aligned}\nm_{o 2 o} & =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{0 \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}}\n\\end{aligned}$$\n\nSupposing $r_{1}=r_{2}=r$, we can thus derive the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$. By simply taking $r=1$, we obtain $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$.\n\n\nA. 3 Details of Rank-Guided Block Design\n\n\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate the numerical rank of the convolution, we reshape its weight to the shape of ( $C_{o}, K^{2} \\times C_{i}$ ), where $C_{o}$ and $C_{i}$ denote the number of output and input channels, and $K$ means the kernel size, respectively.\n\nA. 4 More Results on COCO\n\nWe report the detailed performance of YOLOv 10 on COCO, including $\\mathrm{AP}_{50}^{v a l}$ and $\\mathrm{AP}_{75}^{v a l}$ at different IoU thresholds, as well as $\\mathrm{AP}_{\\text {small }}^{\\text {val }}, \\mathrm{AP}_{\\text {medium }}^{\\text {val }}$, and $\\mathrm{AP}_{\\text {large }}^{v a l}$ across different scales, in Tab. 15.\n\nA.5 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\n\nWe note that reducing the latency of YOLOv10-S (\\#2 in Tab. 2) is particularly challenging due to its small model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a $5.3 \\%$ reduction in latency without compromising performance. This provides substantial support for the further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off with our holistic efficiency-accuracy driven model design, showing a $2.0 \\%$ AP improvement with only",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Supposing $i \\notin \\Omega$, we can obtain: $$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}$$",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "09748216-8ca5-4731-b124-3f80cfedf631",
        "questions": "How can $m_{o 2 o}$ be expressed in terms of $m_{o 2 m}$ and IoU when the condition $(r_{2}-r_{1})=0$ is applied?",
        "answers": "$m_{o 2 o} =m_{o 2 m}^{r_{1}}$",
        "context": "where $\\mathbb{I}(\\cdot)$ is the indicator function. We denote the classification targets of the predictions in $\\Omega$ as $\\left\\{\\hat{t}_{1}, \\hat{t}_{2}, \\ldots, \\hat{t}_{|\\Omega|}\\right\\}$ in descending order, with $\\hat{t}_{1} \\geq \\hat{t}_{2} \\geq \\ldots \\geq \\hat{t}_{|\\Omega|}$. We can then replace $t_{o 2 o, i}$ with $u^{*}$ and obtain:\n$$\\begin{aligned}\nA & =u^{*}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =u^{*}+\\sum_{k \\in \\Omega} t_{o 2 m, k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i} \\\\\n& =u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\n\\end{aligned}$$\n\nWe further discuss the supervision gap in two scenarios, i.e.,\n1. Supposing $i \\notin \\Omega$, we can obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}$$\n2. Supposing $i \\in \\Omega$, we denote $t_{o 2 m, i}=\\hat{t}_{n}$ and obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\hat{t}_{n}$$\n\nDue to $\\hat{t}_{n} \\geq 0$, the second case can lead to smaller supervision gap. Besides, we can observe that $A$ decreases as $\\hat{t}_{n}$ increases, indicating that $n$ decreases and the ranking of $i$ within $\\Omega$ improves. Due to $\\hat{t}_{n} \\leq \\hat{t}_{1}, A$ thus achieves the minimum when $\\hat{t}_{n}=\\hat{t}_{1}$, i.e., $i$ is the best positive sample in $\\Omega$ with $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $t_{o 2 m, i}=u^{*} \\cdot \\frac{m_{o 2 m, i}}{m_{o 2 m}^{*}}=u^{*}$.\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching metric. We suppose $\\alpha_{o 2 m}>0$ and $\\beta_{o 2 m}>0$, which are common in [20, 59, 27, 14, 64]. Similarly, we assume $\\alpha_{o 2 o}>0$ and $\\beta_{o 2 o}>0$. We can obtain $r_{1}=\\frac{\\alpha_{o 2 o}}{\\alpha_{o 2 m}}>0$ and $r_{2}=\\frac{\\beta_{o 2 o}}{\\beta_{o 2 m}}>0$, and then derive $m_{o 2 o}$ by\n$$\\begin{aligned}\nm_{o 2 o} & =s \\cdot p^{\\alpha_{o 2 o}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 o}} \\\\\n& =s \\cdot p^{r_{1} \\cdot \\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{r_{2} \\cdot \\beta_{o 2 m}} \\\\\n& =s \\cdot\\left(p^{\\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 m}}\\right)^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}}\n\\end{aligned}$$\n\nTo achieve $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can make $m_{o 2 o}$ monotonically increase with $m_{o 2 m}$ by assigning $\\left(r_{2}-r_{1}\\right)=0$, i.e.,\n$$\\begin{aligned}\nm_{o 2 o} & =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{0 \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}}\n\\end{aligned}$$\n\nSupposing $r_{1}=r_{2}=r$, we can thus derive the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$. By simply taking $r=1$, we obtain $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$.\n\n\nA. 3 Details of Rank-Guided Block Design\n\n\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate the numerical rank of the convolution, we reshape its weight to the shape of ( $C_{o}, K^{2} \\times C_{i}$ ), where $C_{o}$ and $C_{i}$ denote the number of output and input channels, and $K$ means the kernel size, respectively.\n\nA. 4 More Results on COCO\n\nWe report the detailed performance of YOLOv 10 on COCO, including $\\mathrm{AP}_{50}^{v a l}$ and $\\mathrm{AP}_{75}^{v a l}$ at different IoU thresholds, as well as $\\mathrm{AP}_{\\text {small }}^{\\text {val }}, \\mathrm{AP}_{\\text {medium }}^{\\text {val }}$, and $\\mathrm{AP}_{\\text {large }}^{v a l}$ across different scales, in Tab. 15.\n\nA.5 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\n\nWe note that reducing the latency of YOLOv10-S (\\#2 in Tab. 2) is particularly challenging due to its small model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a $5.3 \\%$ reduction in latency without compromising performance. This provides substantial support for the further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off with our holistic efficiency-accuracy driven model design, showing a $2.0 \\%$ AP improvement with only",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "To achieve $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can make $m_{o 2 o}$ monotonically increase with $m_{o 2 m}$ by assigning $(r_{2}-r_{1})=0$, i.e., $$m_{o 2 o} =m_{o 2 m}^{r_{1}}$$",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2405.14458v1",
        "ID": "0982e050-9c9a-4e13-951d-43c326cbfc7b",
        "questions": "What is the relationship between $m_{o 2 o}$ and IoU if $r_{1}=r_{2}=r$ and $r=1$?",
        "answers": "$\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$",
        "context": "where $\\mathbb{I}(\\cdot)$ is the indicator function. We denote the classification targets of the predictions in $\\Omega$ as $\\left\\{\\hat{t}_{1}, \\hat{t}_{2}, \\ldots, \\hat{t}_{|\\Omega|}\\right\\}$ in descending order, with $\\hat{t}_{1} \\geq \\hat{t}_{2} \\geq \\ldots \\geq \\hat{t}_{|\\Omega|}$. We can then replace $t_{o 2 o, i}$ with $u^{*}$ and obtain:\n$$\\begin{aligned}\nA & =u^{*}-\\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}+\\sum_{k \\in \\Omega \\backslash\\{i\\}} t_{o 2 m, k} \\\\\n& =u^{*}+\\sum_{k \\in \\Omega} t_{o 2 m, k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i} \\\\\n& =u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\mathbb{I}(i \\in \\Omega) t_{o 2 m, i}\n\\end{aligned}$$\n\nWe further discuss the supervision gap in two scenarios, i.e.,\n1. Supposing $i \\notin \\Omega$, we can obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}$$\n2. Supposing $i \\in \\Omega$, we denote $t_{o 2 m, i}=\\hat{t}_{n}$ and obtain:\n$$A=u^{*}+\\sum_{k=1}^{|\\Omega|} \\hat{t}_{k}-2 \\cdot \\hat{t}_{n}$$\n\nDue to $\\hat{t}_{n} \\geq 0$, the second case can lead to smaller supervision gap. Besides, we can observe that $A$ decreases as $\\hat{t}_{n}$ increases, indicating that $n$ decreases and the ranking of $i$ within $\\Omega$ improves. Due to $\\hat{t}_{n} \\leq \\hat{t}_{1}, A$ thus achieves the minimum when $\\hat{t}_{n}=\\hat{t}_{1}$, i.e., $i$ is the best positive sample in $\\Omega$ with $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $t_{o 2 m, i}=u^{*} \\cdot \\frac{m_{o 2 m, i}}{m_{o 2 m}^{*}}=u^{*}$.\nFurthermore, we prove that we can achieve the minimized supervision gap by the consistent matching metric. We suppose $\\alpha_{o 2 m}>0$ and $\\beta_{o 2 m}>0$, which are common in [20, 59, 27, 14, 64]. Similarly, we assume $\\alpha_{o 2 o}>0$ and $\\beta_{o 2 o}>0$. We can obtain $r_{1}=\\frac{\\alpha_{o 2 o}}{\\alpha_{o 2 m}}>0$ and $r_{2}=\\frac{\\beta_{o 2 o}}{\\beta_{o 2 m}}>0$, and then derive $m_{o 2 o}$ by\n$$\\begin{aligned}\nm_{o 2 o} & =s \\cdot p^{\\alpha_{o 2 o}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 o}} \\\\\n& =s \\cdot p^{r_{1} \\cdot \\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{r_{2} \\cdot \\beta_{o 2 m}} \\\\\n& =s \\cdot\\left(p^{\\alpha_{o 2 m}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\beta_{o 2 m}}\\right)^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{\\left(r_{2}-r_{1}\\right) \\cdot \\beta_{o 2 m}}\n\\end{aligned}$$\n\nTo achieve $m_{o 2 m, i}=m_{o 2 m}^{*}$ and $m_{o 2 o, i}=m_{o 2 o}^{*}$, we can make $m_{o 2 o}$ monotonically increase with $m_{o 2 m}$ by assigning $\\left(r_{2}-r_{1}\\right)=0$, i.e.,\n$$\\begin{aligned}\nm_{o 2 o} & =m_{o 2 m}^{r_{1}} \\cdot \\operatorname{IoU}(\\hat{b}, b)^{0 \\cdot \\beta_{o 2 m}} \\\\\n& =m_{o 2 m}^{r_{1}}\n\\end{aligned}$$\n\nSupposing $r_{1}=r_{2}=r$, we can thus derive the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$. By simply taking $r=1$, we obtain $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$.\n\n\nA. 3 Details of Rank-Guided Block Design\n\n\nWe present the details of the algorithm of rank-guided block design in Algo. 1. Besides, to calculate the numerical rank of the convolution, we reshape its weight to the shape of ( $C_{o}, K^{2} \\times C_{i}$ ), where $C_{o}$ and $C_{i}$ denote the number of output and input channels, and $K$ means the kernel size, respectively.\n\nA. 4 More Results on COCO\n\nWe report the detailed performance of YOLOv 10 on COCO, including $\\mathrm{AP}_{50}^{v a l}$ and $\\mathrm{AP}_{75}^{v a l}$ at different IoU thresholds, as well as $\\mathrm{AP}_{\\text {small }}^{\\text {val }}, \\mathrm{AP}_{\\text {medium }}^{\\text {val }}$, and $\\mathrm{AP}_{\\text {large }}^{v a l}$ across different scales, in Tab. 15.\n\nA.5 More Analyses for Holistic Efficiency-Accuracy Driven Model Design\n\nWe note that reducing the latency of YOLOv10-S (\\#2 in Tab. 2) is particularly challenging due to its small model scale. However, as shown in Tab. 2, our efficiency driven model design still achieves a $5.3 \\%$ reduction in latency without compromising performance. This provides substantial support for the further accuracy driven model design. YOLOv10-S achieves a better latency-accuracy trade-off with our holistic efficiency-accuracy driven model design, showing a $2.0 \\%$ AP improvement with only",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Supposing $r_{1}=r_{2}=r$, we can thus derive the consistent matching metric, i.e., $\\alpha_{o 2 o}=r \\cdot \\alpha_{o 2 m}$ and $\\beta_{o 2 o}=r \\cdot \\beta_{o 2 m}$. By simply taking $r=1$, we obtain $\\alpha_{o 2 o}=\\alpha_{o 2 m}$ and $\\beta_{o 2 o}=\\beta_{o 2 m}$.",
        "evidence_page_no": 15,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0984447f-2c5f-4d55-a777-592e6f599e20",
        "questions": "What is the main purpose of the SELF-RAG framework introduced in the document?",
        "answers": "SELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM's original creativity and versatility.",
        "context": "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\n\nConcurrent RAG work. A few concurrent works ${ }^{2}$ on RAG propose new training or prompting strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best possible model output via fine-grained self-reflection, making it widely applicable and more robust and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our self-reflection mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to generate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition, reflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on human preference alignment during training. Other works use general control tokens to guide LM generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 Self-RAG: LeARning To REtriEVE, GENERate and CRitiQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1. SELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM's original creativity and versatility. Our end-to-end training lets an LM $\\mathcal{M}$ generate text informed by retrieved passages, if needed, and criticize the output by learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval or confirm the output's relevance, support, or completeness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources.\n\n3.1 PRoblem FormalIZATION AND OVERVIEW\n\nFormally, given input $x$, we train $\\mathcal{M}$ to sequentially generate textual outputs $y$ consisting of multiple segments $y=\\left[y_{1}, \\ldots, y_{T}\\right]$, where $y_{t}$ indicates a sequence of tokens for the $t$-th segment. ${ }^{3}$ Generated tokens in $y_{t}$ include text from the original vocabulary as well as the reflection tokens (Table 1).\n\n\\footnotetext{\n${ }^{2}$ All work is arXived within a week of this preprint.\n${ }^{3}$ In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence).\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1. SELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM's original creativity and versatility.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "098d49b6-3edb-4746-a118-32cc4ae40aa0",
        "questions": "How does SELF-RAG differ from common RAG approaches in terms of retrieval?",
        "answers": "SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.",
        "context": "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\n\nConcurrent RAG work. A few concurrent works ${ }^{2}$ on RAG propose new training or prompting strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best possible model output via fine-grained self-reflection, making it widely applicable and more robust and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our self-reflection mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to generate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition, reflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on human preference alignment during training. Other works use general control tokens to guide LM generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 Self-RAG: LeARning To REtriEVE, GENERate and CRitiQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1. SELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM's original creativity and versatility. Our end-to-end training lets an LM $\\mathcal{M}$ generate text informed by retrieved passages, if needed, and criticize the output by learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval or confirm the output's relevance, support, or completeness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources.\n\n3.1 PRoblem FormalIZATION AND OVERVIEW\n\nFormally, given input $x$, we train $\\mathcal{M}$ to sequentially generate textual outputs $y$ consisting of multiple segments $y=\\left[y_{1}, \\ldots, y_{T}\\right]$, where $y_{t}$ indicates a sequence of tokens for the $t$-th segment. ${ }^{3}$ Generated tokens in $y_{t}$ include text from the original vocabulary as well as the reflection tokens (Table 1).\n\n\\footnotetext{\n${ }^{2}$ All work is arXived within a week of this preprint.\n${ }^{3}$ In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence).\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09a024d4-31ce-4830-99ce-734e5998e86e",
        "questions": "Does the SELF-RAG framework rely on external models at inference to filter out irrelevant passages?",
        "answers": "No",
        "context": "of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.\n\nConcurrent RAG work. A few concurrent works ${ }^{2}$ on RAG propose new training or prompting strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best possible model output via fine-grained self-reflection, making it widely applicable and more robust and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our self-reflection mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to generate fine-grained self-reflection and customizable inference.\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition, reflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on human preference alignment during training. Other works use general control tokens to guide LM generation (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension (reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural language feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 Self-RAG: LeARning To REtriEVE, GENERate and CRitiQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1. SELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and self-reflection, without sacrificing LLM's original creativity and versatility. Our end-to-end training lets an LM $\\mathcal{M}$ generate text informed by retrieved passages, if needed, and criticize the output by learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval or confirm the output's relevance, support, or completeness. In contrast, common RAG approaches retrieve passages indiscriminately, without ensuring complete support from cited sources.\n\n3.1 PRoblem FormalIZATION AND OVERVIEW\n\nFormally, given input $x$, we train $\\mathcal{M}$ to sequentially generate textual outputs $y$ consisting of multiple segments $y=\\left[y_{1}, \\ldots, y_{T}\\right]$, where $y_{t}$ indicates a sequence of tokens for the $t$-th segment. ${ }^{3}$ Generated tokens in $y_{t}$ include text from the original vocabulary as well as the reflection tokens (Table 1).\n\n\\footnotetext{\n${ }^{2}$ All work is arXived within a week of this preprint.\n${ }^{3}$ In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any segment unit (i.e., sub-sentence).\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09aedbd0-6d31-4b4a-bf6b-8793b318ad4c",
        "questions": "Who is supported by the IBM Fellowship in the work aimed at improving the factuality of LLM outputs?",
        "answers": "Akari Asai",
        "context": "ETHICAL CONCERNS\n\n\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause numerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous advice). While our method shows significant improvements in terms of performance, factuality, and citation accuracy, it can still generate outputs that are not fully supported by the citations. We hope that explicit self-reflection and fine-grained attribution may help users verify factual errors in the model outputs.\n\nACKNOWLEDGMENTS\n\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan for valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations. Akari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing to train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research Program for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n\nREFERENCES\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations, 2020. URL https: / openreview. net/forum? id=SJgVHkrYDH.\n\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics, 2023b. URL https://aclanthology.org/2023. findings-acl. 225.\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037, 2022. URL https://arxiv.org/abs/2212.08037.\n\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Akari Asai is supported by the IBM Fellowship.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09b3f5f8-de9c-4d73-a3f4-9a4207814183",
        "questions": "Which organization provided computing resources to train and evaluate the language models in the work focused on improving LLM outputs?",
        "answers": "Stability AI",
        "context": "ETHICAL CONCERNS\n\n\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause numerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous advice). While our method shows significant improvements in terms of performance, factuality, and citation accuracy, it can still generate outputs that are not fully supported by the citations. We hope that explicit self-reflection and fine-grained attribution may help users verify factual errors in the model outputs.\n\nACKNOWLEDGMENTS\n\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan for valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations. Akari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing to train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research Program for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n\nREFERENCES\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations, 2020. URL https: / openreview. net/forum? id=SJgVHkrYDH.\n\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics, 2023b. URL https://aclanthology.org/2023. findings-acl. 225.\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037, 2022. URL https://arxiv.org/abs/2212.08037.\n\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We thank Stability AI for providing computing to train and evaluate the LMs in this work.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09c67417-306a-4871-beef-71cbc915d113",
        "questions": "Was the work on improving the factuality of LLM outputs funded by the DARPA MCS program?",
        "answers": "Yes",
        "context": "ETHICAL CONCERNS\n\n\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause numerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous advice). While our method shows significant improvements in terms of performance, factuality, and citation accuracy, it can still generate outputs that are not fully supported by the citations. We hope that explicit self-reflection and fine-grained attribution may help users verify factual errors in the model outputs.\n\nACKNOWLEDGMENTS\n\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan for valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations. Akari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing to train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research Program for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n\nREFERENCES\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In International Conference on Learning Representations, 2020. URL https: / openreview. net/forum? id=SJgVHkrYDH.\n\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics, 2023b. URL https://aclanthology.org/2023. findings-acl. 225.\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037, 2022. URL https://arxiv.org/abs/2212.08037.\n\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009, 2023. URL https://arxiv.org/abs/2307.09009.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx.\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. URL https://arxiv.org/abs/2309.11495.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09ce689b-f968-4434-ad21-715531ef5d1e",
        "questions": "What is the percentage agreement between manual assessments and GPT-4 predictions for relevance in the document discussing reflection tokens?",
        "answers": "95%",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ).",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09cef0ea-d6ca-4f41-adf1-fe3fefd9373e",
        "questions": "How many instances were collected for the 'IsSuP' aspect in the GPT-4-based data collections described in the document?",
        "answers": "11,181",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09d1887e-5a23-42f2-a62a-cd409067f726",
        "questions": "What is the main purpose of the 'Retrieval-on-demand' reflection token in the context of the document discussing reflection tokens?",
        "answers": "An LM determines whether the continuation requires factual grounding.",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding.",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09d1a23b-41ed-4ad7-b799-e29648d44935",
        "questions": "What is the output of the ISREL reflection token in SELF-RAG?",
        "answers": "{relevant, irrelevant}",
        "context": "\\begin{tabular}{|c|c|c|c|}\n  Type & Input & Output & Definitions \\\\\n  Retrieve & $x / x, y$ & \\{yes, no, continue \\} & Decides when to retrieve with $\\mathcal{R}$ \\\\\n  $\\$ ISREL & $x, d$ & \\{relevant, irrelevant\\} & $d$ provides useful information to solve $x$. \\\\\n  ISSuP & $x, d, y$ & \\{fully supported, partially supported, no support\\} & All of the verification-worthy statement in $y$ is supported by $d$. \\\\\n  ISUsE & $x, y$ & $\\{5,4,3,2,1\\}$ & $y$ is a useful response to $x$. \\\\\n \n\\end{tabular}\n\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates the most desirable critique tokens. $x, y, d$ indicate input, output, and a relevant passage, respectively.\n```\nAlgorithm 1 SELF-RAG Inference\nRequire: Generator LM $\\mathcal{M}$, Retriever $\\mathcal{R}$, Large-scale passage collections $\\left\\{d_{1}, \\ldots, d_{N}\\right\\}$\n    Input: input prompt $x$ and preceding generation $y_{<t}$, Output: next output segment $y_{t}$\n    $\\mathcal{M}$ predicts Retrieve given $\\left(x, y_{<t}\\right)$\n    if Retrieve $==$ Yes then\n        Retrieve relevant text passages $\\mathbf{D}$ using $\\mathcal{R}$ given $\\left(x, y_{t-1}\\right) \\quad \\triangleright$ Retrieve\n        $\\mathcal{M}$ predicts ISREL given $x, d$ and $y_{t}$ given $x, d, y_{<t}$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Generate\n        $\\mathcal{M}$ predicts IISSP and IsUsE given $x, y_{t}, d$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Critique\n        Rank $y_{t}$ based on ISREL, IsSuP, ISUse $\\triangleright$ Detailed in Section 3.3\n    else if Retrieve $=\\mathrm{N} \\circ$ then\n        $\\mathcal{M}_{\\text {gen }}$ predicts $y_{t}$ given $x \\quad \\triangleright$ Generate\n        $\\mathcal{M}_{\\text {gen }}$ predicts IsUsE given $x, y_{t} \\quad \\triangleright$ Critique\n```\n\nInference overview. Figure 1 and Algorithm 1 present an overview of Self-RAG at inference. For every $x$ and preceding generation $y_{<t}$, the model decodes a retrieval token to evaluate the utility of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage's relevance, the next response segment, and a critique token to evaluate if the information in the response segment is supported by the passage. Finally, a new critique token evaluates the overall utility of the response. ${ }^{4}$ To generate each segment, SELF-RAG processes multiple passages in parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages $d_{1}$ is selected at the first time step since $d_{2}$ does not provide direct evidence ( $\\square$ IsREL is Irrelevant) and $d_{3}$ output is only partially supported while $d_{1}$ are fully supported.\n\nTraining overview. SELf-RAG enables an arbitrary LM to generate text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model $\\mathcal{M}$ on a curated corpus with interleaving passages retrieved by a retriever $\\mathcal{R}$ and reflection tokens predicted by a critic model $\\mathcal{C}$ (summarized in Appendix Algorithm 2). We train $\\mathcal{C}$ to generate reflection tokens for evaluating retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model $(\\mathcal{M})$ using the conventional LM objective (Section 3.2.2) to enable $\\mathcal{M}$ to generate reflection tokens by itself without relying on the critic at inference time.\n\n\n3.2 SELF-RAG TraInING\n\n\nHere, we describe the supervised data collection and training of two models, the critic $\\mathcal{C}$ (Section 3.2.1) and the generator $\\mathcal{M}$ (Section 3.2.2).\n\n3.2.1 Training THE CRItic Model\n\nData collection for critic model. Manual annotation of reflection tokens for each segment is expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n\n\\footnotetext{\n${ }^{4}$ We follow Liu et al. (2023a) in using a \"perceived\" utility value that is independent of retrieved passages.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "$\\$ ISREL & x, d & \\{relevant, irrelevant\\} & d$ provides useful information to solve $x$.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09df8915-dc00-45b5-8dfb-dc9fb5816a50",
        "questions": "In the SELF-RAG inference process, what happens if the retrieval token is not required?",
        "answers": "The model predicts the next output segment, as it does in a standard LM.",
        "context": "\\begin{tabular}{|c|c|c|c|}\n  Type & Input & Output & Definitions \\\\\n  Retrieve & $x / x, y$ & \\{yes, no, continue \\} & Decides when to retrieve with $\\mathcal{R}$ \\\\\n  $\\$ ISREL & $x, d$ & \\{relevant, irrelevant\\} & $d$ provides useful information to solve $x$. \\\\\n  ISSuP & $x, d, y$ & \\{fully supported, partially supported, no support\\} & All of the verification-worthy statement in $y$ is supported by $d$. \\\\\n  ISUsE & $x, y$ & $\\{5,4,3,2,1\\}$ & $y$ is a useful response to $x$. \\\\\n \n\\end{tabular}\n\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates the most desirable critique tokens. $x, y, d$ indicate input, output, and a relevant passage, respectively.\n```\nAlgorithm 1 SELF-RAG Inference\nRequire: Generator LM $\\mathcal{M}$, Retriever $\\mathcal{R}$, Large-scale passage collections $\\left\\{d_{1}, \\ldots, d_{N}\\right\\}$\n    Input: input prompt $x$ and preceding generation $y_{<t}$, Output: next output segment $y_{t}$\n    $\\mathcal{M}$ predicts Retrieve given $\\left(x, y_{<t}\\right)$\n    if Retrieve $==$ Yes then\n        Retrieve relevant text passages $\\mathbf{D}$ using $\\mathcal{R}$ given $\\left(x, y_{t-1}\\right) \\quad \\triangleright$ Retrieve\n        $\\mathcal{M}$ predicts ISREL given $x, d$ and $y_{t}$ given $x, d, y_{<t}$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Generate\n        $\\mathcal{M}$ predicts IISSP and IsUsE given $x, y_{t}, d$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Critique\n        Rank $y_{t}$ based on ISREL, IsSuP, ISUse $\\triangleright$ Detailed in Section 3.3\n    else if Retrieve $=\\mathrm{N} \\circ$ then\n        $\\mathcal{M}_{\\text {gen }}$ predicts $y_{t}$ given $x \\quad \\triangleright$ Generate\n        $\\mathcal{M}_{\\text {gen }}$ predicts IsUsE given $x, y_{t} \\quad \\triangleright$ Critique\n```\n\nInference overview. Figure 1 and Algorithm 1 present an overview of Self-RAG at inference. For every $x$ and preceding generation $y_{<t}$, the model decodes a retrieval token to evaluate the utility of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage's relevance, the next response segment, and a critique token to evaluate if the information in the response segment is supported by the passage. Finally, a new critique token evaluates the overall utility of the response. ${ }^{4}$ To generate each segment, SELF-RAG processes multiple passages in parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages $d_{1}$ is selected at the first time step since $d_{2}$ does not provide direct evidence ( $\\square$ IsREL is Irrelevant) and $d_{3}$ output is only partially supported while $d_{1}$ are fully supported.\n\nTraining overview. SELf-RAG enables an arbitrary LM to generate text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model $\\mathcal{M}$ on a curated corpus with interleaving passages retrieved by a retriever $\\mathcal{R}$ and reflection tokens predicted by a critic model $\\mathcal{C}$ (summarized in Appendix Algorithm 2). We train $\\mathcal{C}$ to generate reflection tokens for evaluating retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model $(\\mathcal{M})$ using the conventional LM objective (Section 3.2.2) to enable $\\mathcal{M}$ to generate reflection tokens by itself without relying on the critic at inference time.\n\n\n3.2 SELF-RAG TraInING\n\n\nHere, we describe the supervised data collection and training of two models, the critic $\\mathcal{C}$ (Section 3.2.1) and the generator $\\mathcal{M}$ (Section 3.2.2).\n\n3.2.1 Training THE CRItic Model\n\nData collection for critic model. Manual annotation of reflection tokens for each segment is expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n\n\\footnotetext{\n${ }^{4}$ We follow Liu et al. (2023a) in using a \"perceived\" utility value that is independent of retrieved passages.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "If retrieval is not required, the model predicts the next output segment, as it does in a standard LM.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09edc0e4-dfb6-46ec-b8a4-c61ca22287fe",
        "questions": "What is the role of the critic model in the training of SELF-RAG?",
        "answers": "The critic model generates reflection tokens for evaluating retrieved passages and the quality of a given task output.",
        "context": "\\begin{tabular}{|c|c|c|c|}\n  Type & Input & Output & Definitions \\\\\n  Retrieve & $x / x, y$ & \\{yes, no, continue \\} & Decides when to retrieve with $\\mathcal{R}$ \\\\\n  $\\$ ISREL & $x, d$ & \\{relevant, irrelevant\\} & $d$ provides useful information to solve $x$. \\\\\n  ISSuP & $x, d, y$ & \\{fully supported, partially supported, no support\\} & All of the verification-worthy statement in $y$ is supported by $d$. \\\\\n  ISUsE & $x, y$ & $\\{5,4,3,2,1\\}$ & $y$ is a useful response to $x$. \\\\\n \n\\end{tabular}\n\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates the most desirable critique tokens. $x, y, d$ indicate input, output, and a relevant passage, respectively.\n```\nAlgorithm 1 SELF-RAG Inference\nRequire: Generator LM $\\mathcal{M}$, Retriever $\\mathcal{R}$, Large-scale passage collections $\\left\\{d_{1}, \\ldots, d_{N}\\right\\}$\n    Input: input prompt $x$ and preceding generation $y_{<t}$, Output: next output segment $y_{t}$\n    $\\mathcal{M}$ predicts Retrieve given $\\left(x, y_{<t}\\right)$\n    if Retrieve $==$ Yes then\n        Retrieve relevant text passages $\\mathbf{D}$ using $\\mathcal{R}$ given $\\left(x, y_{t-1}\\right) \\quad \\triangleright$ Retrieve\n        $\\mathcal{M}$ predicts ISREL given $x, d$ and $y_{t}$ given $x, d, y_{<t}$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Generate\n        $\\mathcal{M}$ predicts IISSP and IsUsE given $x, y_{t}, d$ for each $d \\in \\mathbf{D} \\quad \\triangleright$ Critique\n        Rank $y_{t}$ based on ISREL, IsSuP, ISUse $\\triangleright$ Detailed in Section 3.3\n    else if Retrieve $=\\mathrm{N} \\circ$ then\n        $\\mathcal{M}_{\\text {gen }}$ predicts $y_{t}$ given $x \\quad \\triangleright$ Generate\n        $\\mathcal{M}_{\\text {gen }}$ predicts IsUsE given $x, y_{t} \\quad \\triangleright$ Critique\n```\n\nInference overview. Figure 1 and Algorithm 1 present an overview of Self-RAG at inference. For every $x$ and preceding generation $y_{<t}$, the model decodes a retrieval token to evaluate the utility of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved passage's relevance, the next response segment, and a critique token to evaluate if the information in the response segment is supported by the passage. Finally, a new critique token evaluates the overall utility of the response. ${ }^{4}$ To generate each segment, SELF-RAG processes multiple passages in parallel and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control (Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages $d_{1}$ is selected at the first time step since $d_{2}$ does not provide direct evidence ( $\\square$ IsREL is Irrelevant) and $d_{3}$ output is only partially supported while $d_{1}$ are fully supported.\n\nTraining overview. SELf-RAG enables an arbitrary LM to generate text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary (i.e., the original vocabulary plus reflection tokens). Specifically, we train the generator model $\\mathcal{M}$ on a curated corpus with interleaving passages retrieved by a retriever $\\mathcal{R}$ and reflection tokens predicted by a critic model $\\mathcal{C}$ (summarized in Appendix Algorithm 2). We train $\\mathcal{C}$ to generate reflection tokens for evaluating retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we train the final generator model $(\\mathcal{M})$ using the conventional LM objective (Section 3.2.2) to enable $\\mathcal{M}$ to generate reflection tokens by itself without relying on the critic at inference time.\n\n\n3.2 SELF-RAG TraInING\n\n\nHere, we describe the supervised data collection and training of two models, the critic $\\mathcal{C}$ (Section 3.2.1) and the generator $\\mathcal{M}$ (Section 3.2.2).\n\n3.2.1 Training THE CRItic Model\n\nData collection for critic model. Manual annotation of reflection tokens for each segment is expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n\n\\footnotetext{\n${ }^{4}$ We follow Liu et al. (2023a) in using a \"perceived\" utility value that is independent of retrieved passages.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We train $\\mathcal{C}$ to generate reflection tokens for evaluating retrieved passages and the quality of a given task output (Section 3.2.1).",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09f6048f-e245-4460-8004-884cc167733f",
        "questions": "What is the S & P score for the PopQA dataset according to the human evaluation?",
        "answers": "92.5",
        "context": "(a) PopQA\n\n(b) PubHealth\n\n(c) ASQA (prec)\n\\begin{tabular}{lrr}\n  & Pop & Bio. \\\\\n  S \\& P & 92.5 & 70.0 \\\\\n  ISREL & 95.0 & $-90 . \\overline{0}$ \\\\\n    ISSuP & 90.0 & 85.0 \\\\\n \n\\end{tabular}\n(d) Human evaluation on PopQA and Bio generation.\n\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d) Human analysis on SELF-RAG outputs as well as reflection tokens.\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more fluent, there are often more claims that are not fully supported by citations, consistent with findings by Liu et al. (2023a). Our framework lets practitioners choose and customize models' behaviors at test time by adjusting such parameters without requiring additional training.\n\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers of threshold $\\delta$ (larger $\\delta$ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that the model's retrieval frequencies dramatically change on both datasets. as $\\delta$ varies. On one hand, performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n\nEffects of training data size. We conduct an analysis of how the data scale affects the model's performance. In particular, we randomly sample $5 \\mathrm{k}, 10 \\mathrm{k}, 20 \\mathrm{k}$, and 50 k instances from our original 150k training instances, and fine-tune four SELF-RAG 7\u0432 variants on those subsets. Then, we compare the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SElfRAG trained on the full 150k instances. We also evaluate Figures $4 \\mathrm{a}, 4 \\mathrm{~b}$ and 4 c shows the models' performance trained on different amount of data. Across all datasets, increasing data size often shows upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do not observed such significant improvements on Llama2- $\\mathrm{FT}_{7 \\mathrm{~B}}$ when increasing the training data from 50 k to 150 k . These results also indicate that further expanding the training data of SELF-RAG may lead to further improvements, although in this work we limit our training data size to 150 k .\n\nHuman evaluations. We conduct small human evaluations on Self-RAG outputs, as well as the reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio results. Following Menick et al. (2022), human annotators evaluate $S \\& P$, which indicates whether the model output is plausible (i.e., the output is a reasonable and on-topic response to the question as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to verify the validity of the answer). For S\\&P, we do not consider the instances where SELF-RAG predicts irrelevant or no support. We then ask our annotators whether the model-predicted reflection tokens about ISREL and ISSuP match their inspections (e.g., whether the fully supported output is supported by the cited evidence). Human annotators find SElf-RAG answers are often plausible and supported by relevant passages with higher S\\&P scores on short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find ISREL and ISSuP reflection token predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated examples and explanations on assessments.\n\n\n6 CONCLUSION\n\n\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs through retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on six tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with more parameters or with conventional retrieval-augmented generation approaches.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "S & P & 92.5 & 70.0",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "09f954f8-3b20-4dc1-8668-5be76eb542e7",
        "questions": "How does the training data size affect the performance of the SELF-RAG model on the PopQA and ASQA datasets?",
        "answers": "Increasing data size often shows upward trajectories and the improvements are significantly larger in PopQA and ASQA.",
        "context": "(a) PopQA\n\n(b) PubHealth\n\n(c) ASQA (prec)\n\\begin{tabular}{lrr}\n  & Pop & Bio. \\\\\n  S \\& P & 92.5 & 70.0 \\\\\n  ISREL & 95.0 & $-90 . \\overline{0}$ \\\\\n    ISSuP & 90.0 & 85.0 \\\\\n \n\\end{tabular}\n(d) Human evaluation on PopQA and Bio generation.\n\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d) Human analysis on SELF-RAG outputs as well as reflection tokens.\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more fluent, there are often more claims that are not fully supported by citations, consistent with findings by Liu et al. (2023a). Our framework lets practitioners choose and customize models' behaviors at test time by adjusting such parameters without requiring additional training.\n\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers of threshold $\\delta$ (larger $\\delta$ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that the model's retrieval frequencies dramatically change on both datasets. as $\\delta$ varies. On one hand, performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n\nEffects of training data size. We conduct an analysis of how the data scale affects the model's performance. In particular, we randomly sample $5 \\mathrm{k}, 10 \\mathrm{k}, 20 \\mathrm{k}$, and 50 k instances from our original 150k training instances, and fine-tune four SELF-RAG 7\u0432 variants on those subsets. Then, we compare the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SElfRAG trained on the full 150k instances. We also evaluate Figures $4 \\mathrm{a}, 4 \\mathrm{~b}$ and 4 c shows the models' performance trained on different amount of data. Across all datasets, increasing data size often shows upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do not observed such significant improvements on Llama2- $\\mathrm{FT}_{7 \\mathrm{~B}}$ when increasing the training data from 50 k to 150 k . These results also indicate that further expanding the training data of SELF-RAG may lead to further improvements, although in this work we limit our training data size to 150 k .\n\nHuman evaluations. We conduct small human evaluations on Self-RAG outputs, as well as the reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio results. Following Menick et al. (2022), human annotators evaluate $S \\& P$, which indicates whether the model output is plausible (i.e., the output is a reasonable and on-topic response to the question as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to verify the validity of the answer). For S\\&P, we do not consider the instances where SELF-RAG predicts irrelevant or no support. We then ask our annotators whether the model-predicted reflection tokens about ISREL and ISSuP match their inspections (e.g., whether the fully supported output is supported by the cited evidence). Human annotators find SElf-RAG answers are often plausible and supported by relevant passages with higher S\\&P scores on short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find ISREL and ISSuP reflection token predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated examples and explanations on assessments.\n\n\n6 CONCLUSION\n\n\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs through retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on six tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with more parameters or with conventional retrieval-augmented generation approaches.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Across all datasets, increasing data size often shows upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do not observed such significant improvements on Llama2-FT_{7 B} when increasing the training data from 50 k to 150 k.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a00792f-1475-461e-8cd7-cea507b008f8",
        "questions": "Does the SELF-RAG framework allow practitioners to adjust model behaviors at test time without additional training?",
        "answers": "Yes",
        "context": "(a) PopQA\n\n(b) PubHealth\n\n(c) ASQA (prec)\n\\begin{tabular}{lrr}\n  & Pop & Bio. \\\\\n  S \\& P & 92.5 & 70.0 \\\\\n  ISREL & 95.0 & $-90 . \\overline{0}$ \\\\\n    ISSuP & 90.0 & 85.0 \\\\\n \n\\end{tabular}\n(d) Human evaluation on PopQA and Bio generation.\n\nFigure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d) Human analysis on SELF-RAG outputs as well as reflection tokens.\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more fluent, there are often more claims that are not fully supported by citations, consistent with findings by Liu et al. (2023a). Our framework lets practitioners choose and customize models' behaviors at test time by adjusting such parameters without requiring additional training.\n\nEfficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers of threshold $\\delta$ (larger $\\delta$ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that the model's retrieval frequencies dramatically change on both datasets. as $\\delta$ varies. On one hand, performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n\nEffects of training data size. We conduct an analysis of how the data scale affects the model's performance. In particular, we randomly sample $5 \\mathrm{k}, 10 \\mathrm{k}, 20 \\mathrm{k}$, and 50 k instances from our original 150k training instances, and fine-tune four SELF-RAG 7\u0432 variants on those subsets. Then, we compare the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SElfRAG trained on the full 150k instances. We also evaluate Figures $4 \\mathrm{a}, 4 \\mathrm{~b}$ and 4 c shows the models' performance trained on different amount of data. Across all datasets, increasing data size often shows upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do not observed such significant improvements on Llama2- $\\mathrm{FT}_{7 \\mathrm{~B}}$ when increasing the training data from 50 k to 150 k . These results also indicate that further expanding the training data of SELF-RAG may lead to further improvements, although in this work we limit our training data size to 150 k .\n\nHuman evaluations. We conduct small human evaluations on Self-RAG outputs, as well as the reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio results. Following Menick et al. (2022), human annotators evaluate $S \\& P$, which indicates whether the model output is plausible (i.e., the output is a reasonable and on-topic response to the question as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to verify the validity of the answer). For S\\&P, we do not consider the instances where SELF-RAG predicts irrelevant or no support. We then ask our annotators whether the model-predicted reflection tokens about ISREL and ISSuP match their inspections (e.g., whether the fully supported output is supported by the cited evidence). Human annotators find SElf-RAG answers are often plausible and supported by relevant passages with higher S\\&P scores on short-form PopQA, which is consistent with Menick et al. (2022). Human annotators also find ISREL and ISSuP reflection token predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated examples and explanations on assessments.\n\n\n6 CONCLUSION\n\n\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs through retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate, and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on six tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with more parameters or with conventional retrieval-augmented generation approaches.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Our framework lets practitioners choose and customize models' behaviors at test time by adjusting such parameters without requiring additional training.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a02da60-749d-4601-9f98-377981bd5336",
        "questions": "What is the number of instances in the GPT-4 Alpaca dataset used for training the generator LM?",
        "answers": "26,168",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a06c083-e848-4a50-9524-7332c4f56173",
        "questions": "Which base language model showed the highest reward prediction accuracy for the ISSUP metric?",
        "answers": "Llama2-7B",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Llama2-7B & 93.8 & 93.5 & 80.2 & 73.5",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a091c4a-a6e3-4dc1-a9e1-f6375ab9c945",
        "questions": "Does the critic model predict retrieval is necessary for all instances in the dataset used for training the generator LM?",
        "answers": "No",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "For the instances where the critic predicts Retrieve = Yes, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a0bde93-307e-49b4-b740-79fcca49ae99",
        "questions": "In which year did the Roman Emperor Constantine the Great make Sunday a day of rest from labor?",
        "answers": "321 AD",
        "context": "Input When did sunday became the seventh day of the week (asqa)\nOutput [Retrieve =Yes] <p>Reference [1]<p>[ISREL =Relevant] In 321 AD, the Roman Emperor Constantine the Great made Sunday a day of rest from labor, stating: \"On the venerable Day of the Sun let the magistrates and people residing in cities rest, and let all workshops be closed\".[ ISSUP =Fully Supported] [Retrieve $=$ Yes] $\\langle p>$ Reference [2] $<\\mathrm{p}\\rangle$ [ ISREL =Relevant] The official adoption of Sunday as a day of rest by Constantine the Great in AD 321 made Sunday the seventh day of the week.[2][ IsSuP =Fully Supported][ IsUsE $=5]$\n in 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest from labor, stating: We find a good description of this in sources such as the book Rest Days which states: Early Christian observance of both the spiritual seventh-day sabbath and a Lord\u015b Day assembly is evidenced in Ignatius\u015b letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and the nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The fact that the canon had to be issued at all is an indication that adoption of Constantine's decree of 321 was still not universal ...\n\nInput Tell me a bio about Rory Tapner. (bio generation)\nOutput $<\\mathrm{p}>$ Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as the Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010. [ IsSuP =Contradictory], Reference [2]<p>[ ISREL] =Relevant] Rory Tapner was born on 30 September 1959 in London, England.[ IsSuP =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is UT businessman who, in September $20 \\overline{0} 10$ was appointed CEO of Coutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down as CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017). Rory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL =Relevant] G. Venugopal is a popular playback singer in the Malayalam film industry. [Retrieve=Cont inue] He has sung over 300 songs in Malayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [Retrieve=Yes] <p>Reference [2] $<\\mathrm{p}>\\mathrm{G}$. Venugopal was born on December 10, 1960 in India. [ IsSuP =Fully Supported]\nReference [ $\\overline{1}]^{\\prime} \\mathrm{G}$. Venugopal $\\overline{\\mathrm{G}}$ Venugopal (born $1 \\overline{0} \\overline{\\text { December }} \\overline{1960}$ ) is an Indian playback singer known for his work in Malayalam films. He started his singing career in the film \"Odaruthammaava Aalariyaam\" (1984). Since then he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has won include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and the Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004) third time for the song \"aadedee..aadaadedee\" from the film \"Ullam\".Venugopal'\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Professional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\n\nTable 7: Examples of outputs.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "In 321 AD, the Roman Emperor Constantine the Great made Sunday a day of rest from labor, stating: \"On the venerable Day of the Sun let the magistrates and people residing in cities rest, and let all workshops be closed\".",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a1ef423-c774-4f6f-b3cf-95610dacd533",
        "questions": "What position did Rory Tapner hold at Coutts, a private banking and wealth management company, starting in 2010?",
        "answers": "Chief Executive Officer",
        "context": "Input When did sunday became the seventh day of the week (asqa)\nOutput [Retrieve =Yes] <p>Reference [1]<p>[ISREL =Relevant] In 321 AD, the Roman Emperor Constantine the Great made Sunday a day of rest from labor, stating: \"On the venerable Day of the Sun let the magistrates and people residing in cities rest, and let all workshops be closed\".[ ISSUP =Fully Supported] [Retrieve $=$ Yes] $\\langle p>$ Reference [2] $<\\mathrm{p}\\rangle$ [ ISREL =Relevant] The official adoption of Sunday as a day of rest by Constantine the Great in AD 321 made Sunday the seventh day of the week.[2][ IsSuP =Fully Supported][ IsUsE $=5]$\n in 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest from labor, stating: We find a good description of this in sources such as the book Rest Days which states: Early Christian observance of both the spiritual seventh-day sabbath and a Lord\u015b Day assembly is evidenced in Ignatius\u015b letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and the nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The fact that the canon had to be issued at all is an indication that adoption of Constantine's decree of 321 was still not universal ...\n\nInput Tell me a bio about Rory Tapner. (bio generation)\nOutput $<\\mathrm{p}>$ Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as the Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010. [ IsSuP =Contradictory], Reference [2]<p>[ ISREL] =Relevant] Rory Tapner was born on 30 September 1959 in London, England.[ IsSuP =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is UT businessman who, in September $20 \\overline{0} 10$ was appointed CEO of Coutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down as CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017). Rory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL =Relevant] G. Venugopal is a popular playback singer in the Malayalam film industry. [Retrieve=Cont inue] He has sung over 300 songs in Malayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [Retrieve=Yes] <p>Reference [2] $<\\mathrm{p}>\\mathrm{G}$. Venugopal was born on December 10, 1960 in India. [ IsSuP =Fully Supported]\nReference [ $\\overline{1}]^{\\prime} \\mathrm{G}$. Venugopal $\\overline{\\mathrm{G}}$ Venugopal (born $1 \\overline{0} \\overline{\\text { December }} \\overline{1960}$ ) is an Indian playback singer known for his work in Malayalam films. He started his singing career in the film \"Odaruthammaava Aalariyaam\" (1984). Since then he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has won include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and the Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004) third time for the song \"aadedee..aadaadedee\" from the film \"Ullam\".Venugopal'\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Professional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\n\nTable 7: Examples of outputs.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Rory Tapner is a British businessman who has served as the Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a22b2ce-234e-45fe-bcee-7eb021653fa6",
        "questions": "How many times has G. Venugopal won the Kerala State award for Best Playback Singer?",
        "answers": "Three times",
        "context": "Input When did sunday became the seventh day of the week (asqa)\nOutput [Retrieve =Yes] <p>Reference [1]<p>[ISREL =Relevant] In 321 AD, the Roman Emperor Constantine the Great made Sunday a day of rest from labor, stating: \"On the venerable Day of the Sun let the magistrates and people residing in cities rest, and let all workshops be closed\".[ ISSUP =Fully Supported] [Retrieve $=$ Yes] $\\langle p>$ Reference [2] $<\\mathrm{p}\\rangle$ [ ISREL =Relevant] The official adoption of Sunday as a day of rest by Constantine the Great in AD 321 made Sunday the seventh day of the week.[2][ IsSuP =Fully Supported][ IsUsE $=5]$\n in 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest from labor, stating: We find a good description of this in sources such as the book Rest Days which states: Early Christian observance of both the spiritual seventh-day sabbath and a Lord\u015b Day assembly is evidenced in Ignatius\u015b letter to the Magnesians ca. 110.\nReference [2] Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and the nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The fact that the canon had to be issued at all is an indication that adoption of Constantine's decree of 321 was still not universal ...\n\nInput Tell me a bio about Rory Tapner. (bio generation)\nOutput $<\\mathrm{p}>$ Reference [1]<p>[ ISREL =Relevant] Rory Tapner is a British businessman who has served as the Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010. [ IsSuP =Contradictory], Reference [2]<p>[ ISREL] =Relevant] Rory Tapner was born on 30 September 1959 in London, England.[ IsSuP =Fully Supported]\nReference [1] Rory Tapner Rory Tapner is UT businessman who, in September $20 \\overline{0} 10$ was appointed CEO of Coutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down as CEO when he was replaced in the role by Alison Rose\nReference [2] Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017). Rory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\nInput Tell me a bio about G. Venugopa. (bio generation)\nOutput (ranked 1) [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL =Relevant] G. Venugopal is a popular playback singer in the Malayalam film industry. [Retrieve=Cont inue] He has sung over 300 songs in Malayalam, Tamil, and Hindi movies.[ ISSUP =Partially Supported] [Retrieve=Yes] <p>Reference [2] $<\\mathrm{p}>\\mathrm{G}$. Venugopal was born on December 10, 1960 in India. [ IsSuP =Fully Supported]\nReference [ $\\overline{1}]^{\\prime} \\mathrm{G}$. Venugopal $\\overline{\\mathrm{G}}$ Venugopal (born $1 \\overline{0} \\overline{\\text { December }} \\overline{1960}$ ) is an Indian playback singer known for his work in Malayalam films. He started his singing career in the film \"Odaruthammaava Aalariyaam\" (1984). Since then he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has won include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and the Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004) third time for the song \"aadedee..aadaadedee\" from the film \"Ullam\".Venugopal'\nReference [2] Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Professional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\n\nTable 7: Examples of outputs.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The many awards he has won include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and the Kerala Kaumudi Gallop Poll award (twice).",
        "evidence_page_no": 24,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a24076e-a335-4b03-a152-aaa9cc01bc9b",
        "questions": "Does the instruction 'Give three tips for staying healthy' require retrieval of external documents to generate a better response?",
        "answers": "Yes",
        "context": "Instructions\n\n\nGiven an instruction, please make a judgment on whether finding some external documents from the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or $[N o]$ and write an explanation.\n\nDemonstrations\n\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or some reliable sources to explain the effects of different behaviors on health. So retrieving documents is helpful to improve the response to this query.\n\nInstruction Describe a time when you had to make a difficult decision. Need retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not require one to find some external documents.\n\nInstruction Write a short story in third person narration about a protagonist who has to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external evidence to verify.\n\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a widely known fact, retrieving web documents for this question can still help.\n\nInstruction Find the area of a circle given its radius. Radius $=4$ Need retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents describing a formula, it is unlikely to find a document exactly mentioning the answer.\n\nInstruction Arrange the words in the given sentence to form a grammatically correct sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn't require any external evidence, as it is a simple grammatical question.\n\nInstruction Explain the process of cellular respiration in plants. Need retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is highly likely that we can find a reliable and useful document to support the response.\n\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "There might be some online sources listing three tips for staying healthy or some reliable sources to explain the effects of different behaviors on health. So retrieving documents is helpful to improve the response to this query.",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a31f6e2-ad79-44d6-95cb-74713ed0141c",
        "questions": "What is the explanation for not needing retrieval for the instruction 'Describe a time when you had to make a difficult decision'?",
        "answers": "This instruction is asking about some personal experience and thus it does not require one to find some external documents.",
        "context": "Instructions\n\n\nGiven an instruction, please make a judgment on whether finding some external documents from the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or $[N o]$ and write an explanation.\n\nDemonstrations\n\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or some reliable sources to explain the effects of different behaviors on health. So retrieving documents is helpful to improve the response to this query.\n\nInstruction Describe a time when you had to make a difficult decision. Need retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not require one to find some external documents.\n\nInstruction Write a short story in third person narration about a protagonist who has to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external evidence to verify.\n\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a widely known fact, retrieving web documents for this question can still help.\n\nInstruction Find the area of a circle given its radius. Radius $=4$ Need retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents describing a formula, it is unlikely to find a document exactly mentioning the answer.\n\nInstruction Arrange the words in the given sentence to form a grammatically correct sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn't require any external evidence, as it is a simple grammatical question.\n\nInstruction Explain the process of cellular respiration in plants. Need retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is highly likely that we can find a reliable and useful document to support the response.\n\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "This instruction is asking about some personal experience and thus it does not require one to find some external documents.",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a34886c-802e-4a41-86bc-6b82f5943dda",
        "questions": "Why is retrieval considered helpful for the instruction 'Explain the process of cellular respiration in plants'?",
        "answers": "This instruction asks for a detailed description of a scientific concept, and is highly likely that we can find a reliable and useful document to support the response.",
        "context": "Instructions\n\n\nGiven an instruction, please make a judgment on whether finding some external documents from the web (e.g., Wikipedia) helps to generate a better response. Please answer [Yes] or $[N o]$ and write an explanation.\n\nDemonstrations\n\nInstruction Give three tips for staying healthy.\nNeed retrieval? [Yes]\nExplanation There might be some online sources listing three tips for staying healthy or some reliable sources to explain the effects of different behaviors on health. So retrieving documents is helpful to improve the response to this query.\n\nInstruction Describe a time when you had to make a difficult decision. Need retrieval? [No]\nExplanation This instruction is asking about some personal experience and thus it does not require one to find some external documents.\n\nInstruction Write a short story in third person narration about a protagonist who has to make an important career decision.\nNeed retrieval? [No]\nExplanation This instruction asks us to write a short story, which does not require external evidence to verify.\n\nInstruction What is the capital of France?\nNeed retrieval? [Yes]\nExplanation While the instruction simply asks us to answer the capital of France, which is a widely known fact, retrieving web documents for this question can still help.\n\nInstruction Find the area of a circle given its radius. Radius $=4$ Need retrieval? [No]\nExplanation This is a math question and although we may be able to find some documents describing a formula, it is unlikely to find a document exactly mentioning the answer.\n\nInstruction Arrange the words in the given sentence to form a grammatically correct sentence. quickly the brown fox jumped\nNeed retrieval? [No]\nExplanation This task doesn't require any external evidence, as it is a simple grammatical question.\n\nInstruction Explain the process of cellular respiration in plants. Need retrieval? [Yes]\nExplanation This instruction asks for a detailed description of a scientific concept, and is highly likely that we can find a reliable and useful document to support the response.\n\nTable 8: Instructions and demonstrations for Retrieve aspect given the input only.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "This instruction asks for a detailed description of a scientific concept, and is highly likely that we can find a reliable and useful document to support the response.",
        "evidence_page_no": 25,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a3c2c6f-ef2d-4a38-a6e4-b8df0b6d9611",
        "questions": "What genre does the book 'A Question and Answer Guide to Astronomy' belong to?",
        "answers": "Astronomy and Cosmology",
        "context": "Input What genre is A Question and Answer Guide to Astronomy? (POPQA)\nOutput [Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL $=$ Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ IsSup =Fully Supported][ IsUsE $=5]$\nReference $[\\overline{1}]$ A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is intended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010. It was originally written in French. The content within the book is written using a question and answer format. It contains some 250 questions, which The Science Teacher states each are answered with a concise and well-formulated essay that is informative and readable. The Science Teacher review goes on to state that many of the answers given in the book are little gems of science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a student, and that the answers are informative, well constructed, and thorough. The book covers information about the planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in the Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers, and the Moon illusion.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Supported }} \\overline{\\text { and }} \\overline{\\text { Plausible. }} \\overline{\\text { - }}$.\nCorrectness of ISREL and ISSUP : Both ISREL and ISSUP are correct, as the Reference [1] provides detailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (P OPQA)\nOutput [Retrieve =Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL] $=$ Relevant] The author of The Lie is Sam Harris.[ ISSUP $=$ Fully Supported][ IsUSE $=5$ ]\nReference [ $\\overline{1}] \\bar{L} \\overline{\\text { Ling }} \\overline{\\text { is a }} 20 \\overline{1} 1$ long-form essay book by American author and neuroscience expert Sam Harris. Harris argues that we can radically simplify our lives and improve society by merely telling the truth in situations where others often lie.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible but not }} \\overline{\\text { supported }} \\overline{\\text { bup }}$.\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a different book whose title is similar to The Lie.\n\nInput Tell me a bio about Ronaldinho (Bio)\nOutput [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ [ ISREL =Relevant] Ronaldinho is a retired professional footballer from Brazil.[ ISSuP =Fully Supported][ Retrieve=Continue]He played as an attacking midfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free kicks[ Retrieve =Cont inue]He won two FIFA World Player of the Year awards and a Ballon d'Or. [ IsUse =5]\n simply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder, but was also deployed as a winger. Considered one of the best players of his generation and regarded by many as one of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d'Or. A global icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from free-kicks, as well as his use of tricks, feints, no-look passes and overhead...\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible and }} \\overline{\\text { fully }} \\overline{\\text { supported. }}$\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [Retrieve=Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ IsReL =Relevant] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and producer.[ IsSuP =Fully Supported] Retrieve $=$ Continue]He was born in Moscow, Russia. Retrieve $=$ Continue]He is the president of the Russian Guild of Film Producers. [ IsUse $=5]$\nReference $[\\overline{1}] \\overline{\\text { Sergei Vladimirovich }} \\overline{\\text { Bodrov }}$ (born June $\\overline{28,} \\overline{19} \\overline{48}$ ) is a Russian film director, screenwriter, and producer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\n in Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant and the first sentence is fully supported.\n\nTable 6: Examples of human evaluation results.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology.",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a3db26e-992d-4d24-854f-7963b6e1da18",
        "questions": "Who are the authors of the book 'A Question and Answer Guide to Astronomy'?",
        "answers": "Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy",
        "context": "Input What genre is A Question and Answer Guide to Astronomy? (POPQA)\nOutput [Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL $=$ Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ IsSup =Fully Supported][ IsUsE $=5]$\nReference $[\\overline{1}]$ A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is intended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010. It was originally written in French. The content within the book is written using a question and answer format. It contains some 250 questions, which The Science Teacher states each are answered with a concise and well-formulated essay that is informative and readable. The Science Teacher review goes on to state that many of the answers given in the book are little gems of science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a student, and that the answers are informative, well constructed, and thorough. The book covers information about the planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in the Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers, and the Moon illusion.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Supported }} \\overline{\\text { and }} \\overline{\\text { Plausible. }} \\overline{\\text { - }}$.\nCorrectness of ISREL and ISSUP : Both ISREL and ISSUP are correct, as the Reference [1] provides detailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (P OPQA)\nOutput [Retrieve =Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL] $=$ Relevant] The author of The Lie is Sam Harris.[ ISSUP $=$ Fully Supported][ IsUSE $=5$ ]\nReference [ $\\overline{1}] \\bar{L} \\overline{\\text { Ling }} \\overline{\\text { is a }} 20 \\overline{1} 1$ long-form essay book by American author and neuroscience expert Sam Harris. Harris argues that we can radically simplify our lives and improve society by merely telling the truth in situations where others often lie.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible but not }} \\overline{\\text { supported }} \\overline{\\text { bup }}$.\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a different book whose title is similar to The Lie.\n\nInput Tell me a bio about Ronaldinho (Bio)\nOutput [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ [ ISREL =Relevant] Ronaldinho is a retired professional footballer from Brazil.[ ISSuP =Fully Supported][ Retrieve=Continue]He played as an attacking midfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free kicks[ Retrieve =Cont inue]He won two FIFA World Player of the Year awards and a Ballon d'Or. [ IsUse =5]\n simply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder, but was also deployed as a winger. Considered one of the best players of his generation and regarded by many as one of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d'Or. A global icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from free-kicks, as well as his use of tricks, feints, no-look passes and overhead...\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible and }} \\overline{\\text { fully }} \\overline{\\text { supported. }}$\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [Retrieve=Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ IsReL =Relevant] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and producer.[ IsSuP =Fully Supported] Retrieve $=$ Continue]He was born in Moscow, Russia. Retrieve $=$ Continue]He is the president of the Russian Guild of Film Producers. [ IsUse $=5]$\nReference $[\\overline{1}] \\overline{\\text { Sergei Vladimirovich }} \\overline{\\text { Bodrov }}$ (born June $\\overline{28,} \\overline{19} \\overline{48}$ ) is a Russian film director, screenwriter, and producer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\n in Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant and the first sentence is fully supported.\n\nTable 6: Examples of human evaluation results.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010.",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a4dd54b-4682-4f04-9e7d-0a5fc4ebb04b",
        "questions": "In what year was 'A Question and Answer Guide to Astronomy' published in English by Cambridge University Press?",
        "answers": "2010",
        "context": "Input What genre is A Question and Answer Guide to Astronomy? (POPQA)\nOutput [Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL $=$ Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ IsSup =Fully Supported][ IsUsE $=5]$\nReference $[\\overline{1}]$ A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is intended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010. It was originally written in French. The content within the book is written using a question and answer format. It contains some 250 questions, which The Science Teacher states each are answered with a concise and well-formulated essay that is informative and readable. The Science Teacher review goes on to state that many of the answers given in the book are little gems of science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a student, and that the answers are informative, well constructed, and thorough. The book covers information about the planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in the Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers, and the Moon illusion.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Supported }} \\overline{\\text { and }} \\overline{\\text { Plausible. }} \\overline{\\text { - }}$.\nCorrectness of ISREL and ISSUP : Both ISREL and ISSUP are correct, as the Reference [1] provides detailed information of the book including the genre, and the model output is fully supported by the reference.\nInput Who is the author of The Lie? (P OPQA)\nOutput [Retrieve =Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>[$ ISREL] $=$ Relevant] The author of The Lie is Sam Harris.[ ISSUP $=$ Fully Supported][ IsUSE $=5$ ]\nReference [ $\\overline{1}] \\bar{L} \\overline{\\text { Ling }} \\overline{\\text { is a }} 20 \\overline{1} 1$ long-form essay book by American author and neuroscience expert Sam Harris. Harris argues that we can radically simplify our lives and improve society by merely telling the truth in situations where others often lie.\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible but not }} \\overline{\\text { supported }} \\overline{\\text { bup }}$.\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are incorrect, as the Reference [1] is about a different book whose title is similar to The Lie.\n\nInput Tell me a bio about Ronaldinho (Bio)\nOutput [ Retrieve $=$ Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ [ ISREL =Relevant] Ronaldinho is a retired professional footballer from Brazil.[ ISSuP =Fully Supported][ Retrieve=Continue]He played as an attacking midfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free kicks[ Retrieve =Cont inue]He won two FIFA World Player of the Year awards and a Ballon d'Or. [ IsUse =5]\n simply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder, but was also deployed as a winger. Considered one of the best players of his generation and regarded by many as one of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d'Or. A global icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from free-kicks, as well as his use of tricks, feints, no-look passes and overhead...\n$\\overline{\\mathbf{S \\&}} \\overline{\\mathbf{P}} \\overline{\\text { Plausible and }} \\overline{\\text { fully }} \\overline{\\text { supported. }}$\nCorrectness of ISREL and ISSUP Both ISREL and ISSUP are correct.\nInput Tell me a bio about Sergei Bodrov (Bio)\nOutput [Retrieve=Yes] $<\\mathrm{p}>$ Reference [1] $<\\mathrm{p}>$ IsReL =Relevant] Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and producer.[ IsSuP =Fully Supported] Retrieve $=$ Continue]He was born in Moscow, Russia. Retrieve $=$ Continue]He is the president of the Russian Guild of Film Producers. [ IsUse $=5]$\nReference $[\\overline{1}] \\overline{\\text { Sergei Vladimirovich }} \\overline{\\text { Bodrov }}$ (born June $\\overline{28,} \\overline{19} \\overline{48}$ ) is a Russian film director, screenwriter, and producer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\n in Moscow and there is no description about the Russian Guild of Film Producers.\nCorrectness of ISREL and ISSUP ISREL and ISSUP are correct as the reference document is highly relevant and the first sentence is fully supported.\n\nTable 6: Examples of human evaluation results.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene Roy, and published in English by Cambridge University Press in 2010.",
        "evidence_page_no": 23,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a4ff145-58f8-489d-b848-c24c9f1079df",
        "questions": "What is the purpose of word embeddings in Natural Language Processing according to the document?",
        "answers": "Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies.",
        "context": "Instructions\n\n\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the preceding sentence is given, the output should be the sentence that follows those preceding sentences. Your task is to evaluate if the output is fully supported by the information provided in the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions from the evidence. This is only applicable when the output and part of the evidence are almost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there is major information in the output that is not discussed in the evidence. For example, if an instruction asks about two concepts and the evidence only discusses either of them, it should be considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the evidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the instruction.\n\nMake sure to not use any external information/knowledge to judge whether the output is true or not. Only check whether the output is supported by the evidence, and not whether the output follows the instructions or not.\n\nInstruction Explain the use of word embeddings in Natural Language Processing. Preceding sentences Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension. Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next token predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the evidence mentions all of the applications syntactic parsing, sentiment analysis, next token predictions as well as analogy detection as the applications. Therefore, the score should be [Fully supported].\n\nTable 11: Instructions and demonstrations for IsSuP tokens.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next token predictions as well and analogy detection.",
        "evidence_page_no": 28,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a508313-d91d-45a8-9f0b-2a7076ee73db",
        "questions": "What are some methods mentioned in the document for generating word embeddings in Natural Language Processing?",
        "answers": "Methods include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.",
        "context": "Instructions\n\n\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the preceding sentence is given, the output should be the sentence that follows those preceding sentences. Your task is to evaluate if the output is fully supported by the information provided in the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions from the evidence. This is only applicable when the output and part of the evidence are almost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there is major information in the output that is not discussed in the evidence. For example, if an instruction asks about two concepts and the evidence only discusses either of them, it should be considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the evidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the instruction.\n\nMake sure to not use any external information/knowledge to judge whether the output is true or not. Only check whether the output is supported by the evidence, and not whether the output follows the instructions or not.\n\nInstruction Explain the use of word embeddings in Natural Language Processing. Preceding sentences Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension. Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next token predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the evidence mentions all of the applications syntactic parsing, sentiment analysis, next token predictions as well as analogy detection as the applications. Therefore, the score should be [Fully supported].\n\nTable 11: Instructions and demonstrations for IsSuP tokens.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.",
        "evidence_page_no": 28,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a53381b-2677-49f7-be90-db0bce796b42",
        "questions": "Is the output sentence about the use of word embeddings in Natural Language Processing fully supported by the evidence provided in the document?",
        "answers": "Yes",
        "context": "Instructions\n\n\nYou will receive an instruction, evidence, and output, and optional preceding sentences. If the preceding sentence is given, the output should be the sentence that follows those preceding sentences. Your task is to evaluate if the output is fully supported by the information provided in the evidence.\nUse the following entailment scale to generate a score:\n- [Fully supported] - All information in output is supported by the evidence, or extractions from the evidence. This is only applicable when the output and part of the evidence are almost identical.\n- [Partially supported] - The output is supported by the evidence to some extent, but there is major information in the output that is not discussed in the evidence. For example, if an instruction asks about two concepts and the evidence only discusses either of them, it should be considered a [Partially supported].\n- [No support / Contradictory] - The output completely ignores evidence, is unrelated to the evidence, or contradicts the evidence. This can also happen if the evidence is irrelevant to the instruction.\n\nMake sure to not use any external information/knowledge to judge whether the output is true or not. Only check whether the output is supported by the evidence, and not whether the output follows the instructions or not.\n\nInstruction Explain the use of word embeddings in Natural Language Processing. Preceding sentences Word embeddings are one of the most powerful tools available for Natural Language Processing (NLP). They are mathematical representations of words or phrases in a vector space, allowing similarities between words and the context in which they are used to be measured.\nOutput Word embeddings are useful for tasks such as sentiment analysis, text classification, predicting the next word in a sequence, and understanding synonyms and analogies.\nEvidence Word embedding\nWord embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension. Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis, next token predictions as well and analogy detection.\nScore [Fully supported]\nExplanation The output sentence discusses the application of word embeddings, and the evidence mentions all of the applications syntactic parsing, sentiment analysis, next token predictions as well as analogy detection as the applications. Therefore, the score should be [Fully supported].\n\nTable 11: Instructions and demonstrations for IsSuP tokens.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "The output sentence discusses the application of word embeddings, and the evidence mentions all of the applications syntactic parsing, sentiment analysis, next token predictions as well as analogy detection as the applications.",
        "evidence_page_no": 28,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a59f5dc-395a-4924-ad76-e2117f49992c",
        "questions": "What is the purpose of using reflection tokens in the SELF-RAG model as described in the document?",
        "answers": "Reflection tokens enable the enforcement of hard control and also verification of model outputs easily.",
        "context": "C. 2 Human Evaluation Examples\n\n\nTable 6 shows examples with human evaluations on $\\mathrm{S} \\& \\mathrm{P}$ and correctness of ISREL and ISSUP reflection tokens.\n\nC. 3 Qualitative Examples\n\nTable 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model output to an ASQA question. The first reference states that Emperor Constantine made Sunday a day of rest from labor, and further the second citation supports the fact that the official adoption of Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts Contradictory to the first output as the output says the person has served as the CEO since 2010, while the passage says he stepped down as CEO in 2015. Indicating those factual contradictions as reflection tokens enables to enforcement of hard control and also verification of model outputs easily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially Support to the statement listing the name of the songs, as they were not explicitly mentioned.\n\nD Full LIST OF InStRuctions And DEMONSTRATIONS FOR GPT-4\n\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens. Table 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows the instruction and demonstrations used to collect the three-way output tokens for Retrieve given instruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration and test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations used to collect the three-way output tokens for IsReL. Table 11 shows an instruction and demonstrations used to collect the three-way output tokens for ISREL. Table 12 shows an instruction and demonstrations used to collect the five-way output tokens for IsUse.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Indicating those factual contradictions as reflection tokens enables to enforcement of hard control and also verification of model outputs easily.",
        "evidence_page_no": 20,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a5ee510-fc99-409c-839c-cea17d41213a",
        "questions": "In which year did Emperor Constantine officially adopt Sunday as a day of rest according to the document?",
        "answers": "AD 321",
        "context": "C. 2 Human Evaluation Examples\n\n\nTable 6 shows examples with human evaluations on $\\mathrm{S} \\& \\mathrm{P}$ and correctness of ISREL and ISSUP reflection tokens.\n\nC. 3 Qualitative Examples\n\nTable 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model output to an ASQA question. The first reference states that Emperor Constantine made Sunday a day of rest from labor, and further the second citation supports the fact that the official adoption of Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts Contradictory to the first output as the output says the person has served as the CEO since 2010, while the passage says he stepped down as CEO in 2015. Indicating those factual contradictions as reflection tokens enables to enforcement of hard control and also verification of model outputs easily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially Support to the statement listing the name of the songs, as they were not explicitly mentioned.\n\nD Full LIST OF InStRuctions And DEMONSTRATIONS FOR GPT-4\n\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens. Table 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows the instruction and demonstrations used to collect the three-way output tokens for Retrieve given instruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration and test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations used to collect the three-way output tokens for IsReL. Table 11 shows an instruction and demonstrations used to collect the three-way output tokens for ISREL. Table 12 shows an instruction and demonstrations used to collect the five-way output tokens for IsUse.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The first reference states that Emperor Constantine made Sunday a day of rest from labor, and further the second citation supports the fact that the official adoption of Sunday as a day of rest by Constantine in AD 321.",
        "evidence_page_no": 20,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a73a9a9-df49-4c15-a38b-c512d13d78bc",
        "questions": "Does the SELF-RAG model predict the statement about the CEO's tenure as contradictory?",
        "answers": "Yes",
        "context": "C. 2 Human Evaluation Examples\n\n\nTable 6 shows examples with human evaluations on $\\mathrm{S} \\& \\mathrm{P}$ and correctness of ISREL and ISSUP reflection tokens.\n\nC. 3 Qualitative Examples\n\nTable 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model output to an ASQA question. The first reference states that Emperor Constantine made Sunday a day of rest from labor, and further the second citation supports the fact that the official adoption of Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts Contradictory to the first output as the output says the person has served as the CEO since 2010, while the passage says he stepped down as CEO in 2015. Indicating those factual contradictions as reflection tokens enables to enforcement of hard control and also verification of model outputs easily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially Support to the statement listing the name of the songs, as they were not explicitly mentioned.\n\nD Full LIST OF InStRuctions And DEMONSTRATIONS FOR GPT-4\n\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens. Table 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows the instruction and demonstrations used to collect the three-way output tokens for Retrieve given instruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration and test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations used to collect the three-way output tokens for IsReL. Table 11 shows an instruction and demonstrations used to collect the three-way output tokens for ISREL. Table 12 shows an instruction and demonstrations used to collect the five-way output tokens for IsUse.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "The second example, the model predicts Contradictory to the first output as the output says the person has served as the CEO since 2010, while the passage says he stepped down as CEO in 2015.",
        "evidence_page_no": 20,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a77e6a1-dda6-4272-87a1-cfb21b85e81d",
        "questions": "What is the monthly Wikipedia page view threshold for the long-tail subset of PopQA used in the study by Mallen et al., 2023?",
        "answers": "less than 100",
        "context": "Challenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We aggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\n\nShort-form generations tasks include two open-domain question answering (QA) datasets, PopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need to answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset, consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100 . As the TriviaQA-unfiltered (open) test set is not publicly available, we follow prior work's validation and test split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate performance based on whether gold answers are included in the model generations instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\n\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form QA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al., 2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on MAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. ${ }^{5}$\n\n\n4.2 BASELINES\n\n\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs, Llama ${ }_{7 \\text { \u0432\u0432, } 13 \\text { \u0432 }}$ (Touvron et al., 2023), instruction-tuned models, Alpaca ${ }_{7 \\text { \u0432, } 13 \\text { \u0432 }}$ (Dubois et al., 2023) (our replication based on Llama2); and models trained and reinforced using private data, ChatGPT (Ouyang et al., 2022) and Llama2-chat ${ }_{13 \\mathrm{~B}}$. For instruction-tuned LMs, we use the official system prompt or instruction format used during training if publicly available. We also compare our method to concurrent work, $\\mathrm{CoVE}_{65_{\\text {\u0432 }}}$ (Dhuliawala et al., 2023), which introduces iterative prompt engineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training. The first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output given the query prepended with the top retrieved documents using the same retriever as in our system. It also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the reflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines with LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same augmentation technique above, as well as perplexity.ai, an InstructGPT-based production search system. The second category includes concurrent methods that are trained with retrieved text passages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning data with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023) to pre-train an LM with API calls (e.g., Wikipedia APIs). ${ }^{6}$\n\n4.3 EXPERIMENTAL SETTINGS\n\nTraining data and settings. Our training data consists of diverse instruction-following input-output pairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In total, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as our generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model $\\mathcal{R}$, we use off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten documents for each input. More training details are in the Appendix Section B.1.\n\nInference settings. As a default configuration, we assign the weight terms IsReL, IsSuP , IsUsE values of $1.0,1.0$ and 0.5 , respectively. To encourage frequent retrieval, we set the retrieval threshold to 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed up inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2 . For a token-level generation, we use greedy decoding. By default, we use the top five documents from Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we use additional top five documents retrieved by a web search engine, following Luo et al. (2023); for ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all baselines for a fair comparison.\n\n\\footnotetext{\n${ }^{5}$ https://github.com/princeton-nlp/ALCE\n${ }^{6}$ We report numbers using the results reported in the paper as the implementations are not available.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For PopQA, we use the long-tail subset, consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a828a9a-74f0-487f-9357-827ed6fc07d4",
        "questions": "How many instruction-output pairs are used in the training data for the study involving Llama2 7B and 13B as the generator base LM?",
        "answers": "150k",
        "context": "Challenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We aggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\n\nShort-form generations tasks include two open-domain question answering (QA) datasets, PopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need to answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset, consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100 . As the TriviaQA-unfiltered (open) test set is not publicly available, we follow prior work's validation and test split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate performance based on whether gold answers are included in the model generations instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\n\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form QA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al., 2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on MAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. ${ }^{5}$\n\n\n4.2 BASELINES\n\n\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs, Llama ${ }_{7 \\text { \u0432\u0432, } 13 \\text { \u0432 }}$ (Touvron et al., 2023), instruction-tuned models, Alpaca ${ }_{7 \\text { \u0432, } 13 \\text { \u0432 }}$ (Dubois et al., 2023) (our replication based on Llama2); and models trained and reinforced using private data, ChatGPT (Ouyang et al., 2022) and Llama2-chat ${ }_{13 \\mathrm{~B}}$. For instruction-tuned LMs, we use the official system prompt or instruction format used during training if publicly available. We also compare our method to concurrent work, $\\mathrm{CoVE}_{65_{\\text {\u0432 }}}$ (Dhuliawala et al., 2023), which introduces iterative prompt engineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training. The first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output given the query prepended with the top retrieved documents using the same retriever as in our system. It also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the reflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines with LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same augmentation technique above, as well as perplexity.ai, an InstructGPT-based production search system. The second category includes concurrent methods that are trained with retrieved text passages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning data with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023) to pre-train an LM with API calls (e.g., Wikipedia APIs). ${ }^{6}$\n\n4.3 EXPERIMENTAL SETTINGS\n\nTraining data and settings. Our training data consists of diverse instruction-following input-output pairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In total, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as our generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model $\\mathcal{R}$, we use off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten documents for each input. More training details are in the Appendix Section B.1.\n\nInference settings. As a default configuration, we assign the weight terms IsReL, IsSuP , IsUsE values of $1.0,1.0$ and 0.5 , respectively. To encourage frequent retrieval, we set the retrieval threshold to 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed up inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2 . For a token-level generation, we use greedy decoding. By default, we use the top five documents from Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we use additional top five documents retrieved by a web search engine, following Luo et al. (2023); for ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all baselines for a fair comparison.\n\n\\footnotetext{\n${ }^{5}$ https://github.com/princeton-nlp/ALCE\n${ }^{6}$ We report numbers using the results reported in the paper as the implementations are not available.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "In total, we use 150k instruction-output pairs.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a8f541d-052b-4051-bc24-1ca590425d14",
        "questions": "What is the retrieval threshold set for most tasks in the inference settings of the study, and how does it differ for ALCE due to citation requirements?",
        "answers": "0.2 for most tasks and 0 for ALCE",
        "context": "Challenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We aggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\n\nShort-form generations tasks include two open-domain question answering (QA) datasets, PopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need to answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset, consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100 . As the TriviaQA-unfiltered (open) test set is not publicly available, we follow prior work's validation and test split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate performance based on whether gold answers are included in the model generations instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\n\nLong-form generation tasks include a biography generation task (Min et al., 2023) and a long-form QA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al., 2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on MAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. ${ }^{5}$\n\n\n4.2 BASELINES\n\n\nBaselines without retrievals. We evaluate strong publicly available pre-trained LLMs, Llama ${ }_{7 \\text { \u0432\u0432, } 13 \\text { \u0432 }}$ (Touvron et al., 2023), instruction-tuned models, Alpaca ${ }_{7 \\text { \u0432, } 13 \\text { \u0432 }}$ (Dubois et al., 2023) (our replication based on Llama2); and models trained and reinforced using private data, ChatGPT (Ouyang et al., 2022) and Llama2-chat ${ }_{13 \\mathrm{~B}}$. For instruction-tuned LMs, we use the official system prompt or instruction format used during training if publicly available. We also compare our method to concurrent work, $\\mathrm{CoVE}_{65_{\\text {\u0432 }}}$ (Dhuliawala et al., 2023), which introduces iterative prompt engineering to improve the factuality of LLM generations.\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training. The first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output given the query prepended with the top retrieved documents using the same retriever as in our system. It also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the reflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines with LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same augmentation technique above, as well as perplexity.ai, an InstructGPT-based production search system. The second category includes concurrent methods that are trained with retrieved text passages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning data with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023) to pre-train an LM with API calls (e.g., Wikipedia APIs). ${ }^{6}$\n\n4.3 EXPERIMENTAL SETTINGS\n\nTraining data and settings. Our training data consists of diverse instruction-following input-output pairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and knowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In total, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as our generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model $\\mathcal{R}$, we use off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten documents for each input. More training details are in the Appendix Section B.1.\n\nInference settings. As a default configuration, we assign the weight terms IsReL, IsSuP , IsUsE values of $1.0,1.0$ and 0.5 , respectively. To encourage frequent retrieval, we set the retrieval threshold to 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed up inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2 . For a token-level generation, we use greedy decoding. By default, we use the top five documents from Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we use additional top five documents retrieved by a web search engine, following Luo et al. (2023); for ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all baselines for a fair comparison.\n\n\\footnotetext{\n${ }^{5}$ https://github.com/princeton-nlp/ALCE\n${ }^{6}$ We report numbers using the results reported in the paper as the implementations are not available.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "To encourage frequent retrieval, we set the retrieval threshold to 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a964cb9-a62a-44d0-9047-111af5dcd525",
        "questions": "What is the instruction format for the ARC-C dataset in the zero-shot evaluations?",
        "answers": "Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer with the capitalized alphabet only, without adding any extra phrase or period.",
        "context": "\\begin{tabular}{ll}\n  Dataset & Instruction \\\\\n  ARC-C & \\begin{tabular}{l} \nGiven four answer candidates, A, B, C and D, choose the best answer choice. Please answer \\\\\nwith the capitalized alphabet only, without adding any extra phrase or period.\n\\end{tabular} \\\\\nPubHealth & \\begin{tabular}{l} \nIs the following statement correct or not? Say true if it's correct; otherwise, say false. Don't \\\\\ncapitalize or add periods, just say \"true\" or \"false\".\n\\end{tabular} \\\\\nBis Generation & \\begin{tabular}{l} \nTell me a bio about [Person Name ]\n\\end{tabular} \\\\\nASQA (baseline) \\begin{tabular}{l} \nInstruction: Write an accurate, engaging, and concise answer for the given question using only \\\\\nthe provided search results (some of which might be irrelevant) and cite them properly. Use \\\\\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several \\\\\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each \\\\\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of \\\\\nthe documents.\n\\end{tabular} & \\begin{tabular}{l} \nAnswer the following question. The question may be ambiguous and have multiple correct \\\\\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don't use any task specific instruction and simply use the original questions as input query.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ARC-C & Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer with the capitalized alphabet only, without adding any extra phrase or period.",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a97b6f9-aaf5-4770-a208-1bc46f3454e0",
        "questions": "Does the instruction for the PubHealth dataset require capitalization or punctuation in the response?",
        "answers": "No",
        "context": "\\begin{tabular}{ll}\n  Dataset & Instruction \\\\\n  ARC-C & \\begin{tabular}{l} \nGiven four answer candidates, A, B, C and D, choose the best answer choice. Please answer \\\\\nwith the capitalized alphabet only, without adding any extra phrase or period.\n\\end{tabular} \\\\\nPubHealth & \\begin{tabular}{l} \nIs the following statement correct or not? Say true if it's correct; otherwise, say false. Don't \\\\\ncapitalize or add periods, just say \"true\" or \"false\".\n\\end{tabular} \\\\\nBis Generation & \\begin{tabular}{l} \nTell me a bio about [Person Name ]\n\\end{tabular} \\\\\nASQA (baseline) \\begin{tabular}{l} \nInstruction: Write an accurate, engaging, and concise answer for the given question using only \\\\\nthe provided search results (some of which might be irrelevant) and cite them properly. Use \\\\\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several \\\\\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each \\\\\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of \\\\\nthe documents.\n\\end{tabular} & \\begin{tabular}{l} \nAnswer the following question. The question may be ambiguous and have multiple correct \\\\\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don't use any task specific instruction and simply use the original questions as input query.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "PubHealth & Is the following statement correct or not? Say true if it's correct; otherwise, say false. Don't capitalize or add periods, just say \"true\" or \"false\".",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a9b5b1d-7baa-438d-9c07-f940909bfb6c",
        "questions": "What is the instruction for the ASQA (baseline) dataset regarding the citation of documents in the answer?",
        "answers": "Cite at least one document and at most three documents in each sentence.",
        "context": "\\begin{tabular}{ll}\n  Dataset & Instruction \\\\\n  ARC-C & \\begin{tabular}{l} \nGiven four answer candidates, A, B, C and D, choose the best answer choice. Please answer \\\\\nwith the capitalized alphabet only, without adding any extra phrase or period.\n\\end{tabular} \\\\\nPubHealth & \\begin{tabular}{l} \nIs the following statement correct or not? Say true if it's correct; otherwise, say false. Don't \\\\\ncapitalize or add periods, just say \"true\" or \"false\".\n\\end{tabular} \\\\\nBis Generation & \\begin{tabular}{l} \nTell me a bio about [Person Name ]\n\\end{tabular} \\\\\nASQA (baseline) \\begin{tabular}{l} \nInstruction: Write an accurate, engaging, and concise answer for the given question using only \\\\\nthe provided search results (some of which might be irrelevant) and cite them properly. Use \\\\\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several \\\\\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each \\\\\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of \\\\\nthe documents.\n\\end{tabular} & \\begin{tabular}{l} \nAnswer the following question. The question may be ambiguous and have multiple correct \\\\\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don't use any task specific instruction and simply use the original questions as input query.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence.",
        "evidence_page_no": 22,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0a9ffe96-fdf0-4605-ab7b-2370940ca324",
        "questions": "Who is one of the authors of the paper titled 'Llama 2: Open foundation and fine-tuned chat models'?",
        "answers": "Hugo Touvron",
        "context": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: / / arxiv. org/abs/2307.09288.\n\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. URL https: / arxiv.org/abs/2306.04751.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https: / openreview. net/forum? id=gEZrGCozdqR.\n\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https: //arxiv.org/abs/2306.01693.\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023. URL https://arxiv.org/abs/2305.00633.\n\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/2310. 04408\n\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context, 2023. URL https: / / arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https: //arxiv.org/abs/2305.06311.\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking. arXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2023. URL https: //arxiv.org/abs/2310.04406.\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0aa4e727-04f5-4749-aa3e-e0dd9054a516",
        "questions": "What is the arXiv identifier for the paper authored by Yizhong Wang and others on instruction tuning?",
        "answers": "2306.04751",
        "context": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: / / arxiv. org/abs/2307.09288.\n\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. URL https: / arxiv.org/abs/2306.04751.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https: / openreview. net/forum? id=gEZrGCozdqR.\n\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https: //arxiv.org/abs/2306.01693.\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023. URL https://arxiv.org/abs/2305.00633.\n\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/2310. 04408\n\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context, 2023. URL https: / / arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https: //arxiv.org/abs/2305.06311.\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking. arXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2023. URL https: //arxiv.org/abs/2310.04406.\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0aa99188-77ab-4bd4-93c8-a089a75eff8e",
        "questions": "Is the paper 'Fine-tuning language models from human preferences' published before 2020?",
        "answers": "Yes",
        "context": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https: / / arxiv. org/abs/2307.09288.\n\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. URL https: / arxiv.org/abs/2306.04751.\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https: / openreview. net/forum? id=gEZrGCozdqR.\n\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023. URL https: //arxiv.org/abs/2306.01693.\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023. URL https://arxiv.org/abs/2305.00633.\n\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. URL https://arxiv.org/abs/2310. 04408\n\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context, 2023. URL https: / / arxiv.org/abs/2310.01558.\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311, 2023. URL https: //arxiv.org/abs/2305.06311.\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking. arXiv preprint arXiv:2304.03728, 2023. URL https://arxiv.org/abs/2304.03728.\n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2023. URL https: //arxiv.org/abs/2310.04406.\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https://arxiv.org/abs/1909.08593.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0aab3d43-d6f3-4903-ab28-422234cb41e1",
        "questions": "What is the dataset name that has the highest number of instances and which category does it belong to?",
        "answers": "GPT-4 Alpaca; Instruction-following",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0aacf91c-aa79-4d9b-ad17-e8d81231c521",
        "questions": "Compare the ISSUP performance scores between Llama2-7B and FLAN-3B models. Which model performs better and by how much?",
        "answers": "Llama2-7B performs better by 20.4 percentage points.",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Llama2-7B & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\ FLAN-3B & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ab3e9a0-8bfc-41e8-9768-f0e32d1122ea",
        "questions": "How many datasets listed in the document are sourced from Open-Instruct and instruction-following category?",
        "answers": "Five datasets are sourced from Open-Instruct and belong to the instruction-following category.",
        "context": "```\nAlgorithm 2 SELF-RAG Training\n    Input input-output data $\\mathcal{D}=\\{X, Y\\}$, generator $\\mathcal{M}, \\mathcal{C} \\theta$\n    Initialize $\\mathcal{C}$ with a pre-trained LM\n    Sample data $\\left\\{X^{\\text {sample }}, Y^{\\text {sample }}\\right\\} \\sim\\{X, Y\\} \\quad \\triangleright$ Training Critic LM (Section 3.2.1)\n    for $(x, y) \\in\\left(X^{\\text {sample }}, Y^{\\text {sample }}\\right)$ do $\\triangleright$ Data collections for $\\mathcal{C}$\n        Prompt GPT-4 to collect a reflection token $r$ for $(x, y)$\n        Add $\\{(x, y, r)\\}$ to $\\mathcal{D}_{\\text {critic }}$\n    Update $\\mathcal{C}$ with next token prediction loss $\\quad \\triangleright$ Critic learning; Eq. 1\n    Initialize $\\mathcal{M}$ with a pre-trained LM\n    for $(x, y) \\in(X, Y)$ do\n        $\\triangleright$ Training Generator LM (Section 3.2.2)\n                        $\\Delta$ Data collection for $\\mathcal{M}$ with $\\mathcal{D}_{\\text {critic }}$\n        Run $\\mathcal{C}$ to predict $r$ given $(x, y)$\n        Add $(x, y, r)$ to $\\mathcal{D}_{g e n}$\n    Update $\\mathcal{M}$ on $\\mathcal{D}_{\\text {gen }}$ with next token prediction loss $\\quad \\triangleright$ Generator LM learning; Eq. 2\n```\n\\begin{tabular}{l|ccc}\n  Dataset name & category & Data source & the number of instances \\\\\n  GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\\\\nStanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\\\\nFLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\\\\nShareGPT & Instruction-following & Open-Instruct & 13,406 \\\\\nOpen Assistant 1 & Instruction-following & Open-Instruct & 9,464 \\\\\nWizard of Wikipedia & Knowledge-intensive & KITT & 17,367 \\\\\nNatural Questions & Knowledge-intensive & KILT & 15,535 \\\\\nFEVER & Knowledge-intensive & KILT & 9,966 \\\\\nOpenBoookQA & Knowledge-intensive & HF Dataset & 4,699 \\\\\nArc-Easy & Knowledge-intensive & HF Dataset & 2,147 \\\\\nASQA & Knowledge-intensive & ASQA & 3,897 \\\\\n \n\\end{tabular}\n\nTable 3: The generator LM $\\mathcal{M}$ training data statistics.\n\\begin{tabular}{l|cccc}\n  base LM & Retrieve & ISSUP & ISREL & ISUSE \\\\\n  Llama2-7B & $\\mathbf{9 3 . 8}$ & $\\mathbf{9 3 . 5}$ & 80.2 & $\\mathbf{7 3 . 5}$ \\\\\nFLAN-3B & 85.6 & 73.1 & $\\mathbf{8 2 . 0}$ & 72.1 \\\\\n \n\\end{tabular}\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei et al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final reward predictions. In most aspects, our reward model shows higher than $80 \\%$ accuracy, indicating the powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively lower performance on ISUSE , this is because both models often confuse between the two highest cases (5 and 4 ), where human annotators can also disagree.\n\nDetails of $\\mathcal{M}$ data creation. Here, we provide detailed data creation procedures. Algorithm 3 summarizes the process. Here we set $y_{t}$ to $y$ for simplification. Once we train the critic model, we first run it on input data from the aforementioned datasets, to predict whether retrieval is needed or not. For the instances where the critic predicts Retrieve $=\\mathrm{NO}$, we only predict the IsUsE given input and output. For the instances where the critic predicts Retrieve $=\\mathrm{Yes}$, we first retrieve passages using the input and the entire output as queries, to find passages that are relevant to the entire output. We then split output sentences using Spacy. ${ }^{7}$ For each sentence, we run $\\mathcal{C}$ to predict whether the retrieval is necessary or not, given the input, preceding segments, and the initial retrieved passage. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{NO}$, then do not insert any paragraph at the $t$ th segment. If $\\mathcal{C}$ predicts Retrieve $=\\mathrm{Yes}$, then we use the original input and the $t$ th segment as a retrieval query to find relevant passages for the $t$-th segment. For each retrieved passage, we predict ISREL and ISSUP. If there is any passage and continuation with ISREL =Relevant and ISSUP =Fully Supported/ ISSUP =Partially\n\n\\footnotetext{\n${ }^{7}$ https://spacy.io/\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT-4 Alpaca & Instruction-following & Open-Instruct & 26,168 \\ Stanford Alpaca & Instruction-following & Open-Instruct & 25,153 \\ FLAN-V2 & Instruction-following & Open-Instruct & 17,817 \\ ShareGPT & Instruction-following & Open-Instruct & 13,406 \\ Open Assistant 1 & Instruction-following & Open-Instruct & 9,464",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ac1f2aa-9453-48a3-9d38-bed2c7c854e1",
        "questions": "What are the two types of computer memory based on volatility?",
        "answers": "The two types of computer memory based on volatility are volatile memory and non-volatile memory.",
        "context": "Input Identify the two types of computer memory. (from gpt 4_alpaca)\nOutput [Retrieve]=Yes] $<\\mathrm{p}>$ Computer memory Computer memory - In computing, memory refers to the computer hardware integrated circuits that store information for immediate use in a computer; it is synonymous with the term \"primary storage\". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage; a very common way of doing this is through a memory management technique called \"virtual memory\". An archaic synonym for memory is store. The term \"memory\", meaning \"primary storage\" or \"main memory\", is $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ Primary memory, also known as main memory, is the memory that the computer can directly access and manipulate. [ IsSUP =Fully Supported] Retrieve $=$ Yes] <p>Computer memory-often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory $\\langle p>[$ ISREL $=$ Relevant $]$ Primary memory can be accessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ IsSuP =Partially Supported]...[ [IsUse :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt 4 a lpaca)\nOutput $[$ Retrieve $=\\mathrm{No}]$ The manager realizes that there is an extensive list of tasks that need to be completed in order to make the event a success. [Retrieve $=$ Yes] $<\\mathrm{p}>$ Delegation-Delegation is the assignment of any responsibility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It is one of the core concepts of management leadership. However, the person who delegated the work remains accountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it is a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly done, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too much input, $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ This includes tasks such as creating a guest list, designing and sending invitations, booking a venue, coordinating with vendors, planning the event\u015b program, and monitoring the budget.[ IsSuP =Partially Supported] Retrieve $=\\mathrm{No}$ ] Since it is not possible for the manager to handle everything alone, they must delegate tasks to their team members. Retrieve $=\\mathrm{No}$ ] The manager would then assign each team member specific responsibilities based on their individual skillsets and strengths. [Retrieve $=$ Yes $<<p>$ Joint application design - Subject Matter Experts: These are the business users, the IS professionals, and the outside experts that will be needed for a successful workshop. This group is the backbone of the meeting $<\\mathrm{p}>$ [ IsReL $=$ Irrelevant $]$ For instance, the team\u015b graphic designer might be tasked with designing the invitations, while the event coordinator would be in charge of planning the program and working with vendors... [ Retrieve $=\\mathrm{N} \\circ$ ] By delegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also empowering their team members to take on responsibilities and contribute to the success of the event.[ IsUsE $=5]$\n\nInput Synthesize a poem based on this prompt:\"The sun is setting\" (from stanford_alpaca)\nOutput $[$ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night; [ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night $[$ Retrieve $=\\mathrm{No}$ ] They flicker like a silent dance, A peaceful moment in its trance. [Retrieve=No] They flicker like a silent dance, A peaceful moment in its trance. [ Retrieve $=\\mathrm{No}$ ] A chorus of crickets sing a lullaby, To every creature that roams the sky [ ISUSE $=5$ ]\n\nInput when did walking dead season 7 come out (from nq)\nOutput [Retrieve =Yes]<p>The Walking Dead (season 7)-The Walking Dead (season 7) The seventh season of \"The Walking Dead\", an American post-apocalyptic horror television series on AMC, premiered on October 23, 2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont, the series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard. $\\ldots<\\mathrm{p}>[$ ISREL $=$ Relevant] October 23, 2016[ IsSuP =Fully Supported][ ISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging response: Hello, I love German shepherd dogs and you? - My favorite dog, they are officially known as German Shepherd Dogs? - Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent. [Retrieve $=$ Yes] $<\\mathrm{p}>$ German Shepherd - a simple difference of opinion, it is the fundamental issue of the breed\u015b essential conformation and movement. ... They are known for being easy to train and good for performing tasks and following instructions. They are especially well known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and holding suspects. $\\langle\\mathrm{p}\\rangle$ [ ISREL $=$ Relevant $]$ Because of this, they are often used for disability assistance and for police work. [ IsSup]=Partially Supported][ IsUSE =2]\n\nTable 4: List of the training examples.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "There are two main kinds of semiconductor memory, volatile and non-volatile.",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ac66d00-89e6-4011-8ae7-e8a3f8ac453c",
        "questions": "Which types of memory examples are classified as non-volatile?",
        "answers": "Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM, and EEPROM memory (used for storing firmware such as BIOS).",
        "context": "Input Identify the two types of computer memory. (from gpt 4_alpaca)\nOutput [Retrieve]=Yes] $<\\mathrm{p}>$ Computer memory Computer memory - In computing, memory refers to the computer hardware integrated circuits that store information for immediate use in a computer; it is synonymous with the term \"primary storage\". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage; a very common way of doing this is through a memory management technique called \"virtual memory\". An archaic synonym for memory is store. The term \"memory\", meaning \"primary storage\" or \"main memory\", is $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ Primary memory, also known as main memory, is the memory that the computer can directly access and manipulate. [ IsSUP =Fully Supported] Retrieve $=$ Yes] <p>Computer memory-often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory $\\langle p>[$ ISREL $=$ Relevant $]$ Primary memory can be accessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ IsSuP =Partially Supported]...[ [IsUse :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt 4 a lpaca)\nOutput $[$ Retrieve $=\\mathrm{No}]$ The manager realizes that there is an extensive list of tasks that need to be completed in order to make the event a success. [Retrieve $=$ Yes] $<\\mathrm{p}>$ Delegation-Delegation is the assignment of any responsibility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It is one of the core concepts of management leadership. However, the person who delegated the work remains accountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it is a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly done, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too much input, $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ This includes tasks such as creating a guest list, designing and sending invitations, booking a venue, coordinating with vendors, planning the event\u015b program, and monitoring the budget.[ IsSuP =Partially Supported] Retrieve $=\\mathrm{No}$ ] Since it is not possible for the manager to handle everything alone, they must delegate tasks to their team members. Retrieve $=\\mathrm{No}$ ] The manager would then assign each team member specific responsibilities based on their individual skillsets and strengths. [Retrieve $=$ Yes $<<p>$ Joint application design - Subject Matter Experts: These are the business users, the IS professionals, and the outside experts that will be needed for a successful workshop. This group is the backbone of the meeting $<\\mathrm{p}>$ [ IsReL $=$ Irrelevant $]$ For instance, the team\u015b graphic designer might be tasked with designing the invitations, while the event coordinator would be in charge of planning the program and working with vendors... [ Retrieve $=\\mathrm{N} \\circ$ ] By delegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also empowering their team members to take on responsibilities and contribute to the success of the event.[ IsUsE $=5]$\n\nInput Synthesize a poem based on this prompt:\"The sun is setting\" (from stanford_alpaca)\nOutput $[$ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night; [ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night $[$ Retrieve $=\\mathrm{No}$ ] They flicker like a silent dance, A peaceful moment in its trance. [Retrieve=No] They flicker like a silent dance, A peaceful moment in its trance. [ Retrieve $=\\mathrm{No}$ ] A chorus of crickets sing a lullaby, To every creature that roams the sky [ ISUSE $=5$ ]\n\nInput when did walking dead season 7 come out (from nq)\nOutput [Retrieve =Yes]<p>The Walking Dead (season 7)-The Walking Dead (season 7) The seventh season of \"The Walking Dead\", an American post-apocalyptic horror television series on AMC, premiered on October 23, 2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont, the series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard. $\\ldots<\\mathrm{p}>[$ ISREL $=$ Relevant] October 23, 2016[ IsSuP =Fully Supported][ ISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging response: Hello, I love German shepherd dogs and you? - My favorite dog, they are officially known as German Shepherd Dogs? - Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent. [Retrieve $=$ Yes] $<\\mathrm{p}>$ German Shepherd - a simple difference of opinion, it is the fundamental issue of the breed\u015b essential conformation and movement. ... They are known for being easy to train and good for performing tasks and following instructions. They are especially well known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and holding suspects. $\\langle\\mathrm{p}\\rangle$ [ ISREL $=$ Relevant $]$ Because of this, they are often used for disability assistance and for police work. [ IsSup]=Partially Supported][ IsUSE =2]\n\nTable 4: List of the training examples.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS).",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ad75046-9ea4-446f-8857-cabee086f500",
        "questions": "Which kind of memory is typically used as fast CPU cache memory and what characteristic does it have?",
        "answers": "Fast CPU cache memory is typically static random-access memory (SRAM) that is fast but energy-consuming.",
        "context": "Input Identify the two types of computer memory. (from gpt 4_alpaca)\nOutput [Retrieve]=Yes] $<\\mathrm{p}>$ Computer memory Computer memory - In computing, memory refers to the computer hardware integrated circuits that store information for immediate use in a computer; it is synonymous with the term \"primary storage\". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage; a very common way of doing this is through a memory management technique called \"virtual memory\". An archaic synonym for memory is store. The term \"memory\", meaning \"primary storage\" or \"main memory\", is $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ Primary memory, also known as main memory, is the memory that the computer can directly access and manipulate. [ IsSUP =Fully Supported] Retrieve $=$ Yes] <p>Computer memory-often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory $\\langle p>[$ ISREL $=$ Relevant $]$ Primary memory can be accessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ IsSuP =Partially Supported]...[ [IsUse :5]\nInput Give an example of a situation where you have to delegate tasks (from gpt 4 a lpaca)\nOutput $[$ Retrieve $=\\mathrm{No}]$ The manager realizes that there is an extensive list of tasks that need to be completed in order to make the event a success. [Retrieve $=$ Yes] $<\\mathrm{p}>$ Delegation-Delegation is the assignment of any responsibility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It is one of the core concepts of management leadership. However, the person who delegated the work remains accountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it is a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly done, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too much input, $<\\mathrm{p}>[$ ISREL $=$ Relevant $]$ This includes tasks such as creating a guest list, designing and sending invitations, booking a venue, coordinating with vendors, planning the event\u015b program, and monitoring the budget.[ IsSuP =Partially Supported] Retrieve $=\\mathrm{No}$ ] Since it is not possible for the manager to handle everything alone, they must delegate tasks to their team members. Retrieve $=\\mathrm{No}$ ] The manager would then assign each team member specific responsibilities based on their individual skillsets and strengths. [Retrieve $=$ Yes $<<p>$ Joint application design - Subject Matter Experts: These are the business users, the IS professionals, and the outside experts that will be needed for a successful workshop. This group is the backbone of the meeting $<\\mathrm{p}>$ [ IsReL $=$ Irrelevant $]$ For instance, the team\u015b graphic designer might be tasked with designing the invitations, while the event coordinator would be in charge of planning the program and working with vendors... [ Retrieve $=\\mathrm{N} \\circ$ ] By delegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also empowering their team members to take on responsibilities and contribute to the success of the event.[ IsUsE $=5]$\n\nInput Synthesize a poem based on this prompt:\"The sun is setting\" (from stanford_alpaca)\nOutput $[$ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night; [ Retrieve $=\\mathrm{No}$ ]The stars begin to twinkle bright the trees, scattered throughout the night $[$ Retrieve $=\\mathrm{No}$ ] They flicker like a silent dance, A peaceful moment in its trance. [Retrieve=No] They flicker like a silent dance, A peaceful moment in its trance. [ Retrieve $=\\mathrm{No}$ ] A chorus of crickets sing a lullaby, To every creature that roams the sky [ ISUSE $=5$ ]\n\nInput when did walking dead season 7 come out (from nq)\nOutput [Retrieve =Yes]<p>The Walking Dead (season 7)-The Walking Dead (season 7) The seventh season of \"The Walking Dead\", an American post-apocalyptic horror television series on AMC, premiered on October 23, 2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont, the series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard. $\\ldots<\\mathrm{p}>[$ ISREL $=$ Relevant] October 23, 2016[ IsSuP =Fully Supported][ ISUSE =5]\nInput Given a chat history separated by new lines, generates an informative, knowledgeable and engaging response: Hello, I love German shepherd dogs and you? - My favorite dog, they are officially known as German Shepherd Dogs? - Yes, I have a German shepherd in my house and she is pregnant (from wow)\nOutput hey also are easily trained, obedient, and intelligent. [Retrieve $=$ Yes] $<\\mathrm{p}>$ German Shepherd - a simple difference of opinion, it is the fundamental issue of the breed\u015b essential conformation and movement. ... They are known for being easy to train and good for performing tasks and following instructions. They are especially well known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and holding suspects. $\\langle\\mathrm{p}\\rangle$ [ ISREL $=$ Relevant $]$ Because of this, they are often used for disability assistance and for police work. [ IsSup]=Partially Supported][ IsUSE =2]\n\nTable 4: List of the training examples.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming",
        "evidence_page_no": 21,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0adf1c0a-7e3d-48a3-b1c1-096ecd6e3c6f",
        "questions": "What were the accuracy percentages of the manual analysis for relevance, retrieval necessity, and the degree of support as found in the manual analysis of the GPT-4 predictions in the A Self-RaG Details document?",
        "answers": "95% for relevance, 95% for retrieval necessity, and 90% for the degree of support",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ).",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ae1f4ac-5bd2-4526-aaa6-897a213a6a4a",
        "questions": "How many instances were collected for the usefulness aspect during GPT-4-based data collections in the A Self-RaG Details document?",
        "answers": "3,831",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ae5aba8-bd7d-4e3e-b8df-13d3470df9e6",
        "questions": "What was the overall high agreement percentage reported for the aspect with the lowest manual analysis agreement in the document detailing the A Self-RaG Details?",
        "answers": "80%",
        "context": "A Self-RaG Details\n\n\nA. 1 REFLECTION TOKENS.\n\nDefinitions of reflection tokens. Below, we provide a detailed definition of reflection type and output tokens. The first three aspects will be provided at each segment level, while the final aspect is only given at each output level.\n- Retrieval-on-demand (Retrieve): Given an input and previous-step generation (if applicable), an LM determines whether the continuation requires factual grounding. No indicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue to use evidence, which indicates that a model can continue to use the evidence retrieved previously. For instance, a passage may contain rich factual information, and thus SELF-RAG generates multiple segments based on the passage.\n- Relevant ( ISREL ): Retrieved knowledge may not be always relevant to the input. This aspect indicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n- Supported ( IsSuP ): Attribution is the concept of whether the output is fully supported by certain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully supported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n- Useful ( IsUse ): Following the definitions from Liu et al. (2023a), we define the perceived utility as whether the response is a helpful and informative answer to the query, independently from whether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022). For usefulness, we use a five-scale evaluation ( 1 is the lowest and 5 is the highest).\n\nDetails of GPT-4-based data collections. We use the instruction and demonstration pairs to prompt GPT-4, listed in Section D. Following an official recommendation, we separate instructions and outputs with \"\\#\\#\". We use the temperature 1 and set the maximum output token counts to be 200 . We discard instances where GPT-4 does not follow the designated output formats or output sequences that do not match our expected category names. As a result, we collected 1,2594 for Retrieve, 11,181 for IsSuP, 19,317 for relevance, 3,831 for utility.\n\nManual analysis of the GPT-4 predictions. The authors of this paper manually assess randomly sampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given the same instruction, demonstrations, and test instances. We found our assessments show high agreement with GPT-4 predictions, especially for relevance ( $95 \\%$ ), retrieval necessity ( $95 \\%$ ), and the degree of support ( $90 \\%$ ). Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5 .\n\nA. 2 SElf-RAG Training\n\nOverview of training. Algorithm 2 provides a high-level overview of our training.\nFull list of seed datasets. To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca, OpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al., 2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\nPerformance of the Critic $\\mathcal{C}$. We evaluate the accuracy of reward predictions by splitting GPT-4 generated feedback into training, development, and test sets. The accuracy of the reward model is as follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see, overall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Agreement was slightly lower in usefulness ( $80 \\%$ ), mostly due to the disagreement between 1 and 2 or 4 and 5.",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0ae9e2e6-adaa-41ca-b19b-701bd042c1c8",
        "questions": "What is the citation recall for Ret-ChatGPT in the ASQA task, and how does it compare with the citation recall of Alpaca with retrieval?",
        "answers": "Ret-ChatGPT has a citation recall of 76.6, while Alpaca with retrieval has a citation recall of 7.2.",
        "context": "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among non-proprietary models, and gray-colored bold text indicates the best proprietary model when they outperforms all non-proprietary models. ${ }^{*}$ indicates concurrent or recent results reported by concurrent work. - indicates numbers that are not reported by the original papers or are not applicable. Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em, rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[b]{2}{*}{LM} & \\multicolumn{2}{|l|}{Short-form} & \\multicolumn{2}{|l|}{Closed-set} & \\multicolumn{6}{|c|}{Long-form generations (with citations)} \\\\\n  & \\begin{tabular}{l}\nPopQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nTQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nPub \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nARC \\\\\n(acc)\n\\end{tabular} & $$\\begin{array}{r}\n\\text { Bio } \\\\\n\\text { (FS) }\n\\end{array}$$ & & $(\\mathrm{rg})$ & \\begin{tabular}{l}\nASQA \\\\\n(mau)\n\\end{tabular} & (pre) & (rec) \\\\\n  \\multicolumn{11}{|c|}{LMs with proprietary data} \\\\\n  Llama2-c ${ }_{13}$ & 20.0 & 59.3 & 49.4 & 38.4 & 55.9 & 22.4 & 29.6 & 28.6 & $-$ & - \\\\\n  Ret-Llama2-c ${ }_{138}$ & 51.8 & 59.8 & 52.1 & 37.9 & 79.9 & 32.8 & 34.8 & 43.8 & 19.8 & 36.1 \\\\\n  ChatGPT & 29.3 & 74.3 & 70.1 & 75.3 & 71.8 & 35.3 & 36.2 & 68.8 & - & - \\\\\n  Ret-ChatGPT & 50.8 & 65.7 & 54.7 & 75.3 & - & 40.7 & 39.9 & 79.7 & 65.1 & 76.6 \\\\\n  Perplexity.ai & - & - & - & - & 71.2 & - & - & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines without retrieval} \\\\\n  Llama2 $_{7 \\text { \u0432 }}$ & 14.7 & 30.5 & 34.2 & 21.8 & 44.5 & 7.9 & 15.3 & 19.0 & - & - \\\\\n  Alpaca $_{7^{8}}$ & 23.6 & 54.5 & 49.8 & 45.0 & 45.8 & 18.8 & 29.4 & 61.7 & - & - \\\\\n  Llama2 ${ }_{13 \\text { B }}$ & 14.7 & 38.5 & 29.4 & 29.4 & 53.4 & 7.2 & 12.4 & 16.0 & - & - \\\\\n  Alpaca $^{\\text {3 }}$ & 24.4 & 61.3 & 55.5 & 54.9 & 50.2 & 22.9 & 32.0 & 70.6 & - & - \\\\\n  $\\mathrm{CoVE}_{65 \\text { B }}$ * & $\\square$ & - & - & - & 71.2 & - & $\\square$ & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines with retrieval} \\\\\n  Toolformer* ${ }_{6}$ & $-$ & 48.8 & - & - & $-$ & - & $-$ & $-$ & $-$ & - \\\\\n  Llama27\u0432 & 38.2 & 42.5 & 30.0 & 48.0 & 78.0 & 15.2 & 22.1 & 32.0 & 2.9 & 4.0 \\\\\n  Alpaca $_{\\text {\u0432 }}$ & 46.7 & 64.1 & 40.2 & 48.0 & 76.6 & 30.9 & 33.3 & 57.9 & 5.5 & 7.2 \\\\\n  Llama2-FT ${ }_{78}$ & 48.7 & 57.3 & 64.3 & 65.8 & 78.2 & 31.0 & 35.8 & 51.2 & 5.0 & 7.5 \\\\\n  SAIL* $_{7^{\\text {B }}}$ & $-$ & - & 69.2 & 48.4 & $-$ & - & $-$ & - & $-$ & $-$ \\\\\n  Llama ${ }_{138}$ & 45.7 & 47.0 & 30.2 & 26.0 & 77.5 & 16.3 & 20.5 & 24.7 & 2.3 & 3.6 \\\\\n  Alpaca ${ }^{33}$ & 46.1 & 66.9 & 51.1 & 57.6 & 77.7 & 34.8 & 36.7 & 56.6 & 2.0 & 3.8 \\\\\n   & $54 . \\overline{9}$ & $6 \\overline{6} . \\overline{4}$ & $\\overline{72.4}$ & 67.3 & 81.2 & $\\overline{30 .} \\overline{0}$ & $35.7^{-}$ & $7 \\overline{4} .{ }^{-}$ & $\\overline{6} 6.9$ & $6 \\overline{7} . \\overline{8}$ \\\\\n  Our Self-Rag 13b & 55.8 & 69.3 & 74.5 & 73.1 & 80.2 & 31.7 & 37.0 & 71.6 & 70.3 & 71.3 \\\\\n \n\\end{tabular}\n\n\n5 RESULTS AND ANALYSIS\n\n\n5.1 MAIN RESULTS\n\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without retrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which iteratively prompts Llama $2_{65}$ to refine output.\n\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAG also outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio, powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from their non-retrieval baselines. However, we found that these baselines provide limited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation accuracy. On ASQA, our model shows significantly higher citation precision and recall than all models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even outperforming ChatGPT in citation precision, which measures whether the model-generated claim is fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG 7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Ret-ChatGPT & 50.8 & 65.7 & 54.7 & 75.3 & - & 40.7 & 39.9 & 79.7 & 65.1 & 76.6 ... Alpaca $_{\text {\u0432 }}$ & 46.7 & 64.1 & 40.2 & 48.0 & 76.6 & 30.9 & 33.3 & 57.9 & 5.5 & 7.2",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0aeade49-1276-40bc-a8eb-65950bc798ef",
        "questions": "Which model has the highest accuracy on the ARC task among all listed models, and what is the accuracy percentage?",
        "answers": "ChatGPT has the highest accuracy on the ARC task with an accuracy of 75.3%.",
        "context": "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among non-proprietary models, and gray-colored bold text indicates the best proprietary model when they outperforms all non-proprietary models. ${ }^{*}$ indicates concurrent or recent results reported by concurrent work. - indicates numbers that are not reported by the original papers or are not applicable. Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em, rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[b]{2}{*}{LM} & \\multicolumn{2}{|l|}{Short-form} & \\multicolumn{2}{|l|}{Closed-set} & \\multicolumn{6}{|c|}{Long-form generations (with citations)} \\\\\n  & \\begin{tabular}{l}\nPopQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nTQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nPub \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nARC \\\\\n(acc)\n\\end{tabular} & $$\\begin{array}{r}\n\\text { Bio } \\\\\n\\text { (FS) }\n\\end{array}$$ & & $(\\mathrm{rg})$ & \\begin{tabular}{l}\nASQA \\\\\n(mau)\n\\end{tabular} & (pre) & (rec) \\\\\n  \\multicolumn{11}{|c|}{LMs with proprietary data} \\\\\n  Llama2-c ${ }_{13}$ & 20.0 & 59.3 & 49.4 & 38.4 & 55.9 & 22.4 & 29.6 & 28.6 & $-$ & - \\\\\n  Ret-Llama2-c ${ }_{138}$ & 51.8 & 59.8 & 52.1 & 37.9 & 79.9 & 32.8 & 34.8 & 43.8 & 19.8 & 36.1 \\\\\n  ChatGPT & 29.3 & 74.3 & 70.1 & 75.3 & 71.8 & 35.3 & 36.2 & 68.8 & - & - \\\\\n  Ret-ChatGPT & 50.8 & 65.7 & 54.7 & 75.3 & - & 40.7 & 39.9 & 79.7 & 65.1 & 76.6 \\\\\n  Perplexity.ai & - & - & - & - & 71.2 & - & - & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines without retrieval} \\\\\n  Llama2 $_{7 \\text { \u0432 }}$ & 14.7 & 30.5 & 34.2 & 21.8 & 44.5 & 7.9 & 15.3 & 19.0 & - & - \\\\\n  Alpaca $_{7^{8}}$ & 23.6 & 54.5 & 49.8 & 45.0 & 45.8 & 18.8 & 29.4 & 61.7 & - & - \\\\\n  Llama2 ${ }_{13 \\text { B }}$ & 14.7 & 38.5 & 29.4 & 29.4 & 53.4 & 7.2 & 12.4 & 16.0 & - & - \\\\\n  Alpaca $^{\\text {3 }}$ & 24.4 & 61.3 & 55.5 & 54.9 & 50.2 & 22.9 & 32.0 & 70.6 & - & - \\\\\n  $\\mathrm{CoVE}_{65 \\text { B }}$ * & $\\square$ & - & - & - & 71.2 & - & $\\square$ & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines with retrieval} \\\\\n  Toolformer* ${ }_{6}$ & $-$ & 48.8 & - & - & $-$ & - & $-$ & $-$ & $-$ & - \\\\\n  Llama27\u0432 & 38.2 & 42.5 & 30.0 & 48.0 & 78.0 & 15.2 & 22.1 & 32.0 & 2.9 & 4.0 \\\\\n  Alpaca $_{\\text {\u0432 }}$ & 46.7 & 64.1 & 40.2 & 48.0 & 76.6 & 30.9 & 33.3 & 57.9 & 5.5 & 7.2 \\\\\n  Llama2-FT ${ }_{78}$ & 48.7 & 57.3 & 64.3 & 65.8 & 78.2 & 31.0 & 35.8 & 51.2 & 5.0 & 7.5 \\\\\n  SAIL* $_{7^{\\text {B }}}$ & $-$ & - & 69.2 & 48.4 & $-$ & - & $-$ & - & $-$ & $-$ \\\\\n  Llama ${ }_{138}$ & 45.7 & 47.0 & 30.2 & 26.0 & 77.5 & 16.3 & 20.5 & 24.7 & 2.3 & 3.6 \\\\\n  Alpaca ${ }^{33}$ & 46.1 & 66.9 & 51.1 & 57.6 & 77.7 & 34.8 & 36.7 & 56.6 & 2.0 & 3.8 \\\\\n   & $54 . \\overline{9}$ & $6 \\overline{6} . \\overline{4}$ & $\\overline{72.4}$ & 67.3 & 81.2 & $\\overline{30 .} \\overline{0}$ & $35.7^{-}$ & $7 \\overline{4} .{ }^{-}$ & $\\overline{6} 6.9$ & $6 \\overline{7} . \\overline{8}$ \\\\\n  Our Self-Rag 13b & 55.8 & 69.3 & 74.5 & 73.1 & 80.2 & 31.7 & 37.0 & 71.6 & 70.3 & 71.3 \\\\\n \n\\end{tabular}\n\n\n5 RESULTS AND ANALYSIS\n\n\n5.1 MAIN RESULTS\n\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without retrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which iteratively prompts Llama $2_{65}$ to refine output.\n\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAG also outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio, powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from their non-retrieval baselines. However, we found that these baselines provide limited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation accuracy. On ASQA, our model shows significantly higher citation precision and recall than all models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even outperforming ChatGPT in citation precision, which measures whether the model-generated claim is fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG 7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "ChatGPT & 29.3 & 74.3 & 70.1 & 75.3 & 71.8 & 35.3 & 36.2 & 68.8 & - & -",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0af3b8f2-1b74-4e30-b2a2-9e5708ff1966",
        "questions": "Among the proprietary models, what is the FactScore (FS) for Llama2-c in the Bio generation task?",
        "answers": "The FactScore (FS) for Llama2-c in the Bio generation task is 55.9.",
        "context": "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among non-proprietary models, and gray-colored bold text indicates the best proprietary model when they outperforms all non-proprietary models. ${ }^{*}$ indicates concurrent or recent results reported by concurrent work. - indicates numbers that are not reported by the original papers or are not applicable. Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em, rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[b]{2}{*}{LM} & \\multicolumn{2}{|l|}{Short-form} & \\multicolumn{2}{|l|}{Closed-set} & \\multicolumn{6}{|c|}{Long-form generations (with citations)} \\\\\n  & \\begin{tabular}{l}\nPopQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nTQA \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nPub \\\\\n(acc)\n\\end{tabular} & \\begin{tabular}{l}\nARC \\\\\n(acc)\n\\end{tabular} & $$\\begin{array}{r}\n\\text { Bio } \\\\\n\\text { (FS) }\n\\end{array}$$ & & $(\\mathrm{rg})$ & \\begin{tabular}{l}\nASQA \\\\\n(mau)\n\\end{tabular} & (pre) & (rec) \\\\\n  \\multicolumn{11}{|c|}{LMs with proprietary data} \\\\\n  Llama2-c ${ }_{13}$ & 20.0 & 59.3 & 49.4 & 38.4 & 55.9 & 22.4 & 29.6 & 28.6 & $-$ & - \\\\\n  Ret-Llama2-c ${ }_{138}$ & 51.8 & 59.8 & 52.1 & 37.9 & 79.9 & 32.8 & 34.8 & 43.8 & 19.8 & 36.1 \\\\\n  ChatGPT & 29.3 & 74.3 & 70.1 & 75.3 & 71.8 & 35.3 & 36.2 & 68.8 & - & - \\\\\n  Ret-ChatGPT & 50.8 & 65.7 & 54.7 & 75.3 & - & 40.7 & 39.9 & 79.7 & 65.1 & 76.6 \\\\\n  Perplexity.ai & - & - & - & - & 71.2 & - & - & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines without retrieval} \\\\\n  Llama2 $_{7 \\text { \u0432 }}$ & 14.7 & 30.5 & 34.2 & 21.8 & 44.5 & 7.9 & 15.3 & 19.0 & - & - \\\\\n  Alpaca $_{7^{8}}$ & 23.6 & 54.5 & 49.8 & 45.0 & 45.8 & 18.8 & 29.4 & 61.7 & - & - \\\\\n  Llama2 ${ }_{13 \\text { B }}$ & 14.7 & 38.5 & 29.4 & 29.4 & 53.4 & 7.2 & 12.4 & 16.0 & - & - \\\\\n  Alpaca $^{\\text {3 }}$ & 24.4 & 61.3 & 55.5 & 54.9 & 50.2 & 22.9 & 32.0 & 70.6 & - & - \\\\\n  $\\mathrm{CoVE}_{65 \\text { B }}$ * & $\\square$ & - & - & - & 71.2 & - & $\\square$ & - & - & - \\\\\n  \\multicolumn{11}{|c|}{Baselines with retrieval} \\\\\n  Toolformer* ${ }_{6}$ & $-$ & 48.8 & - & - & $-$ & - & $-$ & $-$ & $-$ & - \\\\\n  Llama27\u0432 & 38.2 & 42.5 & 30.0 & 48.0 & 78.0 & 15.2 & 22.1 & 32.0 & 2.9 & 4.0 \\\\\n  Alpaca $_{\\text {\u0432 }}$ & 46.7 & 64.1 & 40.2 & 48.0 & 76.6 & 30.9 & 33.3 & 57.9 & 5.5 & 7.2 \\\\\n  Llama2-FT ${ }_{78}$ & 48.7 & 57.3 & 64.3 & 65.8 & 78.2 & 31.0 & 35.8 & 51.2 & 5.0 & 7.5 \\\\\n  SAIL* $_{7^{\\text {B }}}$ & $-$ & - & 69.2 & 48.4 & $-$ & - & $-$ & - & $-$ & $-$ \\\\\n  Llama ${ }_{138}$ & 45.7 & 47.0 & 30.2 & 26.0 & 77.5 & 16.3 & 20.5 & 24.7 & 2.3 & 3.6 \\\\\n  Alpaca ${ }^{33}$ & 46.1 & 66.9 & 51.1 & 57.6 & 77.7 & 34.8 & 36.7 & 56.6 & 2.0 & 3.8 \\\\\n   & $54 . \\overline{9}$ & $6 \\overline{6} . \\overline{4}$ & $\\overline{72.4}$ & 67.3 & 81.2 & $\\overline{30 .} \\overline{0}$ & $35.7^{-}$ & $7 \\overline{4} .{ }^{-}$ & $\\overline{6} 6.9$ & $6 \\overline{7} . \\overline{8}$ \\\\\n  Our Self-Rag 13b & 55.8 & 69.3 & 74.5 & 73.1 & 80.2 & 31.7 & 37.0 & 71.6 & 70.3 & 71.3 \\\\\n \n\\end{tabular}\n\n\n5 RESULTS AND ANALYSIS\n\n\n5.1 MAIN RESULTS\n\nComparison against baselines without retrieval. Table 2 (top) presents the baselines without retrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA, biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which iteratively prompts Llama $2_{65}$ to refine output.\n\nComparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAG also outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio, powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from their non-retrieval baselines. However, we found that these baselines provide limited solutions for tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth and ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation accuracy. On ASQA, our model shows significantly higher citation precision and recall than all models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy in this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even outperforming ChatGPT in citation precision, which measures whether the model-generated claim is fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG 7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Llama2-c ${ }_{13}$ & 20.0 & 59.3 & 49.4 & 38.4 & 55.9 & 22.4 & 29.6 & 28.6 & $-$ & -",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0b04ceb9-9fe0-4a14-bae2-a4d5bb03da5b",
        "questions": "What perceived utility score was given to the response to the instruction asking about the current prime minister of the UK as of 2023?",
        "answers": "2",
        "context": "Instructions\n\n\nGiven an instruction and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility. The detailed criterion is as follows: 5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs. 4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence. 3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs. 2: The response still addresses the main request, but it is not complete or not relevant to the query. 1: The response is barely on-topic or completely irrelevant.\n\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime minister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it doesn't answer the instruction. Therefore, the utility is 2.\n\nInstruction Given a description of a travel destination, recommend 10 tourist attractions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn't provide descriptions of each attraction and the number of the attractions is also less than 10 . While this output partially answers the instructions, it doesn't match the instructions strictly.\n\nTable 12: Instructions and demonstrations for ISUSE tokens.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Perceived utility 2",
        "evidence_page_no": 29,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0b0727aa-e4e9-4de5-84a9-2db9360044f2",
        "questions": "In the instruction asking for tourist attractions in Tokyo, Japan, how many attractions were listed in the output?",
        "answers": "7",
        "context": "Instructions\n\n\nGiven an instruction and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility. The detailed criterion is as follows: 5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs. 4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence. 3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs. 2: The response still addresses the main request, but it is not complete or not relevant to the query. 1: The response is barely on-topic or completely irrelevant.\n\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime minister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it doesn't answer the instruction. Therefore, the utility is 2.\n\nInstruction Given a description of a travel destination, recommend 10 tourist attractions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn't provide descriptions of each attraction and the number of the attractions is also less than 10 . While this output partially answers the instructions, it doesn't match the instructions strictly.\n\nTable 12: Instructions and demonstrations for ISUSE tokens.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.",
        "evidence_page_no": 29,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2310.11511v1",
        "ID": "0b0a13d7-d42b-4d03-bd09-e9995e688624",
        "questions": "What is one of the criteria mentioned for rating a response with a perceived utility score of 5?",
        "answers": "The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.",
        "context": "Instructions\n\n\nGiven an instruction and an output, rate whether the response appears to be a helpful and informative answer to the query, from 1 (lowest) - 5 (highest). We call this score perceived utility. The detailed criterion is as follows: 5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs. 4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence. 3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs. 2: The response still addresses the main request, but it is not complete or not relevant to the query. 1: The response is barely on-topic or completely irrelevant.\n\nInstruction Who is the current prime minister of the UK as of 2023?\nOutput Boris Johnson was the prime minister of the UK from 2019 - 2022.\nPerceived utility 2\nExplanation While the output provides a factually correct statement about the UK prime minister from 2019 to 2022, this instruction asks who the prime minister is as of 2023, so it doesn't answer the instruction. Therefore, the utility is 2.\n\nInstruction Given a description of a travel destination, recommend 10 tourist attractions with detailed descriptions of each. The travel destination is Tokyo, Japan.\nOutput Tokyo is a vibrant city full of exciting tourist attractions. Some must-see sights include the Tokyo Skytree, Tokyo Disneyland, Sensoji Temple, Meiji Shrine, Tsukiji Fish Market, Harajuku, and Shinjuku Gyoen.\nPerceived utility 3\nExplanation This output doesn't provide descriptions of each attraction and the number of the attractions is also less than 10 . While this output partially answers the instructions, it doesn't match the instructions strictly.\n\nTable 12: Instructions and demonstrations for ISUSE tokens.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.",
        "evidence_page_no": 29,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b0c6abc-0159-4c81-8480-0e8d682f9440",
        "questions": "What is the input size that the encoder of the GOT model can handle?",
        "answers": "1024 \u00d7 1024",
        "context": "InternVL-1.5 [9] and other models [27,50] utilize a sliding window manner to crop the whole image into multiple sub-patches for high-resolution OCR. Hence, a consensus is that optical character perception and recognition are the foundation of text-driven image understanding, drawing many researchers to pay more attention to LVLMs' OCR booster.\n\nHowever, the popular designs of LVLMs may not be suitable for diverse OCR tasks for the following reasons: 1) The conflicts between perception and reasoning. LVLMs mainly focus on visual reasoning performance, e.g., VQA $[33,42]$, because that is what the LLM excels at. To quickly obtain the QA-gain benefits from LLMs, most LVLMs [15, 24, 49] align image tokens to text ones. However, it is unreasonable to do this for pure perception OCR tasks, especially high-density text scenes, because each aligned vision token (biased towards text token) cannot compress enough characters. Imagine how wasteful it is to use thousands of image tokens, e.g., the image-cropping manner [9, 23], to encode an equal amount of optical characters (e.g., texts within only an A4-PDF page). 2) High iteration and deployment costs. LVLM often enjoys billions of parameters, leading to the post-training and deployment costs being too high. Generally speaking, for LVLMs, fine-tuning is not enough once we want to add a new OCR pattern, e.g., a new language, instead of enough GPU resources for pre-training. However, rerunning the pre-training with billions of parameters, only to introduce a new OCR feature, is also wasteful.\n\nAccordingly, we propose the general OCR theory, i.e., OCR-2.0, to break the bottlenecks of both traditional and LVLM manners on OCR tasks. We think that a model of OCR 2.0 should have the following essential characteristics:\n- End-to-end. Compared to OCR-1.0 models with complex procedures, the OCR-2.0 model should enjoy a unified and end-to-end architecture to ensure lower maintenance costs. It is cool that a beginner can quickly master the entire OCR system in the 2.0 era.\n- Low training and inference costs. The OCR-2.0 model should not be a chatbot, like LVLM, that focuses on reasoning tasks. Its focus should be on strong perception and recognition of optical characters, so it needs a reasonable number of model parameters in exchange for lower training and inference costs.\n- Versatility. The OCR-2.0 model's other important point is versatility, including recognizing more general artificial optical \"characters\", e.g., sheet music, charts, geometric shapes, etc. Besides, the model should support the output format with stronger readability, e.g., $\\mathrm{IAT}_{\\mathrm{E}} \\mathrm{X} / \\mathrm{Markdown}$ format for formulas and tables.\n\nBased on the proposed general OCR theory, we present a primary OCR-2.0 model (GOT) to bridge the gap between OCR-1.0 models and people's higher optical character processing demands. In architecture, we adopt the unsophisticated encoder-decoder paradigm for the model. Specifically, GOT enjoys a high compression rate encoder to transfer the optical image to tokens as well as a long context length decoder to output the corresponding OCR results. The encoder has approximately 80 M parameters posing $1024 \\times 1024$ input size which is enough to deal with commonly used photo/document input styles. Each input image will be compressed to tokens with $256 \\times 1024$ dimensions. The decoder of GOT, with 0.5 B parameters, supports 8 K max length tokens to ensure it can tackle long-context scenarios. We devise an effective and efficient training strategy for GOT, which can be divided into three procedures, i.e., decoupled pre-training of the encoder, joint-training of the encoder with a new decoder, and further post-training of the decoder. Besides, to further lift the practicality of GOT, we additionally adapt the fine-grained OCR feature for better interactivity, dynamic resolution strategy for ultra-high-resolution images (e.g., over 2 K ), and the multi-page OCR technology to alleviate the problem of difficulty in breaking pages in PDF image-text pairs (e.g., page breaks in .tex files). To support each training stage, we do many data engines for synthetic data production, which is the key to the success of GOT and will be described in detail in this paper. The main input data format supported by our model can be seen in Figure 1.\n\nAs a model for envisioning OCR-2.0, GOT demonstrates promising performance in our experiments in various OCR tasks. We hope the proposed simple and elegant GOT can draw more researchers to invest in the research of OCR-2.0. Of course, the path to OCR-2.0 is still long and GOT also enjoys much improvement room, such as supporting more languages, more general artificial signals, and more complex geometries. In this new era led by LVLMs, we are convinced that the pure OCR model is not over, it may even be a new beginning.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The encoder has approximately 80 M parameters posing $1024 \\times 1024$ input size which is enough to deal with commonly used photo/document input styles.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b0eab3c-2620-4321-bb93-3b849e996246",
        "questions": "How many parameters does the decoder of the GOT model have?",
        "answers": "0.5 B",
        "context": "InternVL-1.5 [9] and other models [27,50] utilize a sliding window manner to crop the whole image into multiple sub-patches for high-resolution OCR. Hence, a consensus is that optical character perception and recognition are the foundation of text-driven image understanding, drawing many researchers to pay more attention to LVLMs' OCR booster.\n\nHowever, the popular designs of LVLMs may not be suitable for diverse OCR tasks for the following reasons: 1) The conflicts between perception and reasoning. LVLMs mainly focus on visual reasoning performance, e.g., VQA $[33,42]$, because that is what the LLM excels at. To quickly obtain the QA-gain benefits from LLMs, most LVLMs [15, 24, 49] align image tokens to text ones. However, it is unreasonable to do this for pure perception OCR tasks, especially high-density text scenes, because each aligned vision token (biased towards text token) cannot compress enough characters. Imagine how wasteful it is to use thousands of image tokens, e.g., the image-cropping manner [9, 23], to encode an equal amount of optical characters (e.g., texts within only an A4-PDF page). 2) High iteration and deployment costs. LVLM often enjoys billions of parameters, leading to the post-training and deployment costs being too high. Generally speaking, for LVLMs, fine-tuning is not enough once we want to add a new OCR pattern, e.g., a new language, instead of enough GPU resources for pre-training. However, rerunning the pre-training with billions of parameters, only to introduce a new OCR feature, is also wasteful.\n\nAccordingly, we propose the general OCR theory, i.e., OCR-2.0, to break the bottlenecks of both traditional and LVLM manners on OCR tasks. We think that a model of OCR 2.0 should have the following essential characteristics:\n- End-to-end. Compared to OCR-1.0 models with complex procedures, the OCR-2.0 model should enjoy a unified and end-to-end architecture to ensure lower maintenance costs. It is cool that a beginner can quickly master the entire OCR system in the 2.0 era.\n- Low training and inference costs. The OCR-2.0 model should not be a chatbot, like LVLM, that focuses on reasoning tasks. Its focus should be on strong perception and recognition of optical characters, so it needs a reasonable number of model parameters in exchange for lower training and inference costs.\n- Versatility. The OCR-2.0 model's other important point is versatility, including recognizing more general artificial optical \"characters\", e.g., sheet music, charts, geometric shapes, etc. Besides, the model should support the output format with stronger readability, e.g., $\\mathrm{IAT}_{\\mathrm{E}} \\mathrm{X} / \\mathrm{Markdown}$ format for formulas and tables.\n\nBased on the proposed general OCR theory, we present a primary OCR-2.0 model (GOT) to bridge the gap between OCR-1.0 models and people's higher optical character processing demands. In architecture, we adopt the unsophisticated encoder-decoder paradigm for the model. Specifically, GOT enjoys a high compression rate encoder to transfer the optical image to tokens as well as a long context length decoder to output the corresponding OCR results. The encoder has approximately 80 M parameters posing $1024 \\times 1024$ input size which is enough to deal with commonly used photo/document input styles. Each input image will be compressed to tokens with $256 \\times 1024$ dimensions. The decoder of GOT, with 0.5 B parameters, supports 8 K max length tokens to ensure it can tackle long-context scenarios. We devise an effective and efficient training strategy for GOT, which can be divided into three procedures, i.e., decoupled pre-training of the encoder, joint-training of the encoder with a new decoder, and further post-training of the decoder. Besides, to further lift the practicality of GOT, we additionally adapt the fine-grained OCR feature for better interactivity, dynamic resolution strategy for ultra-high-resolution images (e.g., over 2 K ), and the multi-page OCR technology to alleviate the problem of difficulty in breaking pages in PDF image-text pairs (e.g., page breaks in .tex files). To support each training stage, we do many data engines for synthetic data production, which is the key to the success of GOT and will be described in detail in this paper. The main input data format supported by our model can be seen in Figure 1.\n\nAs a model for envisioning OCR-2.0, GOT demonstrates promising performance in our experiments in various OCR tasks. We hope the proposed simple and elegant GOT can draw more researchers to invest in the research of OCR-2.0. Of course, the path to OCR-2.0 is still long and GOT also enjoys much improvement room, such as supporting more languages, more general artificial signals, and more complex geometries. In this new era led by LVLMs, we are convinced that the pure OCR model is not over, it may even be a new beginning.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The decoder of GOT, with 0.5 B parameters, supports 8 K max length tokens to ensure it can tackle long-context scenarios.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b10c800-2b07-4d36-aa6e-df80fb25a0f9",
        "questions": "Does the GOT model support multi-page OCR technology to handle page breaks in PDF image-text pairs?",
        "answers": "Yes",
        "context": "InternVL-1.5 [9] and other models [27,50] utilize a sliding window manner to crop the whole image into multiple sub-patches for high-resolution OCR. Hence, a consensus is that optical character perception and recognition are the foundation of text-driven image understanding, drawing many researchers to pay more attention to LVLMs' OCR booster.\n\nHowever, the popular designs of LVLMs may not be suitable for diverse OCR tasks for the following reasons: 1) The conflicts between perception and reasoning. LVLMs mainly focus on visual reasoning performance, e.g., VQA $[33,42]$, because that is what the LLM excels at. To quickly obtain the QA-gain benefits from LLMs, most LVLMs [15, 24, 49] align image tokens to text ones. However, it is unreasonable to do this for pure perception OCR tasks, especially high-density text scenes, because each aligned vision token (biased towards text token) cannot compress enough characters. Imagine how wasteful it is to use thousands of image tokens, e.g., the image-cropping manner [9, 23], to encode an equal amount of optical characters (e.g., texts within only an A4-PDF page). 2) High iteration and deployment costs. LVLM often enjoys billions of parameters, leading to the post-training and deployment costs being too high. Generally speaking, for LVLMs, fine-tuning is not enough once we want to add a new OCR pattern, e.g., a new language, instead of enough GPU resources for pre-training. However, rerunning the pre-training with billions of parameters, only to introduce a new OCR feature, is also wasteful.\n\nAccordingly, we propose the general OCR theory, i.e., OCR-2.0, to break the bottlenecks of both traditional and LVLM manners on OCR tasks. We think that a model of OCR 2.0 should have the following essential characteristics:\n- End-to-end. Compared to OCR-1.0 models with complex procedures, the OCR-2.0 model should enjoy a unified and end-to-end architecture to ensure lower maintenance costs. It is cool that a beginner can quickly master the entire OCR system in the 2.0 era.\n- Low training and inference costs. The OCR-2.0 model should not be a chatbot, like LVLM, that focuses on reasoning tasks. Its focus should be on strong perception and recognition of optical characters, so it needs a reasonable number of model parameters in exchange for lower training and inference costs.\n- Versatility. The OCR-2.0 model's other important point is versatility, including recognizing more general artificial optical \"characters\", e.g., sheet music, charts, geometric shapes, etc. Besides, the model should support the output format with stronger readability, e.g., $\\mathrm{IAT}_{\\mathrm{E}} \\mathrm{X} / \\mathrm{Markdown}$ format for formulas and tables.\n\nBased on the proposed general OCR theory, we present a primary OCR-2.0 model (GOT) to bridge the gap between OCR-1.0 models and people's higher optical character processing demands. In architecture, we adopt the unsophisticated encoder-decoder paradigm for the model. Specifically, GOT enjoys a high compression rate encoder to transfer the optical image to tokens as well as a long context length decoder to output the corresponding OCR results. The encoder has approximately 80 M parameters posing $1024 \\times 1024$ input size which is enough to deal with commonly used photo/document input styles. Each input image will be compressed to tokens with $256 \\times 1024$ dimensions. The decoder of GOT, with 0.5 B parameters, supports 8 K max length tokens to ensure it can tackle long-context scenarios. We devise an effective and efficient training strategy for GOT, which can be divided into three procedures, i.e., decoupled pre-training of the encoder, joint-training of the encoder with a new decoder, and further post-training of the decoder. Besides, to further lift the practicality of GOT, we additionally adapt the fine-grained OCR feature for better interactivity, dynamic resolution strategy for ultra-high-resolution images (e.g., over 2 K ), and the multi-page OCR technology to alleviate the problem of difficulty in breaking pages in PDF image-text pairs (e.g., page breaks in .tex files). To support each training stage, we do many data engines for synthetic data production, which is the key to the success of GOT and will be described in detail in this paper. The main input data format supported by our model can be seen in Figure 1.\n\nAs a model for envisioning OCR-2.0, GOT demonstrates promising performance in our experiments in various OCR tasks. We hope the proposed simple and elegant GOT can draw more researchers to invest in the research of OCR-2.0. Of course, the path to OCR-2.0 is still long and GOT also enjoys much improvement room, such as supporting more languages, more general artificial signals, and more complex geometries. In this new era led by LVLMs, we are convinced that the pure OCR model is not over, it may even be a new beginning.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Besides, to further lift the practicality of GOT, we additionally adapt the fine-grained OCR feature for better interactivity, dynamic resolution strategy for ultra-high-resolution images (e.g., over 2 K ), and the multi-page OCR technology to alleviate the problem of difficulty in breaking pages in PDF image-text pairs (e.g., page breaks in .tex files).",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b11354c-3a9e-4435-8a56-e0661be77a10",
        "questions": "What is the name of the model that is equipped with more interactive fine-grained OCR tasks?",
        "answers": "GOT",
        "context": "Prompt: OCR\n\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\nOutput:\n\n\u79be\u4e0d\u9508\u94a2 6 \u7c73\u526a\u677f\u6298\u5f2f\n\nTiredness\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\n\nPrompt: OCR/ [green]OCR/ OCR with format:\n\n\n\nPrompt: [x1,y1,x2,y2] OCR with format:\n\n\nFigure 6: Scene OCR and fine-grained OCR results of GOT. We equip GOT with more interactive fine-grained OCR tasks, allowing it to output OCR results of regions of interest based on prompts.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Figure 6: Scene OCR and fine-grained OCR results of GOT. We equip GOT with more interactive fine-grained OCR tasks, allowing it to output OCR results of regions of interest based on prompts.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b19f5b2-c38b-4a22-8809-dad9f97d975c",
        "questions": "Does the document mention that GOT can output OCR results of regions of interest based on prompts?",
        "answers": "Yes",
        "context": "Prompt: OCR\n\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\nOutput:\n\n\u79be\u4e0d\u9508\u94a2 6 \u7c73\u526a\u677f\u6298\u5f2f\n\nTiredness\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\n\nPrompt: OCR/ [green]OCR/ OCR with format:\n\n\n\nPrompt: [x1,y1,x2,y2] OCR with format:\n\n\nFigure 6: Scene OCR and fine-grained OCR results of GOT. We equip GOT with more interactive fine-grained OCR tasks, allowing it to output OCR results of regions of interest based on prompts.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Figure 6: Scene OCR and fine-grained OCR results of GOT. We equip GOT with more interactive fine-grained OCR tasks, allowing it to output OCR results of regions of interest based on prompts.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b374e56-77d4-487e-93a9-917ffd19757d",
        "questions": "What is the format used in the prompt for OCR in the document?",
        "answers": "[x1,y1,x2,y2]",
        "context": "Prompt: OCR\n\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\nOutput:\n\n\u79be\u4e0d\u9508\u94a2 6 \u7c73\u526a\u677f\u6298\u5f2f\n\nTiredness\n\nThe same source quotes yet another as stating, \"You cannot be saved if you don't believe in the Trinity.\"\n\n\nPrompt: OCR/ [green]OCR/ OCR with format:\n\n\n\nPrompt: [x1,y1,x2,y2] OCR with format:\n\n\nFigure 6: Scene OCR and fine-grained OCR results of GOT. We equip GOT with more interactive fine-grained OCR tasks, allowing it to output OCR results of regions of interest based on prompts.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Prompt: [x1,y1,x2,y2] OCR with format:",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b3cdb8a-0d10-4bc4-98f9-ae8d505f367e",
        "questions": "What is the name of the model that demonstrates outstanding OCR performance in the document?",
        "answers": "GOT",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this section, we provide sufficient output results of GOT to show its outstanding OCR performance.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b489eb1-ef93-44f7-a8be-ec6fab8802d4",
        "questions": "Does GOT work well on full-page texts and table/formula slice texts according to the document?",
        "answers": "Yes",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "GOT works well on full-page texts and table/formula slice texts.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b4cba0a-317e-4b1d-9b83-acd5e2ccddd8",
        "questions": "What is the equation that represents the calculation of $d_{L}(C_{L},\\{v'\\})$ in the document?",
        "answers": "$d_{L}(C_{L},\\{v'\\})=|C_{L}|+|\\{v'\\}|+2(d_{T}(C_{L}, v')-1)$",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$$d_{L}(C_{L},\\{v'\\})=|C_{L}|+|\\{v'\\}|+2(d_{T}(C_{L}, v')-1)$$",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b5846c8-79e3-4aa0-a223-8f5548e422fd",
        "questions": "What is the maximum input resolution supported by GOT for OCR tasks?",
        "answers": "1024 \u00d7 1024",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "GOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b652dbd-a7e3-4bc5-8ef9-e157e439d480",
        "questions": "How many image-text pairs were obtained using the ultra-resolution image synthesis method for OCR tasks?",
        "answers": "50 w",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Through this method, we obtained a total of 50 w image-texts pairs.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b6ccca2-c907-4f5d-8a8f-08658fc929bd",
        "questions": "What is the starting learning rate used in the last post-training stage of GOT?",
        "answers": "2e-5",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b738648-f0eb-4870-ae0b-e868e5a9ff7e",
        "questions": "What is the name of the encoder structure selected for the vision encoder in the GOT framework?",
        "answers": "VitDet",
        "context": "Figure 2: The framework of the proposed GOT. Stage 1: We pre-train the vision encoder using a tiny OPT-125M to adapt the OCR tasks efficiently. Stage 2: GOT is built by connecting the vision encoder to Qwen-0.5B and sufficient OCR-2.0 knowledge of more general optical characters is used in this stage. Stage 3: No modification of the vision encoder is required, and GOT is customized to new character recognition features.\n\n\n3.2 Pre-train the OCR-earmarked Vision Encoder\n\n\nAs aforementioned, GOT enjoys the encoder-decoder structure. Inspired by the LVLMs design, the decoder can be initialized by a well-trained language model. However, we did not find a suitable pre-trained encoder for an OCR-2.0 model, so we must train one ourselves. We hope the new OCR encoder can work well on commonly used scene and document text recognition in various input shapes (both slices and whole pages).\n\n3.2.1 The Vision Encoder Generation.\n\nThe encoder structure we selected is VitDet [17] (base version with about 80M parameters) due to its local attention can greatly reduce the computational cost of high-resolution images. We follow the Vary-tiny setting [46] to design the last two layers of the encoder, which will transfer a $1024 \\times 1024 \\times 3$ input image to $256 \\times 1024$ image tokens. Then, these image tokens are projected into language model (OPT-125M [53]) dimension via a $1024 \\times 768$ linear layer. Unlike the Vary encoder which only focuses on a single document task under a relatively unitary input shape, we incorporated natural scenes and cropped slices during our pre-training. In the pre-processing stage, images of each shape are directly resized to $1024 \\times 1024$ squares, as square shapes can be used to adapt to images of various aspect ratios with a compromise.\n\n3.2.2 Data Engine Towards Encoder Pre-training\n\nIn such an encoder pre-training stage, we use about 5M image-text pairs, including 3 M scene text OCR data and 2 M document OCR data. Their acquisition methods are as follows:\n\nFor the natural scene data, the English and Chinese images are sampled from Laion-2B [40] and Wukong [12] datasets, respectively. Then, the pseudo ground truth in these diverse real scenes is captured using PaddleOCR [10] tools. Overall, we obtain 2M dat with half in Chinese and half in English. For text ground truth, we perform two types of processing: 1) remove the bounding box and",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The encoder structure we selected is VitDet [17] (base version with about 80M parameters) due to its local attention can greatly reduce the computational cost of high-resolution images.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b7fc161-8c1d-46b2-8e33-4a4986cdf496",
        "questions": "How many image-text pairs are used in the encoder pre-training stage of the GOT framework?",
        "answers": "5M",
        "context": "Figure 2: The framework of the proposed GOT. Stage 1: We pre-train the vision encoder using a tiny OPT-125M to adapt the OCR tasks efficiently. Stage 2: GOT is built by connecting the vision encoder to Qwen-0.5B and sufficient OCR-2.0 knowledge of more general optical characters is used in this stage. Stage 3: No modification of the vision encoder is required, and GOT is customized to new character recognition features.\n\n\n3.2 Pre-train the OCR-earmarked Vision Encoder\n\n\nAs aforementioned, GOT enjoys the encoder-decoder structure. Inspired by the LVLMs design, the decoder can be initialized by a well-trained language model. However, we did not find a suitable pre-trained encoder for an OCR-2.0 model, so we must train one ourselves. We hope the new OCR encoder can work well on commonly used scene and document text recognition in various input shapes (both slices and whole pages).\n\n3.2.1 The Vision Encoder Generation.\n\nThe encoder structure we selected is VitDet [17] (base version with about 80M parameters) due to its local attention can greatly reduce the computational cost of high-resolution images. We follow the Vary-tiny setting [46] to design the last two layers of the encoder, which will transfer a $1024 \\times 1024 \\times 3$ input image to $256 \\times 1024$ image tokens. Then, these image tokens are projected into language model (OPT-125M [53]) dimension via a $1024 \\times 768$ linear layer. Unlike the Vary encoder which only focuses on a single document task under a relatively unitary input shape, we incorporated natural scenes and cropped slices during our pre-training. In the pre-processing stage, images of each shape are directly resized to $1024 \\times 1024$ squares, as square shapes can be used to adapt to images of various aspect ratios with a compromise.\n\n3.2.2 Data Engine Towards Encoder Pre-training\n\nIn such an encoder pre-training stage, we use about 5M image-text pairs, including 3 M scene text OCR data and 2 M document OCR data. Their acquisition methods are as follows:\n\nFor the natural scene data, the English and Chinese images are sampled from Laion-2B [40] and Wukong [12] datasets, respectively. Then, the pseudo ground truth in these diverse real scenes is captured using PaddleOCR [10] tools. Overall, we obtain 2M dat with half in Chinese and half in English. For text ground truth, we perform two types of processing: 1) remove the bounding box and",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "In such an encoder pre-training stage, we use about 5M image-text pairs, including 3 M scene text OCR data and 2 M document OCR data.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b7ffede-c676-4324-b95c-e894fdeb26ef",
        "questions": "Does the GOT framework require modification of the vision encoder in Stage 3?",
        "answers": "No",
        "context": "Figure 2: The framework of the proposed GOT. Stage 1: We pre-train the vision encoder using a tiny OPT-125M to adapt the OCR tasks efficiently. Stage 2: GOT is built by connecting the vision encoder to Qwen-0.5B and sufficient OCR-2.0 knowledge of more general optical characters is used in this stage. Stage 3: No modification of the vision encoder is required, and GOT is customized to new character recognition features.\n\n\n3.2 Pre-train the OCR-earmarked Vision Encoder\n\n\nAs aforementioned, GOT enjoys the encoder-decoder structure. Inspired by the LVLMs design, the decoder can be initialized by a well-trained language model. However, we did not find a suitable pre-trained encoder for an OCR-2.0 model, so we must train one ourselves. We hope the new OCR encoder can work well on commonly used scene and document text recognition in various input shapes (both slices and whole pages).\n\n3.2.1 The Vision Encoder Generation.\n\nThe encoder structure we selected is VitDet [17] (base version with about 80M parameters) due to its local attention can greatly reduce the computational cost of high-resolution images. We follow the Vary-tiny setting [46] to design the last two layers of the encoder, which will transfer a $1024 \\times 1024 \\times 3$ input image to $256 \\times 1024$ image tokens. Then, these image tokens are projected into language model (OPT-125M [53]) dimension via a $1024 \\times 768$ linear layer. Unlike the Vary encoder which only focuses on a single document task under a relatively unitary input shape, we incorporated natural scenes and cropped slices during our pre-training. In the pre-processing stage, images of each shape are directly resized to $1024 \\times 1024$ squares, as square shapes can be used to adapt to images of various aspect ratios with a compromise.\n\n3.2.2 Data Engine Towards Encoder Pre-training\n\nIn such an encoder pre-training stage, we use about 5M image-text pairs, including 3 M scene text OCR data and 2 M document OCR data. Their acquisition methods are as follows:\n\nFor the natural scene data, the English and Chinese images are sampled from Laion-2B [40] and Wukong [12] datasets, respectively. Then, the pseudo ground truth in these diverse real scenes is captured using PaddleOCR [10] tools. Overall, we obtain 2M dat with half in Chinese and half in English. For text ground truth, we perform two types of processing: 1) remove the bounding box and",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Stage 3: No modification of the vision encoder is required, and GOT is customized to new character recognition features.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b83a6e8-fef5-4551-87a9-51855166465b",
        "questions": "Which dataset is described as the largest ever for document layout analysis?",
        "answers": "Publaynet",
        "context": "[42] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8317-8326 (2019) 3\n[43] Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with connectionist text proposal network. In: European conference on computer vision. pp. 56-72. Springer (2016) 4\n[44] Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.: Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140 (2016) 8\n[45] Wang, Y., Xie, H., Zha, Z.J., Xing, M., Fu, Z., Zhang, Y.: Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11753-11762 (2020) 4\n[46] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., Zhang, X.: Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109 (2023) 1, 4, 5, 6, 9\n[47] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yu, E., Sun, J., Han, C., Zhang, X.: Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024) 6,9\n[48] Xia, R., Zhang, B., Ye, H., Yan, X., Liu, Q., Zhou, H., Chen, Z., Dou, M., Shi, B., Yan, J., Qiao, Y.: Chartx \\& chartvlm: A versatile benchmark and foundation model for complicated chart reasoning (2024) 10\n[49] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J., et al.: mplugdocowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499 (2023) 1, 3, 4\n[50] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Xu, G., Li, C., Tian, J., Qian, Q., Zhang, J., et al.: Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126 (2023) 3, 4, 9\n[51] Zhang, C., Peng, G., Tao, Y., Fu, F., Jiang, W., Almpanidis, G., Chen, K.: Shopsign: A diverse scene text dataset of chinese shop signs in street views. arXiv preprint arXiv:1903.10412 (2019) 8\n[52] Zhang, S.X., Zhu, X., Yang, C., Wang, H., Yin, X.C.: Adaptive boundary proposal network for arbitrary shape text detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. $1305-1314$ (2021) 4\n[53] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) 5\n[54] Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International conference on document analysis and recognition (ICDAR). pp. 1015-1022. IEEE (2019) 4\n[55] Zhou, X., Yao, C., Wen, H., Wang, Y., Zhou, S., He, W., Liang, J.: East: An efficient and accurate scene text detector. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2017) 4",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis.",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b96eda5-e737-4ad2-97c6-dbbc77222ef6",
        "questions": "In which year was the paper 'Detecting text in natural image with connectionist text proposal network' presented at the European conference on computer vision?",
        "answers": "2016",
        "context": "[42] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8317-8326 (2019) 3\n[43] Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with connectionist text proposal network. In: European conference on computer vision. pp. 56-72. Springer (2016) 4\n[44] Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.: Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140 (2016) 8\n[45] Wang, Y., Xie, H., Zha, Z.J., Xing, M., Fu, Z., Zhang, Y.: Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11753-11762 (2020) 4\n[46] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., Zhang, X.: Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109 (2023) 1, 4, 5, 6, 9\n[47] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yu, E., Sun, J., Han, C., Zhang, X.: Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024) 6,9\n[48] Xia, R., Zhang, B., Ye, H., Yan, X., Liu, Q., Zhou, H., Chen, Z., Dou, M., Shi, B., Yan, J., Qiao, Y.: Chartx \\& chartvlm: A versatile benchmark and foundation model for complicated chart reasoning (2024) 10\n[49] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J., et al.: mplugdocowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499 (2023) 1, 3, 4\n[50] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Xu, G., Li, C., Tian, J., Qian, Q., Zhang, J., et al.: Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126 (2023) 3, 4, 9\n[51] Zhang, C., Peng, G., Tao, Y., Fu, F., Jiang, W., Almpanidis, G., Chen, K.: Shopsign: A diverse scene text dataset of chinese shop signs in street views. arXiv preprint arXiv:1903.10412 (2019) 8\n[52] Zhang, S.X., Zhu, X., Yang, C., Wang, H., Yin, X.C.: Adaptive boundary proposal network for arbitrary shape text detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. $1305-1314$ (2021) 4\n[53] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) 5\n[54] Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International conference on document analysis and recognition (ICDAR). pp. 1015-1022. IEEE (2019) 4\n[55] Zhou, X., Yao, C., Wen, H., Wang, Y., Zhou, S., He, W., Liang, J.: East: An efficient and accurate scene text detector. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2017) 4",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with connectionist text proposal network. In: European conference on computer vision. pp. 56-72. Springer (2016)",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b972d0f-2059-4d20-a66e-d3eeb37f54c5",
        "questions": "Is the paper 'Vary: Scaling up the vision vocabulary for large vision-language models' published in 2023?",
        "answers": "Yes",
        "context": "[42] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8317-8326 (2019) 3\n[43] Tian, Z., Huang, W., He, T., He, P., Qiao, Y.: Detecting text in natural image with connectionist text proposal network. In: European conference on computer vision. pp. 56-72. Springer (2016) 4\n[44] Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.: Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140 (2016) 8\n[45] Wang, Y., Xie, H., Zha, Z.J., Xing, M., Fu, Z., Zhang, Y.: Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11753-11762 (2020) 4\n[46] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., Zhang, X.: Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109 (2023) 1, 4, 5, 6, 9\n[47] Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yu, E., Sun, J., Han, C., Zhang, X.: Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 (2024) 6,9\n[48] Xia, R., Zhang, B., Ye, H., Yan, X., Liu, Q., Zhou, H., Chen, Z., Dou, M., Shi, B., Yan, J., Qiao, Y.: Chartx \\& chartvlm: A versatile benchmark and foundation model for complicated chart reasoning (2024) 10\n[49] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao, C., Xu, G., Li, C., Tian, J., et al.: mplugdocowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499 (2023) 1, 3, 4\n[50] Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Xu, G., Li, C., Tian, J., Qian, Q., Zhang, J., et al.: Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126 (2023) 3, 4, 9\n[51] Zhang, C., Peng, G., Tao, Y., Fu, F., Jiang, W., Almpanidis, G., Chen, K.: Shopsign: A diverse scene text dataset of chinese shop signs in street views. arXiv preprint arXiv:1903.10412 (2019) 8\n[52] Zhang, S.X., Zhu, X., Yang, C., Wang, H., Yin, X.C.: Adaptive boundary proposal network for arbitrary shape text detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. $1305-1314$ (2021) 4\n[53] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) 5\n[54] Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document layout analysis. In: 2019 International conference on document analysis and recognition (ICDAR). pp. 1015-1022. IEEE (2019) 4\n[55] Zhou, X., Yao, C., Wen, H., Wang, Y., Zhou, S., He, W., Liang, J.: East: An efficient and accurate scene text detector. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV) (2017) 4",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., Zhang, X.: Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109 (2023)",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b9b6968-9aa7-4bee-bdbb-adafa758fa63",
        "questions": "What is the main reason for using a pipeline scheme in traditional OCR systems, also known as OCR-1.0?",
        "answers": "The text recognition module (the OCR part) failed to scale up successfully, which can only deal with the image format of small slices.",
        "context": "2 Related Work\n\n\n2.1 Traditional OCR\n\nOptical Character Recognition (OCR) is a classic research topic that aims to convert the image's optical contents into an editable format for further downstream processing. Traditional OCR systems, called OCR-1.0, typically use a framework that is assembled from multiple expert modules. For instance, to handle diverse optical characters, the OCR system [10] is usually developed by integrating several domain expert networks, such as layout analysis [54], text detection [18, 19, 26, 30, 43, 45, $52,55]$, region extraction, and contents recognition $[11,14,16]$. The reason for using such a pipeline scheme is that the text recognition module (the OCR part) failed to scale up successfully, which can only deal with the image format of small slices, resulting in the entire OCR process being in the form of first detecting texts/cropping regions, and then recognizing the results within the slice. However, a system with complicated procedures may suffer potential systematic errors and high maintenance costs. Although some OCR-1.0 models, e.g., Nougat [6] can directly process documents at the whole page level, they are often designed and trained for a specific sub-task, leading to unsatisfactory general ability. In the OCR-1.0 era, one inconvenient thing is that we usually need to switch different models according to various OCR needs.\n\n2.2 LVLM-driven OCR\n\nLarge Vision-Language Models (LVLMs) [5, 9, 20, 24, 27, 46, 49] have attracted lots of attention in the AI-community due to their powerful generalization capabilities. For the current LVLMs owning perception-reasoning comprehensive capacity, the OCR ability has become a hot spot with the increasing demand for text-driven visual understanding. Most LVLMs' OCR capabilities come from the ready-made CLIP [37], especially those that freeze CLIP encoder [24] to complete the entire LVLM training. For such models, the vanilla CLIP, mainly with English scene text knowledge, is the bottleneck for the OCR performance to out-of-domain tasks, such as other languages or documents. Some other LVLMs [5, 49] choose to unfreeze the encoder and freeze the LLM for training to enhance the CLIP-encoder and align the image tokens to text ones. These models will face the problem of low optical character compression rate, as it is difficult for frozen LLM to decode too much text from an aligned image token. To alleviate this problem, some models [9, 27, 50] adopt a sliding window manner to decompose input images into smaller patches. Although this dynamic resolution approach is highly effective in processing high-resolution input images, e.g., PDF, it will result in excessive image tokens and limit the max length of the generated OCR result to some extent.\n\n3 General OCR Theory\n\nIn this work, we propose the general OCR theory, i.e., OCR-2.0 (as expounded in Section 1) to promote the development of the OCR field. Based on the proposed new theory, we present a novel OCR model (GOT). In this section, we will introduce the technical details of our model, including the framework, multi-stage training strategy, and the corresponding data engines.\n\n3.1 Framework\n\nAs illustrated in Figure 2, GOT comprises three modules, i.e., an image encoder, a linear layer, and an output decoder. The linear layer acts as the connector to map the channel dimension between the vision encoder and the language decoder. We utilize three main steps in optimizing the whole GOT model. First, we conduct the pure text recognition task to pre-train the vision encoder. To lift training efficiency and save GPU resources, we choose a tiny decoder to pass gradients to the encoder. In this stage, we feed images containing scene texts and manual images containing document-level characters into the model to allow the encoder to gather the two most commonly used characters' encoding abilities. In the next stage, we form the architecture of GOT by connecting the trained vision encoder to a new larger decoder. We prepare lots of more general OCR data (e.g., sheet music, math/molecular formulas, and geometric shapes) to scale up the OCR-2.0 knowledge for this stage. In the final stage, we intend to improve the generalization and applicability of GOT further. Specifically, fine-grained and muti-crop/page synthetic data are generated and added for GOT to support region prompt OCR [20], huge image OCR, and batched PDF OCR features.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The reason for using such a pipeline scheme is that the text recognition module (the OCR part) failed to scale up successfully, which can only deal with the image format of small slices, resulting in the entire OCR process being in the form of first detecting texts/cropping regions, and then recognizing the results within the slice.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0b9d7787-83cb-49e1-b964-9935cbe13ee9",
        "questions": "Does the OCR-2.0 model, GOT, include a linear layer as part of its framework?",
        "answers": "Yes",
        "context": "2 Related Work\n\n\n2.1 Traditional OCR\n\nOptical Character Recognition (OCR) is a classic research topic that aims to convert the image's optical contents into an editable format for further downstream processing. Traditional OCR systems, called OCR-1.0, typically use a framework that is assembled from multiple expert modules. For instance, to handle diverse optical characters, the OCR system [10] is usually developed by integrating several domain expert networks, such as layout analysis [54], text detection [18, 19, 26, 30, 43, 45, $52,55]$, region extraction, and contents recognition $[11,14,16]$. The reason for using such a pipeline scheme is that the text recognition module (the OCR part) failed to scale up successfully, which can only deal with the image format of small slices, resulting in the entire OCR process being in the form of first detecting texts/cropping regions, and then recognizing the results within the slice. However, a system with complicated procedures may suffer potential systematic errors and high maintenance costs. Although some OCR-1.0 models, e.g., Nougat [6] can directly process documents at the whole page level, they are often designed and trained for a specific sub-task, leading to unsatisfactory general ability. In the OCR-1.0 era, one inconvenient thing is that we usually need to switch different models according to various OCR needs.\n\n2.2 LVLM-driven OCR\n\nLarge Vision-Language Models (LVLMs) [5, 9, 20, 24, 27, 46, 49] have attracted lots of attention in the AI-community due to their powerful generalization capabilities. For the current LVLMs owning perception-reasoning comprehensive capacity, the OCR ability has become a hot spot with the increasing demand for text-driven visual understanding. Most LVLMs' OCR capabilities come from the ready-made CLIP [37], especially those that freeze CLIP encoder [24] to complete the entire LVLM training. For such models, the vanilla CLIP, mainly with English scene text knowledge, is the bottleneck for the OCR performance to out-of-domain tasks, such as other languages or documents. Some other LVLMs [5, 49] choose to unfreeze the encoder and freeze the LLM for training to enhance the CLIP-encoder and align the image tokens to text ones. These models will face the problem of low optical character compression rate, as it is difficult for frozen LLM to decode too much text from an aligned image token. To alleviate this problem, some models [9, 27, 50] adopt a sliding window manner to decompose input images into smaller patches. Although this dynamic resolution approach is highly effective in processing high-resolution input images, e.g., PDF, it will result in excessive image tokens and limit the max length of the generated OCR result to some extent.\n\n3 General OCR Theory\n\nIn this work, we propose the general OCR theory, i.e., OCR-2.0 (as expounded in Section 1) to promote the development of the OCR field. Based on the proposed new theory, we present a novel OCR model (GOT). In this section, we will introduce the technical details of our model, including the framework, multi-stage training strategy, and the corresponding data engines.\n\n3.1 Framework\n\nAs illustrated in Figure 2, GOT comprises three modules, i.e., an image encoder, a linear layer, and an output decoder. The linear layer acts as the connector to map the channel dimension between the vision encoder and the language decoder. We utilize three main steps in optimizing the whole GOT model. First, we conduct the pure text recognition task to pre-train the vision encoder. To lift training efficiency and save GPU resources, we choose a tiny decoder to pass gradients to the encoder. In this stage, we feed images containing scene texts and manual images containing document-level characters into the model to allow the encoder to gather the two most commonly used characters' encoding abilities. In the next stage, we form the architecture of GOT by connecting the trained vision encoder to a new larger decoder. We prepare lots of more general OCR data (e.g., sheet music, math/molecular formulas, and geometric shapes) to scale up the OCR-2.0 knowledge for this stage. In the final stage, we intend to improve the generalization and applicability of GOT further. Specifically, fine-grained and muti-crop/page synthetic data are generated and added for GOT to support region prompt OCR [20], huge image OCR, and batched PDF OCR features.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "As illustrated in Figure 2, GOT comprises three modules, i.e., an image encoder, a linear layer, and an output decoder.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0ba04c24-970a-4251-bc1f-35ebc15b9b39",
        "questions": "What is one of the challenges faced by LVLMs that freeze the CLIP encoder for OCR tasks?",
        "answers": "The bottleneck for the OCR performance to out-of-domain tasks, such as other languages or documents.",
        "context": "2 Related Work\n\n\n2.1 Traditional OCR\n\nOptical Character Recognition (OCR) is a classic research topic that aims to convert the image's optical contents into an editable format for further downstream processing. Traditional OCR systems, called OCR-1.0, typically use a framework that is assembled from multiple expert modules. For instance, to handle diverse optical characters, the OCR system [10] is usually developed by integrating several domain expert networks, such as layout analysis [54], text detection [18, 19, 26, 30, 43, 45, $52,55]$, region extraction, and contents recognition $[11,14,16]$. The reason for using such a pipeline scheme is that the text recognition module (the OCR part) failed to scale up successfully, which can only deal with the image format of small slices, resulting in the entire OCR process being in the form of first detecting texts/cropping regions, and then recognizing the results within the slice. However, a system with complicated procedures may suffer potential systematic errors and high maintenance costs. Although some OCR-1.0 models, e.g., Nougat [6] can directly process documents at the whole page level, they are often designed and trained for a specific sub-task, leading to unsatisfactory general ability. In the OCR-1.0 era, one inconvenient thing is that we usually need to switch different models according to various OCR needs.\n\n2.2 LVLM-driven OCR\n\nLarge Vision-Language Models (LVLMs) [5, 9, 20, 24, 27, 46, 49] have attracted lots of attention in the AI-community due to their powerful generalization capabilities. For the current LVLMs owning perception-reasoning comprehensive capacity, the OCR ability has become a hot spot with the increasing demand for text-driven visual understanding. Most LVLMs' OCR capabilities come from the ready-made CLIP [37], especially those that freeze CLIP encoder [24] to complete the entire LVLM training. For such models, the vanilla CLIP, mainly with English scene text knowledge, is the bottleneck for the OCR performance to out-of-domain tasks, such as other languages or documents. Some other LVLMs [5, 49] choose to unfreeze the encoder and freeze the LLM for training to enhance the CLIP-encoder and align the image tokens to text ones. These models will face the problem of low optical character compression rate, as it is difficult for frozen LLM to decode too much text from an aligned image token. To alleviate this problem, some models [9, 27, 50] adopt a sliding window manner to decompose input images into smaller patches. Although this dynamic resolution approach is highly effective in processing high-resolution input images, e.g., PDF, it will result in excessive image tokens and limit the max length of the generated OCR result to some extent.\n\n3 General OCR Theory\n\nIn this work, we propose the general OCR theory, i.e., OCR-2.0 (as expounded in Section 1) to promote the development of the OCR field. Based on the proposed new theory, we present a novel OCR model (GOT). In this section, we will introduce the technical details of our model, including the framework, multi-stage training strategy, and the corresponding data engines.\n\n3.1 Framework\n\nAs illustrated in Figure 2, GOT comprises three modules, i.e., an image encoder, a linear layer, and an output decoder. The linear layer acts as the connector to map the channel dimension between the vision encoder and the language decoder. We utilize three main steps in optimizing the whole GOT model. First, we conduct the pure text recognition task to pre-train the vision encoder. To lift training efficiency and save GPU resources, we choose a tiny decoder to pass gradients to the encoder. In this stage, we feed images containing scene texts and manual images containing document-level characters into the model to allow the encoder to gather the two most commonly used characters' encoding abilities. In the next stage, we form the architecture of GOT by connecting the trained vision encoder to a new larger decoder. We prepare lots of more general OCR data (e.g., sheet music, math/molecular formulas, and geometric shapes) to scale up the OCR-2.0 knowledge for this stage. In the final stage, we intend to improve the generalization and applicability of GOT further. Specifically, fine-grained and muti-crop/page synthetic data are generated and added for GOT to support region prompt OCR [20], huge image OCR, and batched PDF OCR features.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For such models, the vanilla CLIP, mainly with English scene text knowledge, is the bottleneck for the OCR performance to out-of-domain tasks, such as other languages or documents.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0babd726-0932-43d4-b738-0d2837a5b39e",
        "questions": "What technology is adapted to the GOT model to handle high-resolution images in dual-page paper reading mode?",
        "answers": "dynamic resolution technology",
        "context": "Prompt: OCR with format upon the patch reference:\n\n\nOutput:\n\n\nFigure 7: Dynamic resolution of GOT for high-resolution images. In the dual-page paper reading mode shown in the figure (from [21]), the input resolution of the original GOT is not sufficient to handle it. Therefore, we adapt dynamic resolution technology to make the model no longer limited to the size of the image.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Therefore, we adapt dynamic resolution technology to make the model no longer limited to the size of the image.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bb4a483-e9b6-45fc-9b4f-a21f12ce40a3",
        "questions": "Is the original GOT model sufficient to handle high-resolution images in dual-page paper reading mode?",
        "answers": "No",
        "context": "Prompt: OCR with format upon the patch reference:\n\n\nOutput:\n\n\nFigure 7: Dynamic resolution of GOT for high-resolution images. In the dual-page paper reading mode shown in the figure (from [21]), the input resolution of the original GOT is not sufficient to handle it. Therefore, we adapt dynamic resolution technology to make the model no longer limited to the size of the image.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "In the dual-page paper reading mode shown in the figure (from [21]), the input resolution of the original GOT is not sufficient to handle it.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bba6c1b-aac4-4ad9-bd6e-53f2969a8b69",
        "questions": "What is the purpose of adapting dynamic resolution technology to the GOT model?",
        "answers": "To make the model no longer limited to the size of the image.",
        "context": "Prompt: OCR with format upon the patch reference:\n\n\nOutput:\n\n\nFigure 7: Dynamic resolution of GOT for high-resolution images. In the dual-page paper reading mode shown in the figure (from [21]), the input resolution of the original GOT is not sufficient to handle it. Therefore, we adapt dynamic resolution technology to make the model no longer limited to the size of the image.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Therefore, we adapt dynamic resolution technology to make the model no longer limited to the size of the image.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bbc5bf5-1ea8-45c3-b33b-de6e8cf37c70",
        "questions": "What is the parameter size of the GOT model proposed in the General OCR Theory paper?",
        "answers": "580 M",
        "context": "(5) \\\\ General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\n\n\n\nHaoran Wei ${ ^{1, *}$, Chenglong Liu ${ }^{3, *}$, Jinyue Chen ${ }^{3}$, Jia Wang ${ }^{1}$, Lingyu Kong ${ }^{3}$, Yanming Xu ${ }^{1}$, Zheng Ge ${ }^{1}$, Liang Zhao ${ }^{1}$, Jianjian Sun ${ }^{1}$, Yuang Peng ${ }^{4}$, Chunrui Han ${ }^{2}$, Xiangyu Zhang ${ }^{1,2}$ \\\\ ${ }^{1}$ StepFun ${ }^{2}$ Megvii Technology \\\\ ${ }^{3}$ University of Chinese Academy of Sciences ${ }^{4}$ Tsinghua University \\\\ https://github.com/Ucas-HaoranWei/GOT-OCR2.0\n}\n\n\nTraditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580 M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.\n\n\n1 Introduction\n\nOptical Character Recognition (OCR) is a widely used technology that extracts the characters embedded in an optical image into an editable format. Typical OCR systems [10] in the OCR-1.0 era are mainly designed based on a multi-modular pipeline style, commonly including element detection, region cropping, and character recognition parts. Each module is prone to falling into local optima, making the whole system incur high maintenance costs. Moreover, traditional OCR methods have insufficient general ability, reflected as different OCR-1.0 networks usually designed for different sub-tasks. Nevertheless, choosing a suitable one from diverse OCR models for a special task is always inconvenient for users.\nIn the past year, Large Vision Language models (LVLMs) [5, 9, 24, 27, 36, 46, 49] have developed rapidly and showcased impressive performance. As a highly anticipated ability, the OCR performance of current LVLMs is continuously improving. Based on CLIP [37], LLaVA [24] naturally acquires the English OCR ability after the instruct tuning phase. To lift the OCR accuracy and support other languages, e.g., Chinese, Qwen-VL [5] unfreezes its image encoder (a CLIP-G) and uses lots of OCR data in its stage-two training. Innovatively, Vary [46] generates a new high-resolution OCR vision vocabulary paralleling the CLIP branch to deal with document-level dense OCR. By contrast,\n\n\\footnotetext{\n*Equal contribution\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The GOT, with 580 M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bc6174e-d9cd-4b91-97e7-ea726a65113d",
        "questions": "Does the GOT model support interactive OCR features such as region-level recognition guided by coordinates or colors?",
        "answers": "Yes",
        "context": "(5) \\\\ General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\n\n\n\nHaoran Wei ${ ^{1, *}$, Chenglong Liu ${ }^{3, *}$, Jinyue Chen ${ }^{3}$, Jia Wang ${ }^{1}$, Lingyu Kong ${ }^{3}$, Yanming Xu ${ }^{1}$, Zheng Ge ${ }^{1}$, Liang Zhao ${ }^{1}$, Jianjian Sun ${ }^{1}$, Yuang Peng ${ }^{4}$, Chunrui Han ${ }^{2}$, Xiangyu Zhang ${ }^{1,2}$ \\\\ ${ }^{1}$ StepFun ${ }^{2}$ Megvii Technology \\\\ ${ }^{3}$ University of Chinese Academy of Sciences ${ }^{4}$ Tsinghua University \\\\ https://github.com/Ucas-HaoranWei/GOT-OCR2.0\n}\n\n\nTraditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580 M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.\n\n\n1 Introduction\n\nOptical Character Recognition (OCR) is a widely used technology that extracts the characters embedded in an optical image into an editable format. Typical OCR systems [10] in the OCR-1.0 era are mainly designed based on a multi-modular pipeline style, commonly including element detection, region cropping, and character recognition parts. Each module is prone to falling into local optima, making the whole system incur high maintenance costs. Moreover, traditional OCR methods have insufficient general ability, reflected as different OCR-1.0 networks usually designed for different sub-tasks. Nevertheless, choosing a suitable one from diverse OCR models for a special task is always inconvenient for users.\nIn the past year, Large Vision Language models (LVLMs) [5, 9, 24, 27, 36, 46, 49] have developed rapidly and showcased impressive performance. As a highly anticipated ability, the OCR performance of current LVLMs is continuously improving. Based on CLIP [37], LLaVA [24] naturally acquires the English OCR ability after the instruct tuning phase. To lift the OCR accuracy and support other languages, e.g., Chinese, Qwen-VL [5] unfreezes its image encoder (a CLIP-G) and uses lots of OCR data in its stage-two training. Innovatively, Vary [46] generates a new high-resolution OCR vision vocabulary paralleling the CLIP branch to deal with document-level dense OCR. By contrast,\n\n\\footnotetext{\n*Equal contribution\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bd357e1-d6bf-4828-9798-6708bb3dffe1",
        "questions": "What is one of the technologies adapted to the GOT model to enhance its practicality, as mentioned in the General OCR Theory paper?",
        "answers": "dynamic resolution and multipage OCR technologies",
        "context": "(5) \\\\ General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\n\n\n\nHaoran Wei ${ ^{1, *}$, Chenglong Liu ${ }^{3, *}$, Jinyue Chen ${ }^{3}$, Jia Wang ${ }^{1}$, Lingyu Kong ${ }^{3}$, Yanming Xu ${ }^{1}$, Zheng Ge ${ }^{1}$, Liang Zhao ${ }^{1}$, Jianjian Sun ${ }^{1}$, Yuang Peng ${ }^{4}$, Chunrui Han ${ }^{2}$, Xiangyu Zhang ${ }^{1,2}$ \\\\ ${ }^{1}$ StepFun ${ }^{2}$ Megvii Technology \\\\ ${ }^{3}$ University of Chinese Academy of Sciences ${ }^{4}$ Tsinghua University \\\\ https://github.com/Ucas-HaoranWei/GOT-OCR2.0\n}\n\n\nTraditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580 M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.\n\n\n1 Introduction\n\nOptical Character Recognition (OCR) is a widely used technology that extracts the characters embedded in an optical image into an editable format. Typical OCR systems [10] in the OCR-1.0 era are mainly designed based on a multi-modular pipeline style, commonly including element detection, region cropping, and character recognition parts. Each module is prone to falling into local optima, making the whole system incur high maintenance costs. Moreover, traditional OCR methods have insufficient general ability, reflected as different OCR-1.0 networks usually designed for different sub-tasks. Nevertheless, choosing a suitable one from diverse OCR models for a special task is always inconvenient for users.\nIn the past year, Large Vision Language models (LVLMs) [5, 9, 24, 27, 36, 46, 49] have developed rapidly and showcased impressive performance. As a highly anticipated ability, the OCR performance of current LVLMs is continuously improving. Based on CLIP [37], LLaVA [24] naturally acquires the English OCR ability after the instruct tuning phase. To lift the OCR accuracy and support other languages, e.g., Chinese, Qwen-VL [5] unfreezes its image encoder (a CLIP-G) and uses lots of OCR data in its stage-two training. Innovatively, Vary [46] generates a new high-resolution OCR vision vocabulary paralleling the CLIP branch to deal with document-level dense OCR. By contrast,\n\n\\footnotetext{\n*Equal contribution\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Furthermore, we also adapt dynamic resolution and multipage OCR technologies to GOT for better practicality.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bd89c73-95c1-4122-8f9e-89ef7bc1ad83",
        "questions": "What is the F1-score for the 'All text' type in the multi-crop strategy for Markdown documents?",
        "answers": "0.953",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bde3249-cd06-43c9-9149-fe6377f141cd",
        "questions": "Which model achieved the highest AP@high score in the PlotQA-SE benchmark?",
        "answers": "GOT",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & \\mathbf{0 . 6 4 0} \\",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bded216-0863-4195-a960-a9f6b395bc69",
        "questions": "Does the GOT model perform better than Fox on the color-based referential OCR tasks in Chinese?",
        "answers": "Yes",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "& color & 0.114 & 0.040 \\",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0be1a657-606a-432b-8987-7942eacef95f",
        "questions": "What is the name of the model that can handle multi-page document OCR tasks?",
        "answers": "GOT",
        "context": "Prompt: OCR with format across multi pages:\n\n\nFigure 8: Multi-page (document) OCR ability of GOT. With this feature, researchers can continue to train the GOT with multi-page PDF-text pairs, such as Arxiv paper with .tex file.\n\nPrompt: OCR with format:\n\n\n\n\n\nOutput:\n\n\n\nFigure 9: More general OCR results. GOT can process molecular formulas, sheet music, and charts.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Figure 8: Multi-page (document) OCR ability of GOT.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0be31109-a185-446d-9be9-d91857a798d8",
        "questions": "Can the GOT model process molecular formulas, sheet music, and charts?",
        "answers": "Yes",
        "context": "Prompt: OCR with format across multi pages:\n\n\nFigure 8: Multi-page (document) OCR ability of GOT. With this feature, researchers can continue to train the GOT with multi-page PDF-text pairs, such as Arxiv paper with .tex file.\n\nPrompt: OCR with format:\n\n\n\n\n\nOutput:\n\n\n\nFigure 9: More general OCR results. GOT can process molecular formulas, sheet music, and charts.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Figure 9: More general OCR results. GOT can process molecular formulas, sheet music, and charts.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bef89ce-1e0c-4de7-9cfc-3591349b64a9",
        "questions": "What type of document pairs can researchers use to train the GOT model for multi-page OCR tasks?",
        "answers": "multi-page PDF-text pairs, such as Arxiv paper with .tex file",
        "context": "Prompt: OCR with format across multi pages:\n\n\nFigure 8: Multi-page (document) OCR ability of GOT. With this feature, researchers can continue to train the GOT with multi-page PDF-text pairs, such as Arxiv paper with .tex file.\n\nPrompt: OCR with format:\n\n\n\n\n\nOutput:\n\n\n\nFigure 9: More general OCR results. GOT can process molecular formulas, sheet music, and charts.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "With this feature, researchers can continue to train the GOT with multi-page PDF-text pairs, such as Arxiv paper with .tex file.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bf3d445-f87f-49b8-8b53-d1cdc35cd4c3",
        "questions": "What is the name of the shop that will open on March 29, 2019, inside the Azabu Tailor Square Futako Tamagawa store?",
        "answers": "Trading Post WHITE LABEL Futako Tamagawa store",
        "context": "Prompt: OCR with format:\n\n\nOutput:\n\nNEWS RELEASE\n```\n2019\u5e743\u670822\u65e5\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8\n```\n\n\u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30ba\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u30aa\u30fc\u30d7\u30f3!!\n\u30c8\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30b9\u30c8\u30db\u30ef\u30a4\u30c8\u30ec\u30fc\u30d9\u30eb\nTrading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u30aa\u30fc\u30d6\u30f3\u306e\u304a\u77e5\u3089\u305b\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7d43\u5f79\u793e\u9577: \u5800\u6c5f\u6cf0\u6587)\u306e\u5b50\u4f1a\u793e\u3068\u306a\u308b\u682a\u5f0f\u4f1a\u793e \u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30b9(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7de0\u5f79\u793e\u9577:\u6b66\u4e95\u79c0\u559c)\u306f\u3001Trading Post \u306e\u4e03 \u30f3\u30f3\u30c9\u30e9\u30a4\u30f3 Trading Post WHITE LABEL \u306e\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\n\u30d7 Trading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u300d\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u4e8c\u5b50\u7389\u5ddd\u5e97\u5185\u306b2019\u5e743\u670829\u65e5(\u91d1)\u306b\u30aa\u30fc\u30d6\u30f3\u3044\u305f\u3057\u307e\u3059\u3002\n\nTrading Post WHITE LABEL \u4e8c\u5b50\u7389\u5ddd\u5e97\n\u3053\u308c\u307e\u3067\u30d1\u30fc\u30bd\u30ca\u30eb\u30aa\u30fc\u30c0\u30fc\u306e\u30b9\u30fc\u30c4\u3084\u30b7\u30e3\u30c3\u306a\u3069\u3067\u4eba\u6c17\u306e\u30d3\u30b8\u30cd\u30b9\u30a6\u30a7\u30a2\u306e\u30bb\u30ec\u30af\u30c8\u30b7\u30e7\u30c3 \u30d7\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u306b\u3066\u671f\u9593\u9650\u5b9a\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a4\u30d9\u30f3\u30c8\u3092\u884c\u3063\u3066\u307e\u3044\u308a\u307e\u3057\u305f\u304c\u3001\u3053\u306e\u5ea6\u30c8\u30ec\u30fc\u30c7 \u30a4\u30f3\u30b0\u30dd\u30b9\u30c8\u521d\u3068\u306a\u308b\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u5b50\u7389\u5ddd\u5e97\u306b\u30aa\u30fc\u30d6\u30f3\u3059\u308b\u3053\u3068\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u3053\u3060\u308f\u308a\u3092\u6301\u3061\u3001\u672c\u7269\u3092\u6c42\u3081\u308b\u304a\u5ba2\u3055\u307e\u3078Trading Post \u30aa\u30ea\u30b8\u30ca\u30eb\u30a2\u30a4\u30c6\u30e0\u3092\u4e2d\u5fc3\u306b\u4e0a\u8cea\u3067\u672c\u7269\u306e\u3053\u3060\u308f\u308a\u30a2\u30a4\u30c6\u30e0\u3092\u56fd\u5185\u5916\u304b\u3089\u30bb\u30ec\u30af\u30c8\u3002\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u3001Trading Post WHITE LABEL\u306e\u591a\u5f69\u306a\u5546\u54c1\u5c55\u958b\u3084\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u306e\u63a5\u5ba2\u30b5\u30fc\u30d3\u30b9\u3092\u901a\u3058\u3001\u304a\u5ba2\u3055\n\u3078\u3001\u3088\u308a\u30a2\u6e80\u8db3\u3044\u305f\u3060\u3051\u308b\u30c8\u30fc\u30bf\u30eb\u30b3\u30fc\u30c7\u30a3\u30cd\u30fc\u30c8\u3092\u3054\u63d0\u6848\u81f4\u3057\u307e\u3059\u3002\n(\u4e3b\u306a\u53d6\u308a\u6271\u3044\u3065\u30e9\u30f3\u30c9)\nTrading Post\u3001Soffice\\&Solid, CARMINA\u3001Tricker's, Allen Edmonds, etc\n- No es un pavo, por cierto - dijo la pata-.\n\nFijense en la elegancia con que nada, y en lo derecho que se mantiene. Sin duda que es uno de mis peque\u00f1itos. Y si uno lo mira bien, se da cuenta pronto de que es realmente muy guapo. \u00a1Cuac, cuac! Vamos, vengan conmigo y d\u00e9jenme ense\u00f1arles el mundo y presentarlos al corral entero. Pero no se separen mucho de m\u00ed, no sea que los pisoteen. Y anden con los ojos muy abiertos, por si viene el gato.\nY con esto se encaminaron al corral. Hab\u00eda all\u00ed un escandalo espantoso, pues dos familias se estaban\n\nFigure 10: We do not specifically introduce additional OCR capabilities for GOT other than Chinese and English. Yet the PDF data we crawled may contain a small amount of text in other languages, leading to the GOT seeming to have the ability to recognize other languages. However, we cannot guarantee the OCR quality of other languages. Therefore, we recommend fine-tuning the model with corresponding data if this feature is needed.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Trading Post WHITE LABEL \u306e\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30d7 Trading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u300d\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u4e8c\u5b50\u7389\u5ddd\u5e97\u5185\u306b2019\u5e743\u670829\u65e5(\u91d1)\u306b\u30aa\u30fc\u30d6\u30f3\u3044\u305f\u3057\u307e\u3059\u3002",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0bf4dac5-5a35-4b4d-9781-7a56697bb880",
        "questions": "Who is the representative director of Prestige Shoes Co., Ltd., a subsidiary of G-Foot Co., Ltd.?",
        "answers": "\u6b66\u4e95\u79c0\u559c",
        "context": "Prompt: OCR with format:\n\n\nOutput:\n\nNEWS RELEASE\n```\n2019\u5e743\u670822\u65e5\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8\n```\n\n\u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30ba\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u30aa\u30fc\u30d7\u30f3!!\n\u30c8\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30b9\u30c8\u30db\u30ef\u30a4\u30c8\u30ec\u30fc\u30d9\u30eb\nTrading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u30aa\u30fc\u30d6\u30f3\u306e\u304a\u77e5\u3089\u305b\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7d43\u5f79\u793e\u9577: \u5800\u6c5f\u6cf0\u6587)\u306e\u5b50\u4f1a\u793e\u3068\u306a\u308b\u682a\u5f0f\u4f1a\u793e \u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30b9(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7de0\u5f79\u793e\u9577:\u6b66\u4e95\u79c0\u559c)\u306f\u3001Trading Post \u306e\u4e03 \u30f3\u30f3\u30c9\u30e9\u30a4\u30f3 Trading Post WHITE LABEL \u306e\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\n\u30d7 Trading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u300d\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u4e8c\u5b50\u7389\u5ddd\u5e97\u5185\u306b2019\u5e743\u670829\u65e5(\u91d1)\u306b\u30aa\u30fc\u30d6\u30f3\u3044\u305f\u3057\u307e\u3059\u3002\n\nTrading Post WHITE LABEL \u4e8c\u5b50\u7389\u5ddd\u5e97\n\u3053\u308c\u307e\u3067\u30d1\u30fc\u30bd\u30ca\u30eb\u30aa\u30fc\u30c0\u30fc\u306e\u30b9\u30fc\u30c4\u3084\u30b7\u30e3\u30c3\u306a\u3069\u3067\u4eba\u6c17\u306e\u30d3\u30b8\u30cd\u30b9\u30a6\u30a7\u30a2\u306e\u30bb\u30ec\u30af\u30c8\u30b7\u30e7\u30c3 \u30d7\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u306b\u3066\u671f\u9593\u9650\u5b9a\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a4\u30d9\u30f3\u30c8\u3092\u884c\u3063\u3066\u307e\u3044\u308a\u307e\u3057\u305f\u304c\u3001\u3053\u306e\u5ea6\u30c8\u30ec\u30fc\u30c7 \u30a4\u30f3\u30b0\u30dd\u30b9\u30c8\u521d\u3068\u306a\u308b\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u5b50\u7389\u5ddd\u5e97\u306b\u30aa\u30fc\u30d6\u30f3\u3059\u308b\u3053\u3068\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u3053\u3060\u308f\u308a\u3092\u6301\u3061\u3001\u672c\u7269\u3092\u6c42\u3081\u308b\u304a\u5ba2\u3055\u307e\u3078Trading Post \u30aa\u30ea\u30b8\u30ca\u30eb\u30a2\u30a4\u30c6\u30e0\u3092\u4e2d\u5fc3\u306b\u4e0a\u8cea\u3067\u672c\u7269\u306e\u3053\u3060\u308f\u308a\u30a2\u30a4\u30c6\u30e0\u3092\u56fd\u5185\u5916\u304b\u3089\u30bb\u30ec\u30af\u30c8\u3002\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u3001Trading Post WHITE LABEL\u306e\u591a\u5f69\u306a\u5546\u54c1\u5c55\u958b\u3084\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u306e\u63a5\u5ba2\u30b5\u30fc\u30d3\u30b9\u3092\u901a\u3058\u3001\u304a\u5ba2\u3055\n\u3078\u3001\u3088\u308a\u30a2\u6e80\u8db3\u3044\u305f\u3060\u3051\u308b\u30c8\u30fc\u30bf\u30eb\u30b3\u30fc\u30c7\u30a3\u30cd\u30fc\u30c8\u3092\u3054\u63d0\u6848\u81f4\u3057\u307e\u3059\u3002\n(\u4e3b\u306a\u53d6\u308a\u6271\u3044\u3065\u30e9\u30f3\u30c9)\nTrading Post\u3001Soffice\\&Solid, CARMINA\u3001Tricker's, Allen Edmonds, etc\n- No es un pavo, por cierto - dijo la pata-.\n\nFijense en la elegancia con que nada, y en lo derecho que se mantiene. Sin duda que es uno de mis peque\u00f1itos. Y si uno lo mira bien, se da cuenta pronto de que es realmente muy guapo. \u00a1Cuac, cuac! Vamos, vengan conmigo y d\u00e9jenme ense\u00f1arles el mundo y presentarlos al corral entero. Pero no se separen mucho de m\u00ed, no sea que los pisoteen. Y anden con los ojos muy abiertos, por si viene el gato.\nY con esto se encaminaron al corral. Hab\u00eda all\u00ed un escandalo espantoso, pues dos familias se estaban\n\nFigure 10: We do not specifically introduce additional OCR capabilities for GOT other than Chinese and English. Yet the PDF data we crawled may contain a small amount of text in other languages, leading to the GOT seeming to have the ability to recognize other languages. However, we cannot guarantee the OCR quality of other languages. Therefore, we recommend fine-tuning the model with corresponding data if this feature is needed.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7d43\u5f79\u793e\u9577: \u5800\u6c5f\u6cf0\u6587)\u306e\u5b50\u4f1a\u793e\u3068\u306a\u308b\u682a\u5f0f\u4f1a\u793e \u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30b9(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7de0\u5f79\u793e\u9577:\u6b66\u4e95\u79c0\u559c)\u306f\u3001",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c0e4763-d2b2-447a-8fcf-d418da7f22c1",
        "questions": "Does the GOT model have specific OCR capabilities for languages other than Chinese and English?",
        "answers": "No",
        "context": "Prompt: OCR with format:\n\n\nOutput:\n\nNEWS RELEASE\n```\n2019\u5e743\u670822\u65e5\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8\n```\n\n\u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30ba\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u30aa\u30fc\u30d7\u30f3!!\n\u30c8\u30ec\u30fc\u30c6\u30a3\u30f3\u30b0\u30dd\u30b9\u30c8\u30db\u30ef\u30a4\u30c8\u30ec\u30fc\u30d9\u30eb\nTrading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u30aa\u30fc\u30d6\u30f3\u306e\u304a\u77e5\u3089\u305b\n\u682a\u5f0f\u4f1a\u793e\u30b8\u30fc\u30d5\u30c3\u30c8(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7d43\u5f79\u793e\u9577: \u5800\u6c5f\u6cf0\u6587)\u306e\u5b50\u4f1a\u793e\u3068\u306a\u308b\u682a\u5f0f\u4f1a\u793e \u30d7\u30ec\u30b9\u30c6\u30fc\u30b8\u30b7\u30e5\u30fc\u30b9(\u6771\u4eac\u90fd\u4e2d\u592e\u533a\u65b0\u5ddd\u3001\u4ee3\u8868\u53d6\u7de0\u5f79\u793e\u9577:\u6b66\u4e95\u79c0\u559c)\u306f\u3001Trading Post \u306e\u4e03 \u30f3\u30f3\u30c9\u30e9\u30a4\u30f3 Trading Post WHITE LABEL \u306e\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\n\u30d7 Trading Post WHITE LABEL\u4e8c\u5b50\u7389\u5ddd\u5e97\u300d\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u4e8c\u5b50\u7389\u5ddd\u5e97\u5185\u306b2019\u5e743\u670829\u65e5(\u91d1)\u306b\u30aa\u30fc\u30d6\u30f3\u3044\u305f\u3057\u307e\u3059\u3002\n\nTrading Post WHITE LABEL \u4e8c\u5b50\u7389\u5ddd\u5e97\n\u3053\u308c\u307e\u3067\u30d1\u30fc\u30bd\u30ca\u30eb\u30aa\u30fc\u30c0\u30fc\u306e\u30b9\u30fc\u30c4\u3084\u30b7\u30e3\u30c3\u306a\u3069\u3067\u4eba\u6c17\u306e\u30d3\u30b8\u30cd\u30b9\u30a6\u30a7\u30a2\u306e\u30bb\u30ec\u30af\u30c8\u30b7\u30e7\u30c3 \u30d7\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u306b\u3066\u671f\u9593\u9650\u5b9a\u30dd\u30c3\u30d7\u30a2\u30c3\u30d7\u30a4\u30d9\u30f3\u30c8\u3092\u884c\u3063\u3066\u307e\u3044\u308a\u307e\u3057\u305f\u304c\u3001\u3053\u306e\u5ea6\u30c8\u30ec\u30fc\u30c7 \u30a4\u30f3\u30b0\u30dd\u30b9\u30c8\u521d\u3068\u306a\u308b\u30b7\u30e7\u30c3\u30d7\u30a4\u30f3\u30b7\u30e7\u30c3\u30d7\u3092\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u30b9\u30af\u30a8\u30a2\u5b50\u7389\u5ddd\u5e97\u306b\u30aa\u30fc\u30d6\u30f3\u3059\u308b\u3053\u3068\u3068\u306a\u308a\u307e\u3057\u305f\u3002\n\u3053\u3060\u308f\u308a\u3092\u6301\u3061\u3001\u672c\u7269\u3092\u6c42\u3081\u308b\u304a\u5ba2\u3055\u307e\u3078Trading Post \u30aa\u30ea\u30b8\u30ca\u30eb\u30a2\u30a4\u30c6\u30e0\u3092\u4e2d\u5fc3\u306b\u4e0a\u8cea\u3067\u672c\u7269\u306e\u3053\u3060\u308f\u308a\u30a2\u30a4\u30c6\u30e0\u3092\u56fd\u5185\u5916\u304b\u3089\u30bb\u30ec\u30af\u30c8\u3002\u9ebb\u5e03\u30c6\u30fc\u30e9\u30fc\u3001Trading Post WHITE LABEL\u306e\u591a\u5f69\u306a\u5546\u54c1\u5c55\u958b\u3084\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u306a\u3069\u306e\u63a5\u5ba2\u30b5\u30fc\u30d3\u30b9\u3092\u901a\u3058\u3001\u304a\u5ba2\u3055\n\u3078\u3001\u3088\u308a\u30a2\u6e80\u8db3\u3044\u305f\u3060\u3051\u308b\u30c8\u30fc\u30bf\u30eb\u30b3\u30fc\u30c7\u30a3\u30cd\u30fc\u30c8\u3092\u3054\u63d0\u6848\u81f4\u3057\u307e\u3059\u3002\n(\u4e3b\u306a\u53d6\u308a\u6271\u3044\u3065\u30e9\u30f3\u30c9)\nTrading Post\u3001Soffice\\&Solid, CARMINA\u3001Tricker's, Allen Edmonds, etc\n- No es un pavo, por cierto - dijo la pata-.\n\nFijense en la elegancia con que nada, y en lo derecho que se mantiene. Sin duda que es uno de mis peque\u00f1itos. Y si uno lo mira bien, se da cuenta pronto de que es realmente muy guapo. \u00a1Cuac, cuac! Vamos, vengan conmigo y d\u00e9jenme ense\u00f1arles el mundo y presentarlos al corral entero. Pero no se separen mucho de m\u00ed, no sea que los pisoteen. Y anden con los ojos muy abiertos, por si viene el gato.\nY con esto se encaminaron al corral. Hab\u00eda all\u00ed un escandalo espantoso, pues dos familias se estaban\n\nFigure 10: We do not specifically introduce additional OCR capabilities for GOT other than Chinese and English. Yet the PDF data we crawled may contain a small amount of text in other languages, leading to the GOT seeming to have the ability to recognize other languages. However, we cannot guarantee the OCR quality of other languages. Therefore, we recommend fine-tuning the model with corresponding data if this feature is needed.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We do not specifically introduce additional OCR capabilities for GOT other than Chinese and English.",
        "evidence_page_no": 15,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c132eec-9b19-48d8-b961-464b2c7ff2ed",
        "questions": "Which publication discusses the concept of 'Connectionist temporal classification' and who are the authors?",
        "answers": "Connectionist temporal classification is discussed in the publication by Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c18f41d-46ab-42d9-b256-9b532370539b",
        "questions": "What is the title of the work by Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J. that focuses on a versatile vision-language model?",
        "answers": "Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond.",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c20f65e-fcdc-410b-be30-f2329d2ceb6d",
        "questions": "Is the publication 'Gradient-based learning applied to document recognition' by LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. included in the references, and if so, in which year was it published?",
        "answers": "Yes, it was published in 1998.",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c248f08-0cf1-4812-be78-f5953c62d604",
        "questions": "Which dataset is used as the source to render sheet music for the GOT model's OCR tasks?",
        "answers": "GrandStaff",
        "context": "Figure 3: We use six rendering tools to run data engines to make the GOT work well on diverse OCR tasks. We utilize the $\\mathrm{LT}_{\\mathrm{E}} \\mathrm{X}$ for tables, Mathpix-markdown-it for math/molecular formulas, Tikz for simple geometric shapes, Verovio for sheet music, and Matplotlib/Pyecharts for charts, respectively.\n\nMore general OCR data. We hope GOT can deal with more general optical artificial \"characters\". Accordingly, we collect three related challenging tasks and generate the corresponding data. They are sheet music, geometric shapes, and charts, respectively.\n- Sheet music. Music is a precious part of the cultural heritage and optical music recognition plays an important role in achieving automatic recognition and transcription of sheet music [7, 38]. We choose the GrandStaff [39] dataset as the source to render. The dataset of polyphonic music scores provides the Humdrum ${ }^{* *}$ kern transcriptions from the excerpts of music. In addition to the existing approximately 10 w image-text samples, we also extract some text samples to re-render via the Verovio Python Package. We mainly add new backgrounds from white to real paper styles and randomly add the title and author information. Note that we only render single-system sheet music due to we don't have professionals in the relevant field and we do not know how to assemble single-system sheets to a full page. After rendering, we collect about 0.5 M samples.\n- Geometric shape. Geometry is a key capability of LVLMs and is a necessary step towards AGI. GOT is expected to transform optical geometric elements into TikZ [34] text format. TikZ contains some concise commands to produce basic geometric elements and they can be compiled using LATEX. We employ TikZ-style points and lines and use the simplest point-line spatial relationship to construct simple basic geometric shapes (e.g., circles, rectangles, triangles, and combined shapes) as well as simple function curves (e.g., straight lines, parabolas, ellipses, hyperbolas, and so on). Through this method, we obtained approximately 1 M geometric Tikz data. Of course, the geometric rendering is complicated, and our current work is only a preliminary attempt. GOT can only recognize basic geometry at present, yet we believe that with the development of synthetic data technology and OCR-2.0, future models will be able to identify complex geometric shapes.\n- Chart. Charts are crucial in data visualization and data analysis of several research fields. The proposed GOT refers to the chart structural extraction sub-task as \"Chart OCR\", which converts the visual knowledge (e.g., title, source, $x$-title, y -title, and values) on the chart image into an editable output with a table/Python-dict format. Following OneChart [8], the chart image-text pairs are rendered using Matplotlib and Pyecharts tools. Because GOT is only an OCR model, we don't need the elements of the chart synthesized to be semantically related. Thus, we just randomly extract entity texts (for the title, source, x-title, $y$-title, etc) from the open-access NLP corpus. The numerical values are random numbers under a controlled distribution. Through this method, we obtained 2 M chart data, with half from Matplotlib and half from Pyecharts.\n\n\n3.4 Customizing New OCR Features by Post-training the Decoder\n\n\nAfter compressing the general visual information of the diverse OCR-2.0 optical signals via the above two steps, GOT is ready to perform image-level OCR tasks in various scenarios. Based on",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We choose the GrandStaff [39] dataset as the source to render.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c35a424-a158-472b-b965-44a7834d88db",
        "questions": "How many chart data samples were obtained using Matplotlib and Pyecharts tools for the GOT model?",
        "answers": "2 M",
        "context": "Figure 3: We use six rendering tools to run data engines to make the GOT work well on diverse OCR tasks. We utilize the $\\mathrm{LT}_{\\mathrm{E}} \\mathrm{X}$ for tables, Mathpix-markdown-it for math/molecular formulas, Tikz for simple geometric shapes, Verovio for sheet music, and Matplotlib/Pyecharts for charts, respectively.\n\nMore general OCR data. We hope GOT can deal with more general optical artificial \"characters\". Accordingly, we collect three related challenging tasks and generate the corresponding data. They are sheet music, geometric shapes, and charts, respectively.\n- Sheet music. Music is a precious part of the cultural heritage and optical music recognition plays an important role in achieving automatic recognition and transcription of sheet music [7, 38]. We choose the GrandStaff [39] dataset as the source to render. The dataset of polyphonic music scores provides the Humdrum ${ }^{* *}$ kern transcriptions from the excerpts of music. In addition to the existing approximately 10 w image-text samples, we also extract some text samples to re-render via the Verovio Python Package. We mainly add new backgrounds from white to real paper styles and randomly add the title and author information. Note that we only render single-system sheet music due to we don't have professionals in the relevant field and we do not know how to assemble single-system sheets to a full page. After rendering, we collect about 0.5 M samples.\n- Geometric shape. Geometry is a key capability of LVLMs and is a necessary step towards AGI. GOT is expected to transform optical geometric elements into TikZ [34] text format. TikZ contains some concise commands to produce basic geometric elements and they can be compiled using LATEX. We employ TikZ-style points and lines and use the simplest point-line spatial relationship to construct simple basic geometric shapes (e.g., circles, rectangles, triangles, and combined shapes) as well as simple function curves (e.g., straight lines, parabolas, ellipses, hyperbolas, and so on). Through this method, we obtained approximately 1 M geometric Tikz data. Of course, the geometric rendering is complicated, and our current work is only a preliminary attempt. GOT can only recognize basic geometry at present, yet we believe that with the development of synthetic data technology and OCR-2.0, future models will be able to identify complex geometric shapes.\n- Chart. Charts are crucial in data visualization and data analysis of several research fields. The proposed GOT refers to the chart structural extraction sub-task as \"Chart OCR\", which converts the visual knowledge (e.g., title, source, $x$-title, y -title, and values) on the chart image into an editable output with a table/Python-dict format. Following OneChart [8], the chart image-text pairs are rendered using Matplotlib and Pyecharts tools. Because GOT is only an OCR model, we don't need the elements of the chart synthesized to be semantically related. Thus, we just randomly extract entity texts (for the title, source, x-title, $y$-title, etc) from the open-access NLP corpus. The numerical values are random numbers under a controlled distribution. Through this method, we obtained 2 M chart data, with half from Matplotlib and half from Pyecharts.\n\n\n3.4 Customizing New OCR Features by Post-training the Decoder\n\n\nAfter compressing the general visual information of the diverse OCR-2.0 optical signals via the above two steps, GOT is ready to perform image-level OCR tasks in various scenarios. Based on",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Through this method, we obtained 2 M chart data, with half from Matplotlib and half from Pyecharts.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c377a52-6fed-4601-8d41-c1455a1a0983",
        "questions": "Does the GOT model currently have the capability to recognize complex geometric shapes?",
        "answers": "No",
        "context": "Figure 3: We use six rendering tools to run data engines to make the GOT work well on diverse OCR tasks. We utilize the $\\mathrm{LT}_{\\mathrm{E}} \\mathrm{X}$ for tables, Mathpix-markdown-it for math/molecular formulas, Tikz for simple geometric shapes, Verovio for sheet music, and Matplotlib/Pyecharts for charts, respectively.\n\nMore general OCR data. We hope GOT can deal with more general optical artificial \"characters\". Accordingly, we collect three related challenging tasks and generate the corresponding data. They are sheet music, geometric shapes, and charts, respectively.\n- Sheet music. Music is a precious part of the cultural heritage and optical music recognition plays an important role in achieving automatic recognition and transcription of sheet music [7, 38]. We choose the GrandStaff [39] dataset as the source to render. The dataset of polyphonic music scores provides the Humdrum ${ }^{* *}$ kern transcriptions from the excerpts of music. In addition to the existing approximately 10 w image-text samples, we also extract some text samples to re-render via the Verovio Python Package. We mainly add new backgrounds from white to real paper styles and randomly add the title and author information. Note that we only render single-system sheet music due to we don't have professionals in the relevant field and we do not know how to assemble single-system sheets to a full page. After rendering, we collect about 0.5 M samples.\n- Geometric shape. Geometry is a key capability of LVLMs and is a necessary step towards AGI. GOT is expected to transform optical geometric elements into TikZ [34] text format. TikZ contains some concise commands to produce basic geometric elements and they can be compiled using LATEX. We employ TikZ-style points and lines and use the simplest point-line spatial relationship to construct simple basic geometric shapes (e.g., circles, rectangles, triangles, and combined shapes) as well as simple function curves (e.g., straight lines, parabolas, ellipses, hyperbolas, and so on). Through this method, we obtained approximately 1 M geometric Tikz data. Of course, the geometric rendering is complicated, and our current work is only a preliminary attempt. GOT can only recognize basic geometry at present, yet we believe that with the development of synthetic data technology and OCR-2.0, future models will be able to identify complex geometric shapes.\n- Chart. Charts are crucial in data visualization and data analysis of several research fields. The proposed GOT refers to the chart structural extraction sub-task as \"Chart OCR\", which converts the visual knowledge (e.g., title, source, $x$-title, y -title, and values) on the chart image into an editable output with a table/Python-dict format. Following OneChart [8], the chart image-text pairs are rendered using Matplotlib and Pyecharts tools. Because GOT is only an OCR model, we don't need the elements of the chart synthesized to be semantically related. Thus, we just randomly extract entity texts (for the title, source, x-title, $y$-title, etc) from the open-access NLP corpus. The numerical values are random numbers under a controlled distribution. Through this method, we obtained 2 M chart data, with half from Matplotlib and half from Pyecharts.\n\n\n3.4 Customizing New OCR Features by Post-training the Decoder\n\n\nAfter compressing the general visual information of the diverse OCR-2.0 optical signals via the above two steps, GOT is ready to perform image-level OCR tasks in various scenarios. Based on",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "GOT can only recognize basic geometry at present, yet we believe that with the development of synthetic data technology and OCR-2.0, future models will be able to identify complex geometric shapes.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c45d498-dc4a-4227-a0c6-4521e94ba284",
        "questions": "What types of optical image inputs does the GOT model support?",
        "answers": "GOT supports various optical image types, such as commonly used photographs and documents.",
        "context": "Figure 1: On the input side, GOT supports various optical image types, such as commonly used photographs and documents. Besides, as a general OCR-2.0 model, GOT can handle more tasks, e.g., sheet music, molecular formulas, easy geometric shapes, charts, etc. Moreover, the model can adapt to region-focus OCR, high-resolution OCR, and multiple-page OCR. GOT mainly supports English and Chinese and can control the structure results (Mathpix markdown/tikz/smiles/kern) via a prompt.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "On the input side, GOT supports various optical image types, such as commonly used photographs and documents.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c4a3950-345d-4816-80ed-b229b6b54cde",
        "questions": "Can the GOT model handle tasks involving sheet music and molecular formulas?",
        "answers": "Yes",
        "context": "Figure 1: On the input side, GOT supports various optical image types, such as commonly used photographs and documents. Besides, as a general OCR-2.0 model, GOT can handle more tasks, e.g., sheet music, molecular formulas, easy geometric shapes, charts, etc. Moreover, the model can adapt to region-focus OCR, high-resolution OCR, and multiple-page OCR. GOT mainly supports English and Chinese and can control the structure results (Mathpix markdown/tikz/smiles/kern) via a prompt.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Besides, as a general OCR-2.0 model, GOT can handle more tasks, e.g., sheet music, molecular formulas, easy geometric shapes, charts, etc.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c4b6a95-c05b-4264-8ca3-6ecb40b7dbc2",
        "questions": "What languages does the GOT model primarily support?",
        "answers": "English and Chinese",
        "context": "Figure 1: On the input side, GOT supports various optical image types, such as commonly used photographs and documents. Besides, as a general OCR-2.0 model, GOT can handle more tasks, e.g., sheet music, molecular formulas, easy geometric shapes, charts, etc. Moreover, the model can adapt to region-focus OCR, high-resolution OCR, and multiple-page OCR. GOT mainly supports English and Chinese and can control the structure results (Mathpix markdown/tikz/smiles/kern) via a prompt.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "GOT mainly supports English and Chinese and can control the structure results (Mathpix markdown/tikz/smiles/kern) via a prompt.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c5a9029-c335-433b-ab17-30fef11ea92e",
        "questions": "What is the title and year of the 2024 paper related to chart structure extraction?",
        "answers": "Onechart: Purify the chart structural extraction via one auxiliary token, 2024",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c5f152b-db9f-499f-9ea7-917eb4e6889a",
        "questions": "Identify any two authors of the Qwen-vl paper published in 2023.",
        "answers": "Bai, J. and Bai, S.",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c60a98b-9bba-450a-969d-78309b08492e",
        "questions": "What is the citation number for the 2006 paper on Connectionist Temporal Classification?",
        "answers": "11",
        "context": "References\n\n[1] Casia-hwdb2-line. https://huggingface.co/datasets/Teklia/CASIA-HWDB2-line (2024) 6\n[2] Iam-line. https://huggingface.co/datasets/Teklia/IAM-line (2024) 6\n[3] Norhand-v3-line. https://huggingface.co/datasets/Teklia/NorHand-v3-line (2024) 6\n[4] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., Zhu, T.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023) 6\n[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) $1,4,9,10$\n[6] Blecher, L., Cucurull, G., Scialom, T., Stojnic, R.: Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418 (2023) 4, 6, 8, 9\n[7] Calvo-Zaragoza, J., Jr, J.H., Pacha, A.: Understanding optical music recognition. ACM Computing Surveys (CSUR) 53(4), 1-35 (2020) 7\n[8] Chen, J., Kong, L., Wei, H., Liu, C., Ge, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987 (2024) 7, 10\n[9] Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al.: How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821 (2024) $1,3,4,8,9$\n[10] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021) 1, 4, 5\n[11] Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4\n[12] Gu, J., Meng, X., Lu, G., Hou, L., Minzhe, N., Liang, X., Yao, L., Huang, R., Zhang, W., Jiang, X., et al.: Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems 35, 26418-26431 (2022) 5\n[13] Hu, A., Xu, H., Ye, J., Yan, M., Zhang, L., Zhang, B., Li, C., Zhang, J., Jin, Q., Huang, F., et al.: mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895 (2024) 9, 10\n[14] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998) 4\n[15] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 3\n[16] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformerbased optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023) 4\n[17] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: European conference on computer vision. pp. 280-296. Springer (2022) 5\n[18] Liao, M., Shi, B., Bai, X., Wang, C., Lu, T., Mei, T.: Textboxes: A fast text detector with a single deep neural network. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (2017) 4\n[19] Liao, M., Zou, Z., Wan, Z., Yao, C., Bai, X.: Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE transactions on pattern analysis and machine intelligence 45(1), 919-931 (2022) 4\n[20] Liu, C., Wei, H., Chen, J., Kong, L., Ge, Z., Zhu, Z., Zhao, L., Sun, J., Han, C., Zhang, X.: Focus anywhere for fine-grained multi-page document understanding. arXiv preprint arXiv:2405.14295 (2024) 4, 8, 9, 10",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Graves, A., Fern\u00e1ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: International Conference on Machine Learning (ICML) (2006) 4",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c614595-9d7c-4882-9ce3-2ac34a1e1ea7",
        "questions": "How many epochs does GOT train for in the pre-training stage?",
        "answers": "3",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs.",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c64db4b-516c-4ae3-8535-245897b0d1e2",
        "questions": "What resolution does GOT support for dynamic resolution OCR tasks, and how is it managed to ensure reliable results?",
        "answers": "$1024 \u00d7 1024$",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GOT supports $1024 \u00d7 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ($1024 \u00d7 1024$), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens.",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c7233c7-febd-4778-9327-c6610d54f5bd",
        "questions": "What are the most commonly used frame colors chosen for the color-guided fine-grained OCR sub-task?",
        "answers": "red, green, and blue",
        "context": "this perceptually savvy vision encoder, GOT can be easily tuned to meet the users' needs for input and output. Here, we customize GOT to enable three new features, i.e., fine-grained, multi-page, and dynamic resolution OCR, by only post-training the decoder part.\n\n\n3.4.1 Fine-grained Data Engine for Interactive OCR.\n\n\nAs a high-interactivity feature, fine-grained OCR [20] is the region-level visual perception controlled by spatial coordinates or colors. The user can add box coordinates (box-guided OCR) or color text (color-guided OCR) in the question prompt to request recognition within the region of interest (RoI), avoiding the output of other irrelevant characters. For the natural fine-grained OCR, the source images and annotations are from opensource datasets, including RCTW [41], ReCTS [25], and ShopSign [51], and COCO-Text [44] dataset. The datasets mentioned above provide the text bounding boxes, so we can use them to produce fine-grained (region/color prompt) OCR data directly. For the document-level fine-grained OCR, following Fox [20], we filter out those with the scanned format in the downloaded PDF files and parse the left part using Python packages (Fitz/PDFminer). We record the page-level images, bounding boxes of each line/paragraph, and the corresponding texts to produce the ground truth of the box-guided OCR sub-task. For such a task, each coordinate value is first normalized and then magnified 1000 times. For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image. Overall, we gather about 60 w samples.\n\n3.4.2 Multi-crop Data Engine for Ultra-large-image OCR.\n\nGOT supports $1024 \\times 1024$ input resolution, which is enough for commonly used OCR tasks, e.g., scene OCR or A4-page PDF OCR. However, dynamic resolution is required for some scenes with huge images, such as two-page PDF horizontal stitching (commonly occurring when reading papers). Thanks to our high compression rate encoder, the dynamic resolution of GOT is achieved under a large sliding window ( $1024 \\times 1024$ ), ensuring that our model can complete extreme resolution OCR tasks with acceptable image tokens. We use the InternVL-1.5 [9] cropping method with tiles max to 12. The ultra-resolution images are synthesized using the single-page PDF data mentioned above, including horizontal and vertical stitching. Through this method, we obtained a total of 50 w image-texts pairs.\n\n3.4.3 Multi-page Data Engine for Batched PDF-file OCR.\n\nFor OCR tasks, it is reasonable to use a \"for loop\" for multi-page processing. We introduce the multi-page OCR (without \"for loop\") feature for GOT due to some formatted PDF data making it difficult to break pages (to obtain text that is completely incompatible with each page) to further scale up, such as .tex in Arxiv. We hope that with GOT, researchers no longer have to worry about PDF ground truth page breaks (e.g., Nougat [6]), as they can train on multiple pages directly. To realize such a feature, we randomly sample 2-8 pages from our Mathpix formatted PDF data and join them together to form a single round OCR task. Each selected page contains text that is less than 650 tokens, to ensure that the overall length does not exceed 8 K . In total, we generate about 20 w multi-page OCR data, most of which are interlaced between Chinese and English pages.\n\n4 Experiments\n\n4.1 Implement Details\n\nWe use $8 \\times 8$ L40s GPUs to train GOT. In the pre-training stage, we optimize all model parameters with a global batch size of 128 and train for 3 epochs. We utilize the AdamW [29] optimizer and a cosine annealing scheduler [28] with a start learning rate of 1e-4. The max token length in this stage is set to 4096. In the joint-training stage, we put the max token length to 6000 and train the model with the same optimizer settings as stage 1 for 1 epoch. In the last post-training stage, we expand the max token length to 8192 to allow the model to support multi-patch/page OCR features. In this stage, the beginning learning rate is $2 \\mathrm{e}-5$, and the epoch is set to 1 .\n\nDuring each train-data process, $80 \\%$ of the data from the previous stage is sampled for the following stage to ensure that the basic ability does not degrade when adding new features.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "For the color-guided task, we choose the most commonly used colors (red, green, and blue) as the frame colors and draw them via the corresponding bounding box on the original image.",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c921259-2ae8-465e-8aff-bab8c60e85a7",
        "questions": "What is the precision for the 'multi-crop' strategy when processing formulas in Markdown documents?",
        "answers": "0.858",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c962c3c-4e7c-4463-b908-b06c0a5d8c19",
        "questions": "In the fine-grained OCR performance based on the English region metrics, which model achieved the highest F1-score, and what is this value?",
        "answers": "GOT, 0.970",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "F1-score \\uparrow & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0c9a7fec-92a5-4895-9745-c170d7bcafdf",
        "questions": "Comparing all models listed in ChartQA-SE for AP@slight, which model ranks second, and what is its performance score?",
        "answers": "ChartVLM, 0.814",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  & Types & Edit Distance $\\downarrow$ & F1-score $\\uparrow$ & Precision $\\uparrow$ & Recall $\\uparrow$ & BLEU $\\uparrow$ & METEOR $\\uparrow$ \\\\\n  \\multirow{8}{*}{Markdown document} & single: & & & & & & \\\\\n  & All text & 0.097 & 0.942 & 0.944 & 0.942 & 0.877 & 0.876 \\\\\n  & Formula & 0.269 & 0.749 & 0.771 & 0.751 & 0.512 & 0.716 \\\\\n  & Table & 0.254 & 0.867 & 0.857 & 0.897 & 0.756 & 0.760 \\\\\n  & muti-crop: & & & & & & \\\\\n  & All text & 0.086 & 0.953 & 0.948 & 0.960 & 0.896 & 0.903 \\\\\n  & Formula & 0.159 & 0.865 & 0.858 & 0.882 & 0.628 & 0.828 \\\\\n  & Table & 0.220 & 0.878 & 0.861 & 0.919 & 0.779 & 0.811 \\\\\n  \\multirow[t]{2}{*}{Geneal} & Sheet music & 0.046 & 0.939 & 0.963 & 0.939 & 0.900 & 0.923 \\\\\n  & Geometry & 0.061 & 0.884 & 0.882 & 0.888 & 0.766 & 0.882 \\\\\n \n\\end{tabular}\n\nTable 3: Performances of formatted document (Chinese/English) and more general OCR. Single means the input is the vanilla image and multi-crop represents the dynamic resolution strategy.\nresults prove the effectiveness of GOT on documents with formatted outputs. Besides, the dynamic resolution scheme is a good choice when processing higher-resolution images.\n\n\n4.2.4 Fine-grained OCR performance\n\n\nWe report the fine-grained OCR metrics of GOT. As shown in Table 4, the GOT is overall better than Fox [20] on both the bounding box-based and color-based referential OCR tasks, indicating that our model enjoys excellent interactive OCR capabilities.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow{3}{*}{Metrics} & \\multicolumn{5}{|c|}{English} & \\multicolumn{4}{|c|}{Chinese} \\\\\n  & \\multicolumn{3}{|c|}{region} & \\multicolumn{2}{|l|}{color} & \\multicolumn{2}{|l|}{region} & \\multicolumn{2}{|c|}{color} \\\\\n  & DocOwl1.5 [13] & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT & Fox [20] & GOT \\\\\n  Edit Distance $\\downarrow$ & 0.435 & 0.059 & 0.041 & 0.064 & 0.034 & 0.042 & 0.033 & 0.114 & 0.040 \\\\\n  F1-score $\\uparrow$ & 0.670 & 0.957 & 0.970 & 0.940 & 0.966 & 0.955 & 0.965 & 0.884 & 0.957 \\\\\n  Precision $\\uparrow$ & 0.886 & 0.962 & 0.973 & 0.942 & 0.970 & 0.966 & 0.974 & 0.902 & 0.969 \\\\\n  Recall $\\uparrow$ & 0.617 & 0.955 & 0.969 & 0.942 & 0.964 & 0.947 & 0.958 & 0.873 & 0.948 \\\\\n  BLEU $\\uparrow$ & 0.478 & 0.914 & 0.926 & 0.868 & 0.910 & 0.885 & 0.898 & 0.778 & 0.884 \\\\\n  METEOR $\\uparrow$ & 0.569 & 0.955 & 0.966 & 0.938 & 0.961 & 0.934 & 0.942 & 0.848 & 0.931 \\\\\n \n\\end{tabular}\n\nTable 4: Comparison of fine-grained document OCR.\n\\begin{tabular}{llcccccc}\n  & Metric & \\begin{tabular}{c} \nDeplot \\\\\n$(1.3 B)[22]$\n\\end{tabular} & \\begin{tabular}{c} \nUniChart \\\\\n$(0.26 B)[31]$\n\\end{tabular} & \\begin{tabular}{c} \nChartVLM \\\\\n$(7.3 B)[48]$\n\\end{tabular} & \\begin{tabular}{c} \nGPT-4V \\\\\n$(>100 B)[36]$\n\\end{tabular} & \\begin{tabular}{c} \nQwen-VL \\\\\n$(>72 B)[5]$\n\\end{tabular} & \\begin{tabular}{c} \nGOT \\\\\n$(0.58 B$\n\\end{tabular} \\\\\n  \\multirow{3}{*}{ ChartQA-SE } & AP@ strict & 0.614 & 0.423 & 0.718 & 0.504 & 0.586 & $\\mathbf{0 . 7 4 7}$ \\\\\n& AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & $\\mathbf{0 . 8 4 5}$ \\\\\n& AP@high & 0.729 & 0.560 & 0.842 & 0.643 & 0.727 & $\\mathbf{0 . 8 6 7}$ \\\\\n  \\multirow{3}{*}{ PlotQA-SE } & AP@strict & 0.031 & 0.105 & 0.038 & 0.073 & 0.005 & $\\mathbf{0 . 1 3 3}$ \\\\\n& AP@slight & 16.49 & 0.260 & 0.468 & 0.194 & 0.042 & $\\mathbf{0 . 5 9 6}$ \\\\\n& AP@high & 26.50 & 0.269 & 0.540 & 0.223 & 0.120 & $\\mathbf{0 . 6 4 0}$ \\\\\n \n\\end{tabular}\n\nTable 5: Performance comparisons on number-centric chart OCR.\n\n4.2.5 More general OCR performance\n\nWe utilize the sheet music, geometry, and chart benchmarks to verify GOT's more general OCR performance. For the first two tasks, we separately render 100 and 180 additional samples as benchmarks, and as can be seen in Table 3, GOT still performs well on these new OCR tasks. For chart OCR, we use structure-extraction version [8] ChartQA [32] and PlotQA [35] as benchmarks. In Table 5, the chart OCR ability of GOT is even much better than the chart-specific models and popular LVLMs. All results demonstrate the effectiveness of our model on more general OCR tasks.\n\n5 Conclusion\n\nThis paper presents a primary OCR-2.0 model that is structurally simpler than OCR-1.0 systems, focuses more on pure OCR tasks than LVLMs, and enjoys superior performance. OCR-2.0 integrates various pan-OCR tasks into one model and is a valuable research direction in model design, data engineering, and application scenarios. We want the simple, elegant, effective, and promising GOT OCR-2.0 model to attract more attention to such a task.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "AP@ slight & 0.709 & 53.18 & 0.814 & 0.606 & 0.685 & \\mathbf{0 . 8 4 5}",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0ca97c64-02bc-4d78-a13e-13e71ea0f50f",
        "questions": "What is the Edit Distance for UReader [50] when processing English OCR for document-level pages?",
        "answers": "0.718",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|r|}{$B \\operatorname{BLEU} \\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.718 & - & 0.344 & - & 0.296 & - & 0.469 & - & 0.103 & - & 0.287 & $-$ \\\\\n  LLaVA-NeXT [23] & 34B & 0.430 & - & 0.647 & $-$ & 0.573 & $-$ & 0.881 & $-$ & 0.478 & - & 0.582 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.393 & 0.265 & 0.751 & 0.816 & 0.698 & 0.784 & 0.917 & 0.866 & 0.568 & 0.622 & 0.663 & 0.717 \\\\\n  Nougat [6] & 250M & 0.255 & - & 0.745 & - & 0.720 & - & 0.809 & - & 0.665 & - & 0.761 & - \\\\\n  TextMonkey [27] & 7B & 0.265 & $-$ & 0.821 & - & 0.778 & $-$ & 0.906 & - & 0.671 & - & 0.762 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.258 & - & 0.862 & - & 0.835 & - & 0.962 & - & 0.788 & - & 0.858 & $-$ \\\\\n  Vary [46] & 7B & 0.092 & 0.113 & 0.918 & 0.952 & 0.906 & 0.961 & 0.956 & 0.944 & 0.885 & 0.754 & 0.926 & 0.873 \\\\\n  Vary-toy [47] & 1.8 B & 0.082 & 0.142 & 0.924 & 0.914 & 0.919 & 0.928 & 0.938 & 0.907 & 0.889 & 0.718 & 0.929 & 0.832 \\\\\n  Qwen-VL-Plus [5] & $-$ & 0.096 & 0.121 & 0.931 & 0.895 & 0.921 & 0.903 & 0.950 & 0.890 & 0.893 & 0.684 & 0.936 & 0.828 \\\\\n  Qwen-VL-Max [5] & $>72$ B & 0.057 & 0.091 & 0.964 & 0.931 & 0.955 & 0.917 & 0.977 & 0.946 & 0.942 & 0.756 & 0.971 & 0.885 \\\\\n  Fox [20] & $1.8 B$ & 0.046 & 0.061 & 0.952 & 0.954 & 0.957 & 0.964 & 0.948 & 0.946 & 0.930 & 0.842 & 0.954 & 0.908 \\\\\n  GOT & 580 M & 0.035 & 0.038 & 0.972 & 0.980 & 0.971 & 0.982 & 0.973 & 0.978 & 0.947 & 0.878 & 0.958 & 0.939 \\\\\n \n\\end{tabular}\n\nTable 1: Performance comparison of dense English (en) and Chinese (zh) OCR on document-level pages. The results of other models are from the previous work [20].\n\n\n4.2 Main Results\n\n\nIn this section, we verify the performance of GOT on 5 different OCR tasks, including 1) plain document OCR; 2) scene text OCR; 3) fine-grained document OCR; 4) formatted (Mathpix markdown) document OCR; 5) more general character OCR. Note that the test data for each benchmark undergoes strict text similarity filtering to ensure that it is not included in the training data. Sources of each test benchmark and model performance analysis are as follows.\n\n4.2.1 Plain document OCR performance\n\nWe use the open-source Fox [20] benchmark to test the performance of GOT on both Chinese and English PDF OCR. The metrics we used are those commonly in OCR tasks, i.e., edict distance, F1-score, precision, recall, BLEU, and METEOR. Due to the lengthy text of the document, we use word-level segmentation to calculate each indicator. As shown in Table 1, with only 580M, GOT achieves advanced performance on pure text OCR in the document, proving the excellent PDF text perception and recognition ability.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|l|}{BLEU $\\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.568 & - & 0.661 & - & 0.843 & - & 0.569 & - & 0.258 & - & 0.488 & - \\\\\n  LLaVA-NeXT [23] & 34B & 0.499 & - & 0.558 & - & 0.637 & - & 0.538 & - & 0.379 & - & 0.678 & - \\\\\n  TextMonkey [27] & 7B & 0.331 & - & 0.743 & - & 0.827 & $-$ & 0.710 & $-$ & 0.521 & $-$ & 0.728 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.334 & - & 0.788 & - & 0.887 & - & 0.751 & - & 0.525 & - & 0.708 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.267 & 0.123 & 0.834 & 0.913 & 0.942 & 0.934 & 0.790 & 0.902 & 0.587 & 0.588 & 0.744 & 0.876 \\\\\n  Qwen-VL-Max [5] & $>72 B$ & 0.182 & 0.168 & 0.881 & 0.867 & 0.891 & 0.878 & 0.888 & 0.873 & 0.586 & 0.572 & 0.848 & 0.845 \\\\\n  GOT & 580M & 0.112 & 0.096 & 0.926 & 0.928 & 0.934 & 0.914 & 0.927 & 0.954 & 0.676 & 0.641 & 0.896 & 0.928 \\\\\n \n\\end{tabular}\n\nTable 2: Performance of English (en) and Chinese (zh) OCR for scene texts.\n\n4.2.2 Scene text OCR performance\n\nWe collect 400 natural images, half in Chinese and half in English, as the scene text OCR benchmark. All the ground truth in this benchmark are manually corrected. Because the text in the scene image is relatively short, we use character-level segmentation to calculate various metrics. As shown in Table 2, we can see that GOT also works well on natural images, demonstrating the model's excellent performance on most basic OCR tasks (both document and scene texts).\n\n4.2.3 Formatted document OCR performance\n\nConverting the optical PDF image to a markdown-like format is an important feature of an OCR model. To verify this ability of GOT, we carefully prepare 90 pages of samples as a high-quality benchmark. The benchmark, containing both Chinese and English document pages, is first generating pseudo-labels via Mathpix, and then manually correcting for errors. In Table 3, we can see the single-scale $(1024 \\times 1024)$ GOT can yield satisfactory results. When we use multi-crop inference, the performance of GOT is further lifted especially on formulas and tables with small texts. The",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "UReader [50] & 7B & 0.718 & - & 0.344 & - & 0.296 & - & 0.469 & - & 0.103 & - & 0.287 & $-$",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0ca97ff3-a2cf-4f86-bae0-072f39fc7c2d",
        "questions": "Between Vary [46] and Vary-toy [47], which method achieves a lower Edit Distance for Chinese OCR on document-level pages, and what is the respective Edit Distance value?",
        "answers": "Vary [46] achieves a lower Edit Distance of 0.113.",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|r|}{$B \\operatorname{BLEU} \\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.718 & - & 0.344 & - & 0.296 & - & 0.469 & - & 0.103 & - & 0.287 & $-$ \\\\\n  LLaVA-NeXT [23] & 34B & 0.430 & - & 0.647 & $-$ & 0.573 & $-$ & 0.881 & $-$ & 0.478 & - & 0.582 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.393 & 0.265 & 0.751 & 0.816 & 0.698 & 0.784 & 0.917 & 0.866 & 0.568 & 0.622 & 0.663 & 0.717 \\\\\n  Nougat [6] & 250M & 0.255 & - & 0.745 & - & 0.720 & - & 0.809 & - & 0.665 & - & 0.761 & - \\\\\n  TextMonkey [27] & 7B & 0.265 & $-$ & 0.821 & - & 0.778 & $-$ & 0.906 & - & 0.671 & - & 0.762 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.258 & - & 0.862 & - & 0.835 & - & 0.962 & - & 0.788 & - & 0.858 & $-$ \\\\\n  Vary [46] & 7B & 0.092 & 0.113 & 0.918 & 0.952 & 0.906 & 0.961 & 0.956 & 0.944 & 0.885 & 0.754 & 0.926 & 0.873 \\\\\n  Vary-toy [47] & 1.8 B & 0.082 & 0.142 & 0.924 & 0.914 & 0.919 & 0.928 & 0.938 & 0.907 & 0.889 & 0.718 & 0.929 & 0.832 \\\\\n  Qwen-VL-Plus [5] & $-$ & 0.096 & 0.121 & 0.931 & 0.895 & 0.921 & 0.903 & 0.950 & 0.890 & 0.893 & 0.684 & 0.936 & 0.828 \\\\\n  Qwen-VL-Max [5] & $>72$ B & 0.057 & 0.091 & 0.964 & 0.931 & 0.955 & 0.917 & 0.977 & 0.946 & 0.942 & 0.756 & 0.971 & 0.885 \\\\\n  Fox [20] & $1.8 B$ & 0.046 & 0.061 & 0.952 & 0.954 & 0.957 & 0.964 & 0.948 & 0.946 & 0.930 & 0.842 & 0.954 & 0.908 \\\\\n  GOT & 580 M & 0.035 & 0.038 & 0.972 & 0.980 & 0.971 & 0.982 & 0.973 & 0.978 & 0.947 & 0.878 & 0.958 & 0.939 \\\\\n \n\\end{tabular}\n\nTable 1: Performance comparison of dense English (en) and Chinese (zh) OCR on document-level pages. The results of other models are from the previous work [20].\n\n\n4.2 Main Results\n\n\nIn this section, we verify the performance of GOT on 5 different OCR tasks, including 1) plain document OCR; 2) scene text OCR; 3) fine-grained document OCR; 4) formatted (Mathpix markdown) document OCR; 5) more general character OCR. Note that the test data for each benchmark undergoes strict text similarity filtering to ensure that it is not included in the training data. Sources of each test benchmark and model performance analysis are as follows.\n\n4.2.1 Plain document OCR performance\n\nWe use the open-source Fox [20] benchmark to test the performance of GOT on both Chinese and English PDF OCR. The metrics we used are those commonly in OCR tasks, i.e., edict distance, F1-score, precision, recall, BLEU, and METEOR. Due to the lengthy text of the document, we use word-level segmentation to calculate each indicator. As shown in Table 1, with only 580M, GOT achieves advanced performance on pure text OCR in the document, proving the excellent PDF text perception and recognition ability.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|l|}{BLEU $\\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.568 & - & 0.661 & - & 0.843 & - & 0.569 & - & 0.258 & - & 0.488 & - \\\\\n  LLaVA-NeXT [23] & 34B & 0.499 & - & 0.558 & - & 0.637 & - & 0.538 & - & 0.379 & - & 0.678 & - \\\\\n  TextMonkey [27] & 7B & 0.331 & - & 0.743 & - & 0.827 & $-$ & 0.710 & $-$ & 0.521 & $-$ & 0.728 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.334 & - & 0.788 & - & 0.887 & - & 0.751 & - & 0.525 & - & 0.708 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.267 & 0.123 & 0.834 & 0.913 & 0.942 & 0.934 & 0.790 & 0.902 & 0.587 & 0.588 & 0.744 & 0.876 \\\\\n  Qwen-VL-Max [5] & $>72 B$ & 0.182 & 0.168 & 0.881 & 0.867 & 0.891 & 0.878 & 0.888 & 0.873 & 0.586 & 0.572 & 0.848 & 0.845 \\\\\n  GOT & 580M & 0.112 & 0.096 & 0.926 & 0.928 & 0.934 & 0.914 & 0.927 & 0.954 & 0.676 & 0.641 & 0.896 & 0.928 \\\\\n \n\\end{tabular}\n\nTable 2: Performance of English (en) and Chinese (zh) OCR for scene texts.\n\n4.2.2 Scene text OCR performance\n\nWe collect 400 natural images, half in Chinese and half in English, as the scene text OCR benchmark. All the ground truth in this benchmark are manually corrected. Because the text in the scene image is relatively short, we use character-level segmentation to calculate various metrics. As shown in Table 2, we can see that GOT also works well on natural images, demonstrating the model's excellent performance on most basic OCR tasks (both document and scene texts).\n\n4.2.3 Formatted document OCR performance\n\nConverting the optical PDF image to a markdown-like format is an important feature of an OCR model. To verify this ability of GOT, we carefully prepare 90 pages of samples as a high-quality benchmark. The benchmark, containing both Chinese and English document pages, is first generating pseudo-labels via Mathpix, and then manually correcting for errors. In Table 3, we can see the single-scale $(1024 \\times 1024)$ GOT can yield satisfactory results. When we use multi-crop inference, the performance of GOT is further lifted especially on formulas and tables with small texts. The",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Vary [46] & 7B & 0.092 & 0.113 & 0.918 & 0.952 & 0.906 & 0.961 & 0.956 & 0.944 & 0.885 & 0.754 & 0.926 & 0.873",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0cb1cb5a-68ec-4e22-89fa-67816f79e82c",
        "questions": "What is the F1-score achieved by GOT for both English and Chinese OCR performance on scene texts, and how does it compare to Fox [20]?",
        "answers": "The F1-score achieved by GOT is 0.926 for English and 0.928 for Chinese. In comparison, Fox [20] achieves 0.952 for English and 0.954 for Chinese.",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|r|}{$B \\operatorname{BLEU} \\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.718 & - & 0.344 & - & 0.296 & - & 0.469 & - & 0.103 & - & 0.287 & $-$ \\\\\n  LLaVA-NeXT [23] & 34B & 0.430 & - & 0.647 & $-$ & 0.573 & $-$ & 0.881 & $-$ & 0.478 & - & 0.582 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.393 & 0.265 & 0.751 & 0.816 & 0.698 & 0.784 & 0.917 & 0.866 & 0.568 & 0.622 & 0.663 & 0.717 \\\\\n  Nougat [6] & 250M & 0.255 & - & 0.745 & - & 0.720 & - & 0.809 & - & 0.665 & - & 0.761 & - \\\\\n  TextMonkey [27] & 7B & 0.265 & $-$ & 0.821 & - & 0.778 & $-$ & 0.906 & - & 0.671 & - & 0.762 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.258 & - & 0.862 & - & 0.835 & - & 0.962 & - & 0.788 & - & 0.858 & $-$ \\\\\n  Vary [46] & 7B & 0.092 & 0.113 & 0.918 & 0.952 & 0.906 & 0.961 & 0.956 & 0.944 & 0.885 & 0.754 & 0.926 & 0.873 \\\\\n  Vary-toy [47] & 1.8 B & 0.082 & 0.142 & 0.924 & 0.914 & 0.919 & 0.928 & 0.938 & 0.907 & 0.889 & 0.718 & 0.929 & 0.832 \\\\\n  Qwen-VL-Plus [5] & $-$ & 0.096 & 0.121 & 0.931 & 0.895 & 0.921 & 0.903 & 0.950 & 0.890 & 0.893 & 0.684 & 0.936 & 0.828 \\\\\n  Qwen-VL-Max [5] & $>72$ B & 0.057 & 0.091 & 0.964 & 0.931 & 0.955 & 0.917 & 0.977 & 0.946 & 0.942 & 0.756 & 0.971 & 0.885 \\\\\n  Fox [20] & $1.8 B$ & 0.046 & 0.061 & 0.952 & 0.954 & 0.957 & 0.964 & 0.948 & 0.946 & 0.930 & 0.842 & 0.954 & 0.908 \\\\\n  GOT & 580 M & 0.035 & 0.038 & 0.972 & 0.980 & 0.971 & 0.982 & 0.973 & 0.978 & 0.947 & 0.878 & 0.958 & 0.939 \\\\\n \n\\end{tabular}\n\nTable 1: Performance comparison of dense English (en) and Chinese (zh) OCR on document-level pages. The results of other models are from the previous work [20].\n\n\n4.2 Main Results\n\n\nIn this section, we verify the performance of GOT on 5 different OCR tasks, including 1) plain document OCR; 2) scene text OCR; 3) fine-grained document OCR; 4) formatted (Mathpix markdown) document OCR; 5) more general character OCR. Note that the test data for each benchmark undergoes strict text similarity filtering to ensure that it is not included in the training data. Sources of each test benchmark and model performance analysis are as follows.\n\n4.2.1 Plain document OCR performance\n\nWe use the open-source Fox [20] benchmark to test the performance of GOT on both Chinese and English PDF OCR. The metrics we used are those commonly in OCR tasks, i.e., edict distance, F1-score, precision, recall, BLEU, and METEOR. Due to the lengthy text of the document, we use word-level segmentation to calculate each indicator. As shown in Table 1, with only 580M, GOT achieves advanced performance on pure text OCR in the document, proving the excellent PDF text perception and recognition ability.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Method} & \\multirow[t]{2}{*}{Size} & \\multicolumn{2}{|l|}{Edit Distance $\\downarrow$} & \\multicolumn{2}{|l|}{F1-score $\\uparrow$} & \\multicolumn{2}{|l|}{Precision $\\uparrow$} & \\multicolumn{2}{|l|}{Recall $\\uparrow$} & \\multicolumn{2}{|l|}{BLEU $\\uparrow$} & \\multicolumn{2}{|l|}{METEOR $\\uparrow$} \\\\\n  & & en & zh & en & zh & en & zh & en & zh & en & zh & en & zh \\\\\n  UReader [50] & 7B & 0.568 & - & 0.661 & - & 0.843 & - & 0.569 & - & 0.258 & - & 0.488 & - \\\\\n  LLaVA-NeXT [23] & 34B & 0.499 & - & 0.558 & - & 0.637 & - & 0.538 & - & 0.379 & - & 0.678 & - \\\\\n  TextMonkey [27] & 7B & 0.331 & - & 0.743 & - & 0.827 & $-$ & 0.710 & $-$ & 0.521 & $-$ & 0.728 & - \\\\\n  DocOwl1.5 [13] & 7B & 0.334 & - & 0.788 & - & 0.887 & - & 0.751 & - & 0.525 & - & 0.708 & - \\\\\n  InternVL-ChatV1.5[9] & 26B & 0.267 & 0.123 & 0.834 & 0.913 & 0.942 & 0.934 & 0.790 & 0.902 & 0.587 & 0.588 & 0.744 & 0.876 \\\\\n  Qwen-VL-Max [5] & $>72 B$ & 0.182 & 0.168 & 0.881 & 0.867 & 0.891 & 0.878 & 0.888 & 0.873 & 0.586 & 0.572 & 0.848 & 0.845 \\\\\n  GOT & 580M & 0.112 & 0.096 & 0.926 & 0.928 & 0.934 & 0.914 & 0.927 & 0.954 & 0.676 & 0.641 & 0.896 & 0.928 \\\\\n \n\\end{tabular}\n\nTable 2: Performance of English (en) and Chinese (zh) OCR for scene texts.\n\n4.2.2 Scene text OCR performance\n\nWe collect 400 natural images, half in Chinese and half in English, as the scene text OCR benchmark. All the ground truth in this benchmark are manually corrected. Because the text in the scene image is relatively short, we use character-level segmentation to calculate various metrics. As shown in Table 2, we can see that GOT also works well on natural images, demonstrating the model's excellent performance on most basic OCR tasks (both document and scene texts).\n\n4.2.3 Formatted document OCR performance\n\nConverting the optical PDF image to a markdown-like format is an important feature of an OCR model. To verify this ability of GOT, we carefully prepare 90 pages of samples as a high-quality benchmark. The benchmark, containing both Chinese and English document pages, is first generating pseudo-labels via Mathpix, and then manually correcting for errors. In Table 3, we can see the single-scale $(1024 \\times 1024)$ GOT can yield satisfactory results. When we use multi-crop inference, the performance of GOT is further lifted especially on formulas and tables with small texts. The",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GOT & 580M & 0.112 & 0.096 & 0.926 & 0.928 & 0.934 & 0.914 & 0.927 & 0.954 & 0.676 & 0.641 & 0.896 & 0.928",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0cb22049-7ec9-4efc-a8c7-00b57379f4cc",
        "questions": "What is the expression for the length function $d_{L}(C_{v}, S_{v}^{*})$ based on radial and topological distance components?",
        "answers": "$d_{L}(C_{v}, S_{v}^{*})+1+2(\\operatorname{rad} T-1-d_{T}(C_{v}, S_{v}^{*}))$",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$d_{L}(C_{v}, S_{v}^{*})+1+2(\\operatorname{rad} T-1-d_{T}(C_{v}, S_{v}^{*}))$",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0cb6021e-afe6-4875-b98d-086d18401176",
        "questions": "Does the equation for $d_{L}(C_{L},\\{v'\\})$ include a component for the set cardinality $|\\{v'\\}|$?",
        "answers": "Yes",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "equation",
        "evidence_context": "$d_{L}(C_{L},\\{v'\\})=|C_{L}|+|\\{v'\\}|+2(d_{T}(C_{L}, v')-1)$",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2409.01704v1",
        "ID": "0cbdc014-dbed-442e-ada3-13e783e9973b",
        "questions": "What additional term is subtracted in the equation for $d_{L}(C_{L},\\{v'\\})$ compared to the expression for $d_{L}(C_{v}, S_{v}^{*})$?",
        "answers": "The term $1$ is subtracted in the expression for $d_{L}(C_{L},\\{v'\\})$ compared to that for $d_{L}(C_{v}, S_{v}^{*})$.",
        "context": "6 Appendix\n\n\nIn this section, we provide sufficient output results of GOT to show its outstanding OCR performance. We also demonstrate the format of the corresponding input prompt for different types of OCR tasks.\n\nPrompt: OCR with format:\n\n\nOutput:\n\n$$\\begin{gathered}\nd_{L}\\left(C_{L},\\left\\{v^{\\prime}\\right\\}\\right)=\\left|C_{L}\\right|+\\left|\\left\\{v^{\\prime}\\right\\}\\right|+2\\left(d_{T}\\left(C_{L}, v^{\\prime}\\right)-1\\right) \\\\\n=\\left|C_{v}\\right|-1+\\left|S_{v}^{*}\\right|+2(\\operatorname{rad} T-1) \\\\\n=\\left|C_{v}\\right|+\\left|S_{v}^{*}\\right|+2\\left(d_{T}\\left(C_{v}, S_{v}^{*}\\right)-1\\right) \\\\\n\\quad-1+2\\left(\\operatorname{rad} T-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=d_{L}\\left(C_{v}, S_{v}^{*}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) \\\\\n=e_{L}\\left(C_{v}\\right)+1+2\\left(\\operatorname{rad} T-1-d_{T}\\left(C_{v}, S_{v}^{*}\\right)\\right) .\n\\end{gathered}$$\n\nFigure 4: The formatted text OCR ability of GOT. GOT works well on full-page texts and table/formula slice texts. These input forms are the most commonly used in document OCR, which proves that GOT has great prospects in application.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$d_{L}(C_{v}, S_{v}^{*})+1+2(\\operatorname{rad} T-1-d_{T}(C_{v}, S_{v}^{*}))$",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0cc458db-4700-4f81-ab9f-f7cfdc99c42c",
        "questions": "What is the average multi-modal gain for the MMBench benchmark across all models?",
        "answers": "50.1",
        "context": "Table 6: Evaluation of various LVLMs on 7 Benchmarks with multi-modal gain (MG) and multi-modal leakage (ML) metrics. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. The bottom row represents the average across models for the same benchmark, while the rightmost column shows the average across benchmarks for the same LVLM. The best results are highlighted in bold and underlined. The worst results of MG and ML metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Model} & \\multirow[t]{2}{*}{Param.} & \\multicolumn{2}{|l|}{MMMU} & \\multicolumn{2}{|l|}{MMB} & \\multicolumn{2}{|l|}{ScienceQA} & \\multicolumn{2}{|l|}{AI2D} & \\multicolumn{2}{|l|}{SEED} & \\multicolumn{2}{|l|}{$\\mid$ MathVista} & \\multicolumn{2}{|l|}{MMStar} & \\multicolumn{2}{|r|}{Avg.} \\\\\n  & & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow \\mathrm{M}$ & ML $\\downarrow$ & MG\u4e2a & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & $\\uparrow \\mathrm{ML} \\downarrow \\mid$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & MG\u4e2a & $\\uparrow \\mathrm{ML} \\downarrow$ \\\\\n  \\multicolumn{18}{|c|}{Closed-source LVLMs} \\\\\n  GPT4V[35] & - & 8.5 & 3.9 & 52.0 & 5.4 & 13.2 & 3.9 & 12.8 & 2.8 & 43.2 & 18.3 & 19.3 & 1.2 & 32.6 & 1.3 & 25.9 & 5.3 \\\\\n  GeminiPro-Vision[41] & - & $\\overline{5.0}$ & 0.0 & 51.4 & 0.0 & 14.3 & 0.0 & 13.5 & 0.0 & 36.4 & 0.0 & 11.5 & 1.2 & 27.4 & 0.0 & $\\overline{22.8}$ & 0.2 \\\\\n  \\multicolumn{18}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & 3B & 6.0 & 10.0 & 45.9 & 13.8 & 6.8 & 15.2 & 10.5 & 13.2 & 32.9 & 10.8 & 5.4 & 1.5 & 16.4 & 7.6 & 17.7 & 10.3 \\\\\n  Yi-VL[49] & 6B & 5.3 & 7.4 & 45.6 & 14.1 & 5.1 & 9.4 & 3.9 & 16.6 & 29.2 & 10.9 & 3.8 & 3.0 & 15.6 & 0.0 & 15.5 & 8.8 \\\\\n  LLaVA-1.5[24] & 7B & 4.5 & 0.0 & 45.5 & & 4.6 & 5.2 & 6.9 & 6.2 & 28.1 & 4.9 & 3.3 & 0.0 & 10.7 & 0.0 & 14.8 & 3.6 \\\\\n  ShareGPT4V[5] & 7B & 3.5 & 1.8 & 49.1 & 10.1 & 4.2 & 6.3 & 8.5 & 6.9 & 31.7 & 5.1 & 3.0 & 0.7 & 11.9 & 0.0 & 16.0 & 4.4 \\\\\n  InternLM-XC2[12] & 7B & 7.5 & 1.4 & 53.4 & 17.3 & 24.8 & 7.9 & 18.1 & 15.0 & 36.8 & 6.2 & 28.0 & 10.5 & 28.1 & 7.5 & 28.1 & 9.4 \\\\\n  Qwen-VL-Chat[2] & 8B & 10.0 & 4.2 & 49.6 & 0.3 & 11.0 & 4.0 & 12.3 & 6.4 & 44.5 & ( 11.9 & 11.4 & 0.3 & 23.9 & 0.0 & 23.2 & 3.9 \\\\\n  Deepseek-VL[28] & 8B & 3.2 & 10.6 & 49.6 & 15.5 & 14.3 & 10.8 & 11.6 & 14.9 & 33.7 & 23.1 & 11.4 & 3.3 & 15.7 & 0.0 & 19.9 & 11.2 \\\\\n  Monkey-Chat[23] & 10B & 4.7 & 12.6 & 55.4 & 7.2 & 11.3 & 18.4 & 11.7 & 14.2 & 33.0 & 28.5 & 9.0 & 4.5 & 13.5 & 11.1 & 19.8 & 13.8 \\\\\n  LLaVA-1.5[24] & 13B & 9.6 & 0.0 & 47.2 & 9.8 & 5.7 & 7.0 & 8.6 & 7.2 & 31.1 & 10.7 & 5.3 & 1.5 & 13.9 & 0.0 & 17.3 & 5.2 \\\\\n  CogVLM-Chat[45] & 17B & 4.1 & 0.2 & 47.9 & 5.2 & 11.7 & 0.0 & 10.8 & 10.0 & 32.0 & 4.1 & 9.7 & 3.0 & 14.9 & 0 & 18.7 & 3.2 \\\\\n  Yi-VL[49] & 34B & 5.9 & 0.2 & 48.3 & 12.7 & 6.7 & 15.0 & 6.0 & 2.6 & 27.1 & 3.7 & 2.9 & 1.0 & 18.8 & 0.0 & 16.5 & 5.0 \\\\\n  LLaVA-Next[25] & 34B & 6.6 & 2.8 & 54.7 & 4.8 & 11.2 & 1.5 & 12.8 & 5.6 & 34.1 & 6.7 & 16.5 & 4.3 & 29.4 & 2.4 & 23.6 & 4.0 \\\\\n  InternVL-Chat-v1.2[6] & 40B & 7.4 & 4.1 & 58.5 & 3.8 & 12.2 & 0.9 & 13.5 & 4.8 & 34.9 & 5.5 & 23.7 & 6.1 & 32.6 & 0.0 & 26.1 & 3.6 \\\\\n  Sphinx-X-MoE[15] & 57B & 1.2 & 17.9 & 48.7 & 11.9 & 3.8 & 11.2 & 3.9 & 12.4 & 31.2 & 26.4 & 9.7 & 5.0 & 14.8 & $\\overline{1.0}$ & 16.2 & 12.3 \\\\\n  Avg. across models & - & 5.8 & 4.9 & 50.1 & 8.9 & 10.0 & 7.4 & 10.3 & 8.7 & 33.7 & $11.1 \\mid$ & 10.8 & 3.0 & 20.0 & 1.9 & - & - \\\\\n \n\\end{tabular}\nable given that LLaVA-1.5-7B employed the least amount of multi-modal training data among these open-source LVLMs. Despite LLaVA-1.5-7B having the lowest average multi-modal gain, it exhibits minimal multi-modal leakage. Additionally, models like Monkey-Chat, Spinx-X-MoE, and Deepspeek-VL display higher degrees of multi-modal leakage, highlighting the need for the community to consider this factor for fair comparisons.\nAnalysis from the benchmark perspective. In the final row of Table 6, we list the average multimodal gain and multi-modal leakage for existing LVLMs across all benchmarks for analysis. MMBench registers the highest average multi-modal gain at 50.1 , indicating a significant overlap between the domains covered by existing LVLMs' training data and MMBench. Conversely, MMMU exhibits the lowest average multi-modal gain at 5.8, suggesting a lesser degree of overlap between the domains of existing LVLMs' training corpora and those included in MMMU. Additionally, MMStar, as expected, has the lowest degree of multi-modal leakage at 1.9. This provides a comprehensive and fair arena for comparing existing LVLMs. Moreover, we believe evaluating existing LVLMs to derive average ML metrics can also be helpful to the following works in examining newly developed multi-modal benchmarks.\n\n\n6 Conclusion\n\n\nIn this work, we dig into current evaluation works for large vision-language models (LVLMs) and identify two primary issues: 1) visual content is unnecessary for many samples, and 2) unintentional data leakage exists in LLM and LVLM training. To address these issues, we develop an elite vision-dependent multi-modal benchmark named MMStar and propose two metrics to measure the data leakage and actual performance gain in LVLMs' multi-modal training. MMStar undergoes the manual review of each sample, covering 6 core capabilities and 18 detailed axes for an in-depth evaluation of LVLMs' multimodal capabilities. In our evaluation of 16 diverse LVLMs on MMStar, even the best model scores under 60 on average. We also analyze the MG and ML metrics across 6 multimodal benchmarks and MMStar, providing valuable insights for the community on gathering multimodal training data and crafting new benchmarks. In the future, we plan to expand MMStar into a larger, online test set and explore dynamic evaluation methods to maintain sample visual dependency and reduce accidental data leakage into LLM's and LVLM's training corpora.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "MMBench registers the highest average multi-modal gain at 50.1 , indicating a significant overlap between the domains covered by existing LVLMs' training data and MMBench.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0cce0637-c7ff-40a1-8239-4ef28c184ba0",
        "questions": "Which open-source LVLM model has the highest multi-modal leakage on the SEED benchmark?",
        "answers": "Monkey-Chat",
        "context": "Table 6: Evaluation of various LVLMs on 7 Benchmarks with multi-modal gain (MG) and multi-modal leakage (ML) metrics. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. The bottom row represents the average across models for the same benchmark, while the rightmost column shows the average across benchmarks for the same LVLM. The best results are highlighted in bold and underlined. The worst results of MG and ML metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Model} & \\multirow[t]{2}{*}{Param.} & \\multicolumn{2}{|l|}{MMMU} & \\multicolumn{2}{|l|}{MMB} & \\multicolumn{2}{|l|}{ScienceQA} & \\multicolumn{2}{|l|}{AI2D} & \\multicolumn{2}{|l|}{SEED} & \\multicolumn{2}{|l|}{$\\mid$ MathVista} & \\multicolumn{2}{|l|}{MMStar} & \\multicolumn{2}{|r|}{Avg.} \\\\\n  & & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow \\mathrm{M}$ & ML $\\downarrow$ & MG\u4e2a & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & $\\uparrow \\mathrm{ML} \\downarrow \\mid$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & MG\u4e2a & $\\uparrow \\mathrm{ML} \\downarrow$ \\\\\n  \\multicolumn{18}{|c|}{Closed-source LVLMs} \\\\\n  GPT4V[35] & - & 8.5 & 3.9 & 52.0 & 5.4 & 13.2 & 3.9 & 12.8 & 2.8 & 43.2 & 18.3 & 19.3 & 1.2 & 32.6 & 1.3 & 25.9 & 5.3 \\\\\n  GeminiPro-Vision[41] & - & $\\overline{5.0}$ & 0.0 & 51.4 & 0.0 & 14.3 & 0.0 & 13.5 & 0.0 & 36.4 & 0.0 & 11.5 & 1.2 & 27.4 & 0.0 & $\\overline{22.8}$ & 0.2 \\\\\n  \\multicolumn{18}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & 3B & 6.0 & 10.0 & 45.9 & 13.8 & 6.8 & 15.2 & 10.5 & 13.2 & 32.9 & 10.8 & 5.4 & 1.5 & 16.4 & 7.6 & 17.7 & 10.3 \\\\\n  Yi-VL[49] & 6B & 5.3 & 7.4 & 45.6 & 14.1 & 5.1 & 9.4 & 3.9 & 16.6 & 29.2 & 10.9 & 3.8 & 3.0 & 15.6 & 0.0 & 15.5 & 8.8 \\\\\n  LLaVA-1.5[24] & 7B & 4.5 & 0.0 & 45.5 & & 4.6 & 5.2 & 6.9 & 6.2 & 28.1 & 4.9 & 3.3 & 0.0 & 10.7 & 0.0 & 14.8 & 3.6 \\\\\n  ShareGPT4V[5] & 7B & 3.5 & 1.8 & 49.1 & 10.1 & 4.2 & 6.3 & 8.5 & 6.9 & 31.7 & 5.1 & 3.0 & 0.7 & 11.9 & 0.0 & 16.0 & 4.4 \\\\\n  InternLM-XC2[12] & 7B & 7.5 & 1.4 & 53.4 & 17.3 & 24.8 & 7.9 & 18.1 & 15.0 & 36.8 & 6.2 & 28.0 & 10.5 & 28.1 & 7.5 & 28.1 & 9.4 \\\\\n  Qwen-VL-Chat[2] & 8B & 10.0 & 4.2 & 49.6 & 0.3 & 11.0 & 4.0 & 12.3 & 6.4 & 44.5 & ( 11.9 & 11.4 & 0.3 & 23.9 & 0.0 & 23.2 & 3.9 \\\\\n  Deepseek-VL[28] & 8B & 3.2 & 10.6 & 49.6 & 15.5 & 14.3 & 10.8 & 11.6 & 14.9 & 33.7 & 23.1 & 11.4 & 3.3 & 15.7 & 0.0 & 19.9 & 11.2 \\\\\n  Monkey-Chat[23] & 10B & 4.7 & 12.6 & 55.4 & 7.2 & 11.3 & 18.4 & 11.7 & 14.2 & 33.0 & 28.5 & 9.0 & 4.5 & 13.5 & 11.1 & 19.8 & 13.8 \\\\\n  LLaVA-1.5[24] & 13B & 9.6 & 0.0 & 47.2 & 9.8 & 5.7 & 7.0 & 8.6 & 7.2 & 31.1 & 10.7 & 5.3 & 1.5 & 13.9 & 0.0 & 17.3 & 5.2 \\\\\n  CogVLM-Chat[45] & 17B & 4.1 & 0.2 & 47.9 & 5.2 & 11.7 & 0.0 & 10.8 & 10.0 & 32.0 & 4.1 & 9.7 & 3.0 & 14.9 & 0 & 18.7 & 3.2 \\\\\n  Yi-VL[49] & 34B & 5.9 & 0.2 & 48.3 & 12.7 & 6.7 & 15.0 & 6.0 & 2.6 & 27.1 & 3.7 & 2.9 & 1.0 & 18.8 & 0.0 & 16.5 & 5.0 \\\\\n  LLaVA-Next[25] & 34B & 6.6 & 2.8 & 54.7 & 4.8 & 11.2 & 1.5 & 12.8 & 5.6 & 34.1 & 6.7 & 16.5 & 4.3 & 29.4 & 2.4 & 23.6 & 4.0 \\\\\n  InternVL-Chat-v1.2[6] & 40B & 7.4 & 4.1 & 58.5 & 3.8 & 12.2 & 0.9 & 13.5 & 4.8 & 34.9 & 5.5 & 23.7 & 6.1 & 32.6 & 0.0 & 26.1 & 3.6 \\\\\n  Sphinx-X-MoE[15] & 57B & 1.2 & 17.9 & 48.7 & 11.9 & 3.8 & 11.2 & 3.9 & 12.4 & 31.2 & 26.4 & 9.7 & 5.0 & 14.8 & $\\overline{1.0}$ & 16.2 & 12.3 \\\\\n  Avg. across models & - & 5.8 & 4.9 & 50.1 & 8.9 & 10.0 & 7.4 & 10.3 & 8.7 & 33.7 & $11.1 \\mid$ & 10.8 & 3.0 & 20.0 & 1.9 & - & - \\\\\n \n\\end{tabular}\nable given that LLaVA-1.5-7B employed the least amount of multi-modal training data among these open-source LVLMs. Despite LLaVA-1.5-7B having the lowest average multi-modal gain, it exhibits minimal multi-modal leakage. Additionally, models like Monkey-Chat, Spinx-X-MoE, and Deepspeek-VL display higher degrees of multi-modal leakage, highlighting the need for the community to consider this factor for fair comparisons.\nAnalysis from the benchmark perspective. In the final row of Table 6, we list the average multimodal gain and multi-modal leakage for existing LVLMs across all benchmarks for analysis. MMBench registers the highest average multi-modal gain at 50.1 , indicating a significant overlap between the domains covered by existing LVLMs' training data and MMBench. Conversely, MMMU exhibits the lowest average multi-modal gain at 5.8, suggesting a lesser degree of overlap between the domains of existing LVLMs' training corpora and those included in MMMU. Additionally, MMStar, as expected, has the lowest degree of multi-modal leakage at 1.9. This provides a comprehensive and fair arena for comparing existing LVLMs. Moreover, we believe evaluating existing LVLMs to derive average ML metrics can also be helpful to the following works in examining newly developed multi-modal benchmarks.\n\n\n6 Conclusion\n\n\nIn this work, we dig into current evaluation works for large vision-language models (LVLMs) and identify two primary issues: 1) visual content is unnecessary for many samples, and 2) unintentional data leakage exists in LLM and LVLM training. To address these issues, we develop an elite vision-dependent multi-modal benchmark named MMStar and propose two metrics to measure the data leakage and actual performance gain in LVLMs' multi-modal training. MMStar undergoes the manual review of each sample, covering 6 core capabilities and 18 detailed axes for an in-depth evaluation of LVLMs' multimodal capabilities. In our evaluation of 16 diverse LVLMs on MMStar, even the best model scores under 60 on average. We also analyze the MG and ML metrics across 6 multimodal benchmarks and MMStar, providing valuable insights for the community on gathering multimodal training data and crafting new benchmarks. In the future, we plan to expand MMStar into a larger, online test set and explore dynamic evaluation methods to maintain sample visual dependency and reduce accidental data leakage into LLM's and LVLM's training corpora.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Monkey-Chat[23] & 10B & 4.7 & 12.6 & 55.4 & 7.2 & 11.3 & 18.4 & 11.7 & 14.2 & 33.0 & 28.5 & 9.0 & 4.5 & 13.5 & 11.1 & 19.8 & 13.8",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0cda520b-9aaf-4a93-a0f2-10f006ade140",
        "questions": "Does the MMStar benchmark have the lowest degree of multi-modal leakage among the benchmarks evaluated?",
        "answers": "Yes",
        "context": "Table 6: Evaluation of various LVLMs on 7 Benchmarks with multi-modal gain (MG) and multi-modal leakage (ML) metrics. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. The bottom row represents the average across models for the same benchmark, while the rightmost column shows the average across benchmarks for the same LVLM. The best results are highlighted in bold and underlined. The worst results of MG and ML metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{Model} & \\multirow[t]{2}{*}{Param.} & \\multicolumn{2}{|l|}{MMMU} & \\multicolumn{2}{|l|}{MMB} & \\multicolumn{2}{|l|}{ScienceQA} & \\multicolumn{2}{|l|}{AI2D} & \\multicolumn{2}{|l|}{SEED} & \\multicolumn{2}{|l|}{$\\mid$ MathVista} & \\multicolumn{2}{|l|}{MMStar} & \\multicolumn{2}{|r|}{Avg.} \\\\\n  & & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow \\mathrm{M}$ & ML $\\downarrow$ & MG\u4e2a & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & $\\uparrow \\mathrm{ML} \\downarrow \\mid$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ & MG\u4e2a & $\\uparrow \\mathrm{ML} \\downarrow$ \\\\\n  \\multicolumn{18}{|c|}{Closed-source LVLMs} \\\\\n  GPT4V[35] & - & 8.5 & 3.9 & 52.0 & 5.4 & 13.2 & 3.9 & 12.8 & 2.8 & 43.2 & 18.3 & 19.3 & 1.2 & 32.6 & 1.3 & 25.9 & 5.3 \\\\\n  GeminiPro-Vision[41] & - & $\\overline{5.0}$ & 0.0 & 51.4 & 0.0 & 14.3 & 0.0 & 13.5 & 0.0 & 36.4 & 0.0 & 11.5 & 1.2 & 27.4 & 0.0 & $\\overline{22.8}$ & 0.2 \\\\\n  \\multicolumn{18}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & 3B & 6.0 & 10.0 & 45.9 & 13.8 & 6.8 & 15.2 & 10.5 & 13.2 & 32.9 & 10.8 & 5.4 & 1.5 & 16.4 & 7.6 & 17.7 & 10.3 \\\\\n  Yi-VL[49] & 6B & 5.3 & 7.4 & 45.6 & 14.1 & 5.1 & 9.4 & 3.9 & 16.6 & 29.2 & 10.9 & 3.8 & 3.0 & 15.6 & 0.0 & 15.5 & 8.8 \\\\\n  LLaVA-1.5[24] & 7B & 4.5 & 0.0 & 45.5 & & 4.6 & 5.2 & 6.9 & 6.2 & 28.1 & 4.9 & 3.3 & 0.0 & 10.7 & 0.0 & 14.8 & 3.6 \\\\\n  ShareGPT4V[5] & 7B & 3.5 & 1.8 & 49.1 & 10.1 & 4.2 & 6.3 & 8.5 & 6.9 & 31.7 & 5.1 & 3.0 & 0.7 & 11.9 & 0.0 & 16.0 & 4.4 \\\\\n  InternLM-XC2[12] & 7B & 7.5 & 1.4 & 53.4 & 17.3 & 24.8 & 7.9 & 18.1 & 15.0 & 36.8 & 6.2 & 28.0 & 10.5 & 28.1 & 7.5 & 28.1 & 9.4 \\\\\n  Qwen-VL-Chat[2] & 8B & 10.0 & 4.2 & 49.6 & 0.3 & 11.0 & 4.0 & 12.3 & 6.4 & 44.5 & ( 11.9 & 11.4 & 0.3 & 23.9 & 0.0 & 23.2 & 3.9 \\\\\n  Deepseek-VL[28] & 8B & 3.2 & 10.6 & 49.6 & 15.5 & 14.3 & 10.8 & 11.6 & 14.9 & 33.7 & 23.1 & 11.4 & 3.3 & 15.7 & 0.0 & 19.9 & 11.2 \\\\\n  Monkey-Chat[23] & 10B & 4.7 & 12.6 & 55.4 & 7.2 & 11.3 & 18.4 & 11.7 & 14.2 & 33.0 & 28.5 & 9.0 & 4.5 & 13.5 & 11.1 & 19.8 & 13.8 \\\\\n  LLaVA-1.5[24] & 13B & 9.6 & 0.0 & 47.2 & 9.8 & 5.7 & 7.0 & 8.6 & 7.2 & 31.1 & 10.7 & 5.3 & 1.5 & 13.9 & 0.0 & 17.3 & 5.2 \\\\\n  CogVLM-Chat[45] & 17B & 4.1 & 0.2 & 47.9 & 5.2 & 11.7 & 0.0 & 10.8 & 10.0 & 32.0 & 4.1 & 9.7 & 3.0 & 14.9 & 0 & 18.7 & 3.2 \\\\\n  Yi-VL[49] & 34B & 5.9 & 0.2 & 48.3 & 12.7 & 6.7 & 15.0 & 6.0 & 2.6 & 27.1 & 3.7 & 2.9 & 1.0 & 18.8 & 0.0 & 16.5 & 5.0 \\\\\n  LLaVA-Next[25] & 34B & 6.6 & 2.8 & 54.7 & 4.8 & 11.2 & 1.5 & 12.8 & 5.6 & 34.1 & 6.7 & 16.5 & 4.3 & 29.4 & 2.4 & 23.6 & 4.0 \\\\\n  InternVL-Chat-v1.2[6] & 40B & 7.4 & 4.1 & 58.5 & 3.8 & 12.2 & 0.9 & 13.5 & 4.8 & 34.9 & 5.5 & 23.7 & 6.1 & 32.6 & 0.0 & 26.1 & 3.6 \\\\\n  Sphinx-X-MoE[15] & 57B & 1.2 & 17.9 & 48.7 & 11.9 & 3.8 & 11.2 & 3.9 & 12.4 & 31.2 & 26.4 & 9.7 & 5.0 & 14.8 & $\\overline{1.0}$ & 16.2 & 12.3 \\\\\n  Avg. across models & - & 5.8 & 4.9 & 50.1 & 8.9 & 10.0 & 7.4 & 10.3 & 8.7 & 33.7 & $11.1 \\mid$ & 10.8 & 3.0 & 20.0 & 1.9 & - & - \\\\\n \n\\end{tabular}\nable given that LLaVA-1.5-7B employed the least amount of multi-modal training data among these open-source LVLMs. Despite LLaVA-1.5-7B having the lowest average multi-modal gain, it exhibits minimal multi-modal leakage. Additionally, models like Monkey-Chat, Spinx-X-MoE, and Deepspeek-VL display higher degrees of multi-modal leakage, highlighting the need for the community to consider this factor for fair comparisons.\nAnalysis from the benchmark perspective. In the final row of Table 6, we list the average multimodal gain and multi-modal leakage for existing LVLMs across all benchmarks for analysis. MMBench registers the highest average multi-modal gain at 50.1 , indicating a significant overlap between the domains covered by existing LVLMs' training data and MMBench. Conversely, MMMU exhibits the lowest average multi-modal gain at 5.8, suggesting a lesser degree of overlap between the domains of existing LVLMs' training corpora and those included in MMMU. Additionally, MMStar, as expected, has the lowest degree of multi-modal leakage at 1.9. This provides a comprehensive and fair arena for comparing existing LVLMs. Moreover, we believe evaluating existing LVLMs to derive average ML metrics can also be helpful to the following works in examining newly developed multi-modal benchmarks.\n\n\n6 Conclusion\n\n\nIn this work, we dig into current evaluation works for large vision-language models (LVLMs) and identify two primary issues: 1) visual content is unnecessary for many samples, and 2) unintentional data leakage exists in LLM and LVLM training. To address these issues, we develop an elite vision-dependent multi-modal benchmark named MMStar and propose two metrics to measure the data leakage and actual performance gain in LVLMs' multi-modal training. MMStar undergoes the manual review of each sample, covering 6 core capabilities and 18 detailed axes for an in-depth evaluation of LVLMs' multimodal capabilities. In our evaluation of 16 diverse LVLMs on MMStar, even the best model scores under 60 on average. We also analyze the MG and ML metrics across 6 multimodal benchmarks and MMStar, providing valuable insights for the community on gathering multimodal training data and crafting new benchmarks. In the future, we plan to expand MMStar into a larger, online test set and explore dynamic evaluation methods to maintain sample visual dependency and reduce accidental data leakage into LLM's and LVLM's training corpora.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Additionally, MMStar, as expected, has the lowest degree of multi-modal leakage at 1.9.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0cdff20e-c1e7-4ade-858e-8dfb98468947",
        "questions": "What percentage did GeminiPro achieve on the MMMU benchmark without any visual input?",
        "answers": "42.9%",
        "context": "Are We on the Right Way for Evaluating Large Vision-Language Models?\n\n\n\nLin Chen ${ ^{1,3 *}$ Jinsong Li $^{2,3 *}$ Xiaoyi Dong ${ }^{2,3}$ Pan Zhang ${ }^{3}$ Yuhang Zang ${ }^{3}$ Zehui Chen ${ }^{1}$ Haodong Duan ${ }^{3}$ Jiaqi Wang ${ }^{3 \\dagger}$ Yu Qiao $^{3}$ Dahua Lin ${ }^{2,3}$ Feng Zhao $^{1 \\dagger}$ \\\\ ${ }^{1}$ University of Science and Technology of China \\\\ ${ }^{2}$ The Chinese University of Hong Kong \\\\ ${ }^{3}$ Shanghai AI Laboratory \\\\ https://mmstar-benchmark.github.io/\n}\n\n\nLarge vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over $24 \\%$ on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.\n\n\n1 Introduction\n\nEncouraged by the rapid development of large language models (LLMs) [47, 4, 8, 9, 13, 1, 43], integrating visual modality into LLMs to enhance models' interactivity capabilities has witnessed ever-changing advances in recent days [54, 26, 24, 11, 52, 2, 48, 31, 5, 12]. These large visionlanguage models (LVLMs) showcase powerful visual perception and understanding capabilities, enabling them to accept image inputs from users and engage in dialogues, thereby offering a more enriched interactive experience. These achievements have further inspired the research community\n\n\\footnotetext{\n${ }^{*}$ Equal contribution. This work is done during internship in Shanghai AI Laboratory.\n${ }^{\\dagger}$ Correspoding author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For instance, GeminiPro achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over $24 \\%$ on average.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ce19926-aaad-4b0d-a55c-cf51f3e77c60",
        "questions": "What is the name of the benchmark that comprises 1,500 samples selected to evaluate LVLMs' multi-modal capacities?",
        "answers": "MMStar",
        "context": "Are We on the Right Way for Evaluating Large Vision-Language Models?\n\n\n\nLin Chen ${ ^{1,3 *}$ Jinsong Li $^{2,3 *}$ Xiaoyi Dong ${ }^{2,3}$ Pan Zhang ${ }^{3}$ Yuhang Zang ${ }^{3}$ Zehui Chen ${ }^{1}$ Haodong Duan ${ }^{3}$ Jiaqi Wang ${ }^{3 \\dagger}$ Yu Qiao $^{3}$ Dahua Lin ${ }^{2,3}$ Feng Zhao $^{1 \\dagger}$ \\\\ ${ }^{1}$ University of Science and Technology of China \\\\ ${ }^{2}$ The Chinese University of Hong Kong \\\\ ${ }^{3}$ Shanghai AI Laboratory \\\\ https://mmstar-benchmark.github.io/\n}\n\n\nLarge vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over $24 \\%$ on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.\n\n\n1 Introduction\n\nEncouraged by the rapid development of large language models (LLMs) [47, 4, 8, 9, 13, 1, 43], integrating visual modality into LLMs to enhance models' interactivity capabilities has witnessed ever-changing advances in recent days [54, 26, 24, 11, 52, 2, 48, 31, 5, 12]. These large visionlanguage models (LVLMs) showcase powerful visual perception and understanding capabilities, enabling them to accept image inputs from users and engage in dialogues, thereby offering a more enriched interactive experience. These achievements have further inspired the research community\n\n\\footnotetext{\n${ }^{*}$ Equal contribution. This work is done during internship in Shanghai AI Laboratory.\n${ }^{\\dagger}$ Correspoding author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ce22c82-6262-402c-af46-d003d2cc42b1",
        "questions": "Does Sphinx-X-MoE surpass its LLM backbone on the MMMU benchmark without accessing images?",
        "answers": "Yes",
        "context": "Are We on the Right Way for Evaluating Large Vision-Language Models?\n\n\n\nLin Chen ${ ^{1,3 *}$ Jinsong Li $^{2,3 *}$ Xiaoyi Dong ${ }^{2,3}$ Pan Zhang ${ }^{3}$ Yuhang Zang ${ }^{3}$ Zehui Chen ${ }^{1}$ Haodong Duan ${ }^{3}$ Jiaqi Wang ${ }^{3 \\dagger}$ Yu Qiao $^{3}$ Dahua Lin ${ }^{2,3}$ Feng Zhao $^{1 \\dagger}$ \\\\ ${ }^{1}$ University of Science and Technology of China \\\\ ${ }^{2}$ The Chinese University of Hong Kong \\\\ ${ }^{3}$ Shanghai AI Laboratory \\\\ https://mmstar-benchmark.github.io/\n}\n\n\nLarge vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over $24 \\%$ on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.\n\n\n1 Introduction\n\nEncouraged by the rapid development of large language models (LLMs) [47, 4, 8, 9, 13, 1, 43], integrating visual modality into LLMs to enhance models' interactivity capabilities has witnessed ever-changing advances in recent days [54, 26, 24, 11, 52, 2, 48, 31, 5, 12]. These large visionlanguage models (LVLMs) showcase powerful visual perception and understanding capabilities, enabling them to accept image inputs from users and engage in dialogues, thereby offering a more enriched interactive experience. These achievements have further inspired the research community\n\n\\footnotetext{\n${ }^{*}$ Equal contribution. This work is done during internship in Shanghai AI Laboratory.\n${ }^{\\dagger}$ Correspoding author.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "For example, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ce72660-40b7-4fa8-ba92-f71ca40573b3",
        "questions": "What is the formula for calculating the multi-modal gain (MG) metric for a given Large Vision-Language Model (LVLM) on a particular benchmark?",
        "answers": "MG = S_v - S_wv",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "To calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation: $$M G=S_{v}-S_{w v}$$",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d08f653-9d76-4420-a4aa-f2b63602fef9",
        "questions": "Which two closed-source Large Vision-Language Models (LVLMs) were prepared for the experiments conducted in the study?",
        "answers": "GPT4V and GeminiPro-Vision",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d0bb530-85a9-4ac4-97bb-f4d15fb16a54",
        "questions": "Is the 0-shot strategy used for evaluating Large Vision-Language Models (LVLMs) on the MMStar benchmark?",
        "answers": "No",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d0bd1c4-921a-4d1c-9408-ebd5e17425e7",
        "questions": "How many high-quality samples were selected to construct the MMStar benchmark after the data curation process?",
        "answers": "1,500",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d0c61fd-4700-4eb8-b408-850e0fdc26f4",
        "questions": "What is the purpose of employing two closed-source LLMs and six open-source LLMs during the data curation process for the MMStar benchmark?",
        "answers": "To serve as inspectors for preliminarily filtering out samples that do not meet the first two criteria.",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d0d82f9-1b13-4405-b484-db42e58000cf",
        "questions": "What percentage of LLM inspectors must fail to provide the correct answer for a sample to be retained after the initial coarse filtering in the MMStar benchmark data curation process?",
        "answers": "Around 75%",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d0da057-270a-4e47-b278-3dc3691712fc",
        "questions": "What is the average score achieved by the GPT4V model using the LVLM strategy across the six popular multi-modal benchmarks?",
        "answers": "66.0",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT4V[35] (GPT4-Turbo[34]) & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d148001-3440-4f39-a66f-6fa04289e5b6",
        "questions": "Which model achieved the highest average score using the LVLM-text strategy among the open-source LVLMs?",
        "answers": "InternLM2-XC2",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "InternLM2-XC2[12] (InternLM2-7B[42]) & 7B & LVLM-text & 34.2 & 26.2/79.6 & 71.9 & 63.3 & 38.1 & 29.4/57.4 & 43.9",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d16422e-94db-4613-9fb5-559e8b9e1fb2",
        "questions": "Does the document suggest that data leakage in LLM and LVLM training can lead to unfair comparisons in benchmark evaluations?",
        "answers": "Yes",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d270777-8762-4cc3-a724-c1fd5e1641ef",
        "questions": "Which closed-source LLM achieved the highest average score across the six multi-modal benchmarks?",
        "answers": "GeminiPro",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d38bf6e-86a2-49d7-be35-ea22fd1b285d",
        "questions": "What is the average score of the open-source LLM model InternLM2-7B across the six multi-modal benchmarks?",
        "answers": "34.1",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d58f90a-6eb8-4594-a47b-7b4da46dfd05",
        "questions": "Does the document suggest that visual content is always necessary for evaluating LVLMs on multi-modal benchmarks?",
        "answers": "No",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "First issue: visual content is unnecessary for many evaluation samples.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d5e0895-d751-4fe6-983f-c86afad2873a",
        "questions": "Which paper discusses the development of a benchmark for general-purpose foundation models on low-level vision?",
        "answers": "Q-bench: A benchmark for general-purpose foundation models on low-level vision.",
        "context": "[43] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[44] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.\n[45] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.\n[46] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.\n[47] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.\n[48] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[49] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.\n[50] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n[51] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\n[52] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.\n[53] B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.\n[54] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d7198d2-b6c7-4c17-8ea9-39c51a2ae07f",
        "questions": "What is the arXiv identifier for the paper authored by A. Young and others on open foundation models by 01.ai?",
        "answers": "arXiv:2403.04652",
        "context": "[43] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[44] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.\n[45] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.\n[46] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.\n[47] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.\n[48] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[49] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.\n[50] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n[51] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\n[52] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.\n[53] B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.\n[54] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d742256-0b36-43cd-8acf-eba4441030ae",
        "questions": "Is the paper titled 'Tinyllava: A framework of small-scale large multimodal models' expected to be published in 2024?",
        "answers": "Yes",
        "context": "[43] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[44] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023.\n[45] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.\n[46] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.\n[47] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.\n[48] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n[49] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.\n[50] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n[51] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.\n[52] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023.\n[53] B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.\n[54] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "B. Zhou, Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang. Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024.",
        "evidence_page_no": 19,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d76aa16-2605-4ad2-9e4b-45b57cbb6187",
        "questions": "What percentage of questions in the ScienceQA benchmark can be solved by most LLMs without relying on visual content?",
        "answers": "More than 50%",
        "context": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.\nto develop a variety of multi-modal benchmarks $[21,14,27,38,50,51,29,20,30]$, constructed to explore the powerful capabilities emerging from LVLMs and provide a comprehensive and objective platform for quantitatively comparing the continually evolving models. Despite the race among existing evaluation works to construct as many axes as possible to assess the capabilities of LVLMs, we have identified two primary issues upon delving into existing evaluation samples and processes.\n\nFirst, visual content is unnecessary for many samples. A qualified multi-modal evaluation sample should compel LVLMs to understand and reason with the visual content for correct answers. Otherwise, the evaluation sample would degrade into assessing the textual capabilities of LLM bases. Unfortunately, we have identified numerous samples across multiple popular benchmarks [27, 21, 51, 30, 20] where answers can be correctly deduced without relying on visual content. As shown in Figure 1 (a) and (b), some samples have answers directly included within the questions (e.g., What is the shape of the round dirt circle?), while others can be effortlessly answered by leveraging the rich world knowledge embedded within the LLM bases (e.g., What is the capital of Nebraska?). With a comprehensive quantitative analysis of 25 LLMs on 6 benchmarks, we observe this phenomenon is prevalent and serious. For example, more than $50 \\%$ questions of ScienceQA and $20 \\%$ questions of MMMU can be solved by most LLMs directly. For the powerful close source LLM GeminiPro, it achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks by over $24 \\%$ on average.\nTaking aside the inappropriate samples in evaluation, we also observed strange results that LLM and LVLM could still answer some visual-necessary questions without visual content (Figure 1 (c) and (d)). A plausible explanation for this could be the inadvertent memorization of these samples during the large-scale training process, suggesting the presence of unintentional data leakage in the training of LLM and LVLM. Through a detailed study of 22 LVLMs on the 6 benchmarks, we find the unexpected leaking problem during the LVLM training is particularly serious. For example, we find Yi-VL-34B gets $15.0 \\%$ higher performance than its LLM backbone on ScienceQA, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$, even surpassing many leading LVLMs with accessing images.\n\nThe existence of inappropriate questions and data leaking would lead to misjudgments of actual multi-modal performance gains and potentially misguide the study of LVLM. In pursuit of a more accurate and comprehensive evaluation, we introduce the MMStar Benchmark. MMStar is a premier, vision-critical multi-modal benchmark that includes 1,500 challenging samples, each rigorously validated by humans. It is structured to test 6 fundamental capabilities and 18 specific di-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For example, more than $50 \\%$ questions of ScienceQA and $20 \\%$ questions of MMMU can be solved by most LLMs directly.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d898ef3-c657-44eb-a9e9-4d344dfa119e",
        "questions": "What is the performance increase of Yi-VL-34B compared to its LLM backbone on the ScienceQA benchmark?",
        "answers": "15.0%",
        "context": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.\nto develop a variety of multi-modal benchmarks $[21,14,27,38,50,51,29,20,30]$, constructed to explore the powerful capabilities emerging from LVLMs and provide a comprehensive and objective platform for quantitatively comparing the continually evolving models. Despite the race among existing evaluation works to construct as many axes as possible to assess the capabilities of LVLMs, we have identified two primary issues upon delving into existing evaluation samples and processes.\n\nFirst, visual content is unnecessary for many samples. A qualified multi-modal evaluation sample should compel LVLMs to understand and reason with the visual content for correct answers. Otherwise, the evaluation sample would degrade into assessing the textual capabilities of LLM bases. Unfortunately, we have identified numerous samples across multiple popular benchmarks [27, 21, 51, 30, 20] where answers can be correctly deduced without relying on visual content. As shown in Figure 1 (a) and (b), some samples have answers directly included within the questions (e.g., What is the shape of the round dirt circle?), while others can be effortlessly answered by leveraging the rich world knowledge embedded within the LLM bases (e.g., What is the capital of Nebraska?). With a comprehensive quantitative analysis of 25 LLMs on 6 benchmarks, we observe this phenomenon is prevalent and serious. For example, more than $50 \\%$ questions of ScienceQA and $20 \\%$ questions of MMMU can be solved by most LLMs directly. For the powerful close source LLM GeminiPro, it achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks by over $24 \\%$ on average.\nTaking aside the inappropriate samples in evaluation, we also observed strange results that LLM and LVLM could still answer some visual-necessary questions without visual content (Figure 1 (c) and (d)). A plausible explanation for this could be the inadvertent memorization of these samples during the large-scale training process, suggesting the presence of unintentional data leakage in the training of LLM and LVLM. Through a detailed study of 22 LVLMs on the 6 benchmarks, we find the unexpected leaking problem during the LVLM training is particularly serious. For example, we find Yi-VL-34B gets $15.0 \\%$ higher performance than its LLM backbone on ScienceQA, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$, even surpassing many leading LVLMs with accessing images.\n\nThe existence of inappropriate questions and data leaking would lead to misjudgments of actual multi-modal performance gains and potentially misguide the study of LVLM. In pursuit of a more accurate and comprehensive evaluation, we introduce the MMStar Benchmark. MMStar is a premier, vision-critical multi-modal benchmark that includes 1,500 challenging samples, each rigorously validated by humans. It is structured to test 6 fundamental capabilities and 18 specific di-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For example, we find Yi-VL-34B gets $15.0 \\%$ higher performance than its LLM backbone on ScienceQA.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d9b598a-ec57-4e06-8e7e-fc7e2568533e",
        "questions": "Does the MMStar Benchmark include samples that are rigorously validated by humans?",
        "answers": "Yes",
        "context": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.\nto develop a variety of multi-modal benchmarks $[21,14,27,38,50,51,29,20,30]$, constructed to explore the powerful capabilities emerging from LVLMs and provide a comprehensive and objective platform for quantitatively comparing the continually evolving models. Despite the race among existing evaluation works to construct as many axes as possible to assess the capabilities of LVLMs, we have identified two primary issues upon delving into existing evaluation samples and processes.\n\nFirst, visual content is unnecessary for many samples. A qualified multi-modal evaluation sample should compel LVLMs to understand and reason with the visual content for correct answers. Otherwise, the evaluation sample would degrade into assessing the textual capabilities of LLM bases. Unfortunately, we have identified numerous samples across multiple popular benchmarks [27, 21, 51, 30, 20] where answers can be correctly deduced without relying on visual content. As shown in Figure 1 (a) and (b), some samples have answers directly included within the questions (e.g., What is the shape of the round dirt circle?), while others can be effortlessly answered by leveraging the rich world knowledge embedded within the LLM bases (e.g., What is the capital of Nebraska?). With a comprehensive quantitative analysis of 25 LLMs on 6 benchmarks, we observe this phenomenon is prevalent and serious. For example, more than $50 \\%$ questions of ScienceQA and $20 \\%$ questions of MMMU can be solved by most LLMs directly. For the powerful close source LLM GeminiPro, it achieves $42.9 \\%$ on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks by over $24 \\%$ on average.\nTaking aside the inappropriate samples in evaluation, we also observed strange results that LLM and LVLM could still answer some visual-necessary questions without visual content (Figure 1 (c) and (d)). A plausible explanation for this could be the inadvertent memorization of these samples during the large-scale training process, suggesting the presence of unintentional data leakage in the training of LLM and LVLM. Through a detailed study of 22 LVLMs on the 6 benchmarks, we find the unexpected leaking problem during the LVLM training is particularly serious. For example, we find Yi-VL-34B gets $15.0 \\%$ higher performance than its LLM backbone on ScienceQA, Sphinx-X-MoE gets $43.6 \\%$ on MMMU without accessing images, surpassing its LLM backbone with $17.9 \\%$, even surpassing many leading LVLMs with accessing images.\n\nThe existence of inappropriate questions and data leaking would lead to misjudgments of actual multi-modal performance gains and potentially misguide the study of LVLM. In pursuit of a more accurate and comprehensive evaluation, we introduce the MMStar Benchmark. MMStar is a premier, vision-critical multi-modal benchmark that includes 1,500 challenging samples, each rigorously validated by humans. It is structured to test 6 fundamental capabilities and 18 specific di-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "In pursuit of a more accurate and comprehensive evaluation, we introduce the MMStar Benchmark. MMStar is a premier, vision-critical multi-modal benchmark that includes 1,500 challenging samples, each rigorously validated by humans.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0d9ecdf1-29eb-4c72-a19e-a432f78c9f9b",
        "questions": "Which open-source LVLM achieved the highest average multi-modal gain across all benchmarks?",
        "answers": "InternLM-XComposer2",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0db11439-213a-4a5f-b985-0730c5df290b",
        "questions": "What is the parameter size of the LLaVA-Next model evaluated on MMStar?",
        "answers": "34 B",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0db5d5b3-0286-4463-85d1-feeb9f493357",
        "questions": "Did any LVLMs achieve a passing average score of 60% in the core capabilities of fine-grained perception, logical reasoning, science & technology, and mathematics?",
        "answers": "No",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0dbcad25-daac-43a3-82f8-9ef40b9bcb7e",
        "questions": "How many samples are there for each core capability in the MMStar benchmark?",
        "answers": "250",
        "context": "Figure 5: Distribution of capability dimensions on the MMStar benchmark. In MMStar, we display 6 core capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the number of samples for each detailed dimension. Each core capability contains a meticulously balanced 250 samples. We further ensure a relatively even distribution across the 18 detailed axes.\n\nIn Figure 5, we provide statistics for each core capability and their detailed axes on the MMStar benchmark.\n\nCoarse Perception (CP). This core dimension refers to the capability to understand and interpret the overarching characteristics and themes of an image without delving into the finer details. It encompasses a broad, holistic view of the visual content, enabling the identification of: 1) image style \\& quality; 2) image scene \\& topic; and 3) image emotion.\n\nFine-grained Perception (FP). This core dimension represents a sophisticated level of image understanding that focuses on the detailed and nuanced aspects of visual content. It involves a deep dive into the specifics of images: 1) attribute \\& celebrity recognition; 2) object location; and 3) object counting. This core dimension unveils the subtle intricacies that coarse perception might overlook.\nInstance Reasoning (IR). It encapsulates a set of advanced cognitive capabilities focused on understanding and interpreting individual and collective object attributes and interrelations within an image. This process goes beyond mere recognition, delving into the analytical assessment of: 1) single-instance attribute reasoning; 2) cross-instance attribute comparison; and 3) cross-instance relation reasoning. It is a critical component for systems requiring a deep semantic understanding of visual content, enabling nuanced interaction with and response to complex visual content.\nLogical Reasoning (LR). This core dimension encompasses a sophisticated framework of cognitive processes designed to interpret, deduce, and infer conclusions from visual content through a structured approach to logic and reasoning. This multi-faceted capability marries the intuitive understanding of visual content with the structured rigor of logical deduction, enabling: 1) diagram reasoning; 2) code \\& sequence reasoning; and 3) common reasoning.\nScience \\& Technology (ST). It consists of a comprehensive framework for the application and integration of knowledge across a broad spectrum of science and technology. This domain combines the theoretical underpinnings and practical applications of various fields: 1) natural science; 2) engineering; and 3) geography \\& earth science.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Each core capability contains a meticulously balanced 250 samples.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0dbed695-5bcf-4ce1-96f0-e0606aca8118",
        "questions": "What are the three detailed aspects of Fine-grained Perception in the MMStar benchmark?",
        "answers": "1) attribute & celebrity recognition; 2) object location; and 3) object counting",
        "context": "Figure 5: Distribution of capability dimensions on the MMStar benchmark. In MMStar, we display 6 core capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the number of samples for each detailed dimension. Each core capability contains a meticulously balanced 250 samples. We further ensure a relatively even distribution across the 18 detailed axes.\n\nIn Figure 5, we provide statistics for each core capability and their detailed axes on the MMStar benchmark.\n\nCoarse Perception (CP). This core dimension refers to the capability to understand and interpret the overarching characteristics and themes of an image without delving into the finer details. It encompasses a broad, holistic view of the visual content, enabling the identification of: 1) image style \\& quality; 2) image scene \\& topic; and 3) image emotion.\n\nFine-grained Perception (FP). This core dimension represents a sophisticated level of image understanding that focuses on the detailed and nuanced aspects of visual content. It involves a deep dive into the specifics of images: 1) attribute \\& celebrity recognition; 2) object location; and 3) object counting. This core dimension unveils the subtle intricacies that coarse perception might overlook.\nInstance Reasoning (IR). It encapsulates a set of advanced cognitive capabilities focused on understanding and interpreting individual and collective object attributes and interrelations within an image. This process goes beyond mere recognition, delving into the analytical assessment of: 1) single-instance attribute reasoning; 2) cross-instance attribute comparison; and 3) cross-instance relation reasoning. It is a critical component for systems requiring a deep semantic understanding of visual content, enabling nuanced interaction with and response to complex visual content.\nLogical Reasoning (LR). This core dimension encompasses a sophisticated framework of cognitive processes designed to interpret, deduce, and infer conclusions from visual content through a structured approach to logic and reasoning. This multi-faceted capability marries the intuitive understanding of visual content with the structured rigor of logical deduction, enabling: 1) diagram reasoning; 2) code \\& sequence reasoning; and 3) common reasoning.\nScience \\& Technology (ST). It consists of a comprehensive framework for the application and integration of knowledge across a broad spectrum of science and technology. This domain combines the theoretical underpinnings and practical applications of various fields: 1) natural science; 2) engineering; and 3) geography \\& earth science.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "It involves a deep dive into the specifics of images: 1) attribute & celebrity recognition; 2) object location; and 3) object counting.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0dbee2ce-8fda-48e3-99b0-91e91f43f2bd",
        "questions": "Does the Instance Reasoning core dimension in the MMStar benchmark include cross-instance relation reasoning?",
        "answers": "Yes",
        "context": "Figure 5: Distribution of capability dimensions on the MMStar benchmark. In MMStar, we display 6 core capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the number of samples for each detailed dimension. Each core capability contains a meticulously balanced 250 samples. We further ensure a relatively even distribution across the 18 detailed axes.\n\nIn Figure 5, we provide statistics for each core capability and their detailed axes on the MMStar benchmark.\n\nCoarse Perception (CP). This core dimension refers to the capability to understand and interpret the overarching characteristics and themes of an image without delving into the finer details. It encompasses a broad, holistic view of the visual content, enabling the identification of: 1) image style \\& quality; 2) image scene \\& topic; and 3) image emotion.\n\nFine-grained Perception (FP). This core dimension represents a sophisticated level of image understanding that focuses on the detailed and nuanced aspects of visual content. It involves a deep dive into the specifics of images: 1) attribute \\& celebrity recognition; 2) object location; and 3) object counting. This core dimension unveils the subtle intricacies that coarse perception might overlook.\nInstance Reasoning (IR). It encapsulates a set of advanced cognitive capabilities focused on understanding and interpreting individual and collective object attributes and interrelations within an image. This process goes beyond mere recognition, delving into the analytical assessment of: 1) single-instance attribute reasoning; 2) cross-instance attribute comparison; and 3) cross-instance relation reasoning. It is a critical component for systems requiring a deep semantic understanding of visual content, enabling nuanced interaction with and response to complex visual content.\nLogical Reasoning (LR). This core dimension encompasses a sophisticated framework of cognitive processes designed to interpret, deduce, and infer conclusions from visual content through a structured approach to logic and reasoning. This multi-faceted capability marries the intuitive understanding of visual content with the structured rigor of logical deduction, enabling: 1) diagram reasoning; 2) code \\& sequence reasoning; and 3) common reasoning.\nScience \\& Technology (ST). It consists of a comprehensive framework for the application and integration of knowledge across a broad spectrum of science and technology. This domain combines the theoretical underpinnings and practical applications of various fields: 1) natural science; 2) engineering; and 3) geography \\& earth science.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "It encapsulates a set of advanced cognitive capabilities focused on understanding and interpreting individual and collective object attributes and interrelations within an image. This process goes beyond mere recognition, delving into the analytical assessment of: 1) single-instance attribute reasoning; 2) cross-instance attribute comparison; and 3) cross-instance relation reasoning.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0dcf035b-7552-46e8-a953-a1155a62ddc4",
        "questions": "What is the title of the paper authored by J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, and F. Huang in 2023?",
        "answers": "Qwen technical report",
        "context": "References\n\n[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[2] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n[3] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n[5] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\n[6] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.\n[7] S. Cheng, Z. Guo, J. Wu, K. Fang, P. Li, H. Liu, and Y. Liu. Can vision-language models think from a first-person perspective? arXiv preprint arXiv:2311.15596, 2023.\n[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[10] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https:// github. com/open-compass/opencompass, 2023.\n[11] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n[12] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in visionlanguage large model. arXiv preprint arXiv:2401.16420, 2024.\n[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.\n[14] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\n[15] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. Sphinxx : Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.\n[16] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.\n[17] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.\n[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. 1. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[19] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n[20] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0de63c6d-92b0-4864-a8d4-c57ed379f377",
        "questions": "Which paper discusses the concept of 'Mixtral of experts' and who are the authors?",
        "answers": "A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al.",
        "context": "References\n\n[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[2] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n[3] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n[5] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\n[6] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.\n[7] S. Cheng, Z. Guo, J. Wu, K. Fang, P. Li, H. Liu, and Y. Liu. Can vision-language models think from a first-person perspective? arXiv preprint arXiv:2311.15596, 2023.\n[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[10] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https:// github. com/open-compass/opencompass, 2023.\n[11] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n[12] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in visionlanguage large model. arXiv preprint arXiv:2401.16420, 2024.\n[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.\n[14] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\n[15] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. Sphinxx : Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.\n[16] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.\n[17] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.\n[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. 1. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[19] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n[20] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "[19] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0de90230-6ba7-4a7e-a282-90c0b7ddeba8",
        "questions": "Is the paper titled 'Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks' published in 2023?",
        "answers": "Yes",
        "context": "References\n\n[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[2] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.\n[3] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n[5] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\n[6] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.\n[7] S. Cheng, Z. Guo, J. Wu, K. Fang, P. Li, H. Liu, and Y. Liu. Can vision-language models think from a first-person perspective? arXiv preprint arXiv:2311.15596, 2023.\n[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[10] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https:// github. com/open-compass/opencompass, 2023.\n[11] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n[12] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, X. Wei, S. Zhang, H. Duan, M. Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in visionlanguage large model. arXiv preprint arXiv:2401.16420, 2024.\n[13] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.\n[14] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.\n[15] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. Sphinxx : Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.\n[16] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017.\n[17] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904-4916. PMLR, 2021.\n[18] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. 1. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[19] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. 1. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n[20] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. ArXiv, abs/1603.07396, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "[6] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0df099c5-62a7-4dc7-b273-38fcdfa69923",
        "questions": "What is the average performance gain of the LVLMs Sphinx-X-8x7B and Monkey-Chat compared to their original LLMs?",
        "answers": "14.1% and 14.2%",
        "context": "Figure 3: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLMLVLM ${ }^{\\text {text }}$ pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLMLVLM $^{\\text {text }}$ pairs, underscoring the issue of data leakage during the multi-modal training process.\n\nIn Figure 1 (d) and Figure 3, we showcase some examples where original LLMs fail, but LVLMs without accessing images succeed. Despite these questions requiring image content for accurate answers, the LVLMs without accessing images are capable of correctly answering these questions which stump original LLMs. To further support our hypotheses of data leakage during LVLMs\u2019 multi-modal training, we conduct an intriguing experiment. We remove the image inputs for LVLMs and only utilize textual questions and options for evaluation, with the results reported in Table 3. We compare the performance gains of LVLMs set to receive only text inputs (LVLM-text) against their corresponding LLM bases (LLM) to quantitatively assess the degree of data leakage in LVLMs' multi-modal training. As shown in Table 3, most LVLMs exhibit varying degrees of data leakage during multi-modal training. For example, the LLMs of Sphinx-X-8x7B [15] and Monkey-Chat [23], show a respective average performance gain of $14.1 \\%$ and $14.2 \\%$ compared to their original LLMs.\nDrawing from our observations, we posit that the issue of data leakage in multi-modal datasets is a significant concern that warrants attention. For research to be transparent and equitable, it is imperative to account for and mitigate such leakage. This will ensure that the performance of models is evaluated on their true ability to integrate and interpret multi-modal data, rather than their capacity to memorize specific samples. A proper benchmark would be a crucial step toward advancing the field of multi-modal language model research.\n\n\n4 MMStar\n\n\nWith the aforementioned analysis, we present an elite vision-dependent multi-modal benchmark, dubbed as MMStar. In Section 4.1, we elaborate on the data curation process of MMStar. Section",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For example, the LLMs of Sphinx-X-8x7B [15] and Monkey-Chat [23], show a respective average performance gain of $14.1 \\%$ and $14.2 \\%$ compared to their original LLMs.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0df23548-2c12-429c-887b-0acfc535c13d",
        "questions": "Does the document suggest that data leakage is a concern in multi-modal datasets?",
        "answers": "Yes",
        "context": "Figure 3: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLMLVLM ${ }^{\\text {text }}$ pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLMLVLM $^{\\text {text }}$ pairs, underscoring the issue of data leakage during the multi-modal training process.\n\nIn Figure 1 (d) and Figure 3, we showcase some examples where original LLMs fail, but LVLMs without accessing images succeed. Despite these questions requiring image content for accurate answers, the LVLMs without accessing images are capable of correctly answering these questions which stump original LLMs. To further support our hypotheses of data leakage during LVLMs\u2019 multi-modal training, we conduct an intriguing experiment. We remove the image inputs for LVLMs and only utilize textual questions and options for evaluation, with the results reported in Table 3. We compare the performance gains of LVLMs set to receive only text inputs (LVLM-text) against their corresponding LLM bases (LLM) to quantitatively assess the degree of data leakage in LVLMs' multi-modal training. As shown in Table 3, most LVLMs exhibit varying degrees of data leakage during multi-modal training. For example, the LLMs of Sphinx-X-8x7B [15] and Monkey-Chat [23], show a respective average performance gain of $14.1 \\%$ and $14.2 \\%$ compared to their original LLMs.\nDrawing from our observations, we posit that the issue of data leakage in multi-modal datasets is a significant concern that warrants attention. For research to be transparent and equitable, it is imperative to account for and mitigate such leakage. This will ensure that the performance of models is evaluated on their true ability to integrate and interpret multi-modal data, rather than their capacity to memorize specific samples. A proper benchmark would be a crucial step toward advancing the field of multi-modal language model research.\n\n\n4 MMStar\n\n\nWith the aforementioned analysis, we present an elite vision-dependent multi-modal benchmark, dubbed as MMStar. In Section 4.1, we elaborate on the data curation process of MMStar. Section",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Drawing from our observations, we posit that the issue of data leakage in multi-modal datasets is a significant concern that warrants attention.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0dfb75cf-59d8-4ac1-b3d1-f783e311da73",
        "questions": "How many LLMLVLM text pairs are used to illustrate data leakage during LVLMs' multi-modal training processes?",
        "answers": "16 pairs",
        "context": "Figure 3: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLMLVLM ${ }^{\\text {text }}$ pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLMLVLM $^{\\text {text }}$ pairs, underscoring the issue of data leakage during the multi-modal training process.\n\nIn Figure 1 (d) and Figure 3, we showcase some examples where original LLMs fail, but LVLMs without accessing images succeed. Despite these questions requiring image content for accurate answers, the LVLMs without accessing images are capable of correctly answering these questions which stump original LLMs. To further support our hypotheses of data leakage during LVLMs\u2019 multi-modal training, we conduct an intriguing experiment. We remove the image inputs for LVLMs and only utilize textual questions and options for evaluation, with the results reported in Table 3. We compare the performance gains of LVLMs set to receive only text inputs (LVLM-text) against their corresponding LLM bases (LLM) to quantitatively assess the degree of data leakage in LVLMs' multi-modal training. As shown in Table 3, most LVLMs exhibit varying degrees of data leakage during multi-modal training. For example, the LLMs of Sphinx-X-8x7B [15] and Monkey-Chat [23], show a respective average performance gain of $14.1 \\%$ and $14.2 \\%$ compared to their original LLMs.\nDrawing from our observations, we posit that the issue of data leakage in multi-modal datasets is a significant concern that warrants attention. For research to be transparent and equitable, it is imperative to account for and mitigate such leakage. This will ensure that the performance of models is evaluated on their true ability to integrate and interpret multi-modal data, rather than their capacity to memorize specific samples. A proper benchmark would be a crucial step toward advancing the field of multi-modal language model research.\n\n\n4 MMStar\n\n\nWith the aforementioned analysis, we present an elite vision-dependent multi-modal benchmark, dubbed as MMStar. In Section 4.1, we elaborate on the data curation process of MMStar. Section",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Each LLMLVLM ${ }^{\text {text }}$ pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e011113-15f3-4777-a63c-32210c099d16",
        "questions": "What is the average score achieved by the closed-source LLM GeminiPro on the six popular multi-modal benchmarks under a 2-shot inference strategy?",
        "answers": "42.4",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & \\underline{42.4}",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e061345-1afd-44a1-ad50-9b48156377d9",
        "questions": "Which open-source LLM achieved the highest average score on the six popular multi-modal benchmarks under a 2-shot inference strategy?",
        "answers": "Qwen1.5-72B",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e1654b8-1e96-4b89-b7ac-aa5628826512",
        "questions": "What is the abnormal hit rate percentage for the ScienceQA benchmark when evaluated with closed-source and open-source LLMs?",
        "answers": "57.2%",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e19967b-8102-4e40-9b93-5e6128d1af89",
        "questions": "What is the average score of the GPT4V model when evaluated using the LVLM-text strategy across the six multi-modal benchmarks?",
        "answers": "41.2",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT4V[35] (GPT4-Turbo[34]) & - & LLM LVLM-text LVLM & 41.2 45.1/53.6 & 12.2 17.6 69.6 & 64.3 68.2 81.4 & 10.1 28.4/71.6 & 24.2 25.4 44.7 & 35.3 41.2 66.0",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e1bebee-5052-4fb0-b33a-463cd3e01f89",
        "questions": "Does the InternLM2-XC2 model achieve a higher score in the AI2D benchmark using the LVLM-text strategy compared to the LLaVA-1.5 model?",
        "answers": "Yes",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "InternLM2-XC2[12] (InternLM2-7B[42]) & 7B & LLM LVLM-text LVLM & 32.8 34.2 41.7 & 8.9 26.2 79.6 & 64.0 71.9 96.7 & 48.3 63.3 81.4 & 31.9 38.1 74.9 & 18.9 29.4 57.4 & 34.1 43.9 72.0",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e1d0ad4-2fe4-45cc-beb5-31cce23e4a50",
        "questions": "Which model has the highest score in the SEED benchmark using the LVLM-text strategy among the open-source LVLMs?",
        "answers": "Monkey-Chat",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Monkey-Chat[23] (Qwen-7B[1]) & 10B & LLM LVLM-text LVLM & 19.8 32.4 37.1 & 8.4 15.6 71.0 & 52.7 71.1 82.4 & 42.6 56.8 68.5 & 7.6 36.1 69.1 & 20.5 25.0 34.0 & 25.3 39.5 60.4",
        "evidence_page_no": 16,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e1ee981-43e3-4f49-b4bd-ba0d8e4a5806",
        "questions": "What is the title of the work by NousResearch that was published in 2023?",
        "answers": "Nous-hermes-2-yi-34b",
        "context": "[21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[22] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[23] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023.\n[24] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[25] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.\n[26] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n[27] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.\n[28] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.\n[29] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n[30] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n[31] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv:2305.15023, 2023.\n[32] Microsoft. Phi2: The surprising power of small language models. https://www.microsoft.com/ en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/, 2023.\n[33] NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023.\n[34] OpenAI. Chatgpt. https://chat.openai.com/, 2023.\n[35] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.\n[36] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n[38] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022.\n[39] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.\n[40] H. Taud and J.-F. Mas. Multilayer perceptron (mlp). Geomatic approaches for modeling land change scenarios, pages 451-455, 2018.\n[41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[42] I. Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023.",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e21a2af-357c-402d-b49e-f679c1611767",
        "questions": "Which conference proceedings include the work by A. Radford and colleagues on learning transferable visual models from natural language supervision?",
        "answers": "International conference on machine learning",
        "context": "[21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[22] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[23] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023.\n[24] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[25] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.\n[26] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n[27] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.\n[28] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.\n[29] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n[30] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n[31] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv:2305.15023, 2023.\n[32] Microsoft. Phi2: The surprising power of small language models. https://www.microsoft.com/ en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/, 2023.\n[33] NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023.\n[34] OpenAI. Chatgpt. https://chat.openai.com/, 2023.\n[35] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.\n[36] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n[38] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022.\n[39] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.\n[40] H. Taud and J.-F. Mas. Multilayer perceptron (mlp). Geomatic approaches for modeling land change scenarios, pages 451-455, 2018.\n[41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[42] I. Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e2a04bd-3c16-40af-b3be-516709463e6f",
        "questions": "Is the work titled 'Deepseek-vl: Towards real-world vision-language understanding' expected to be published in 2024?",
        "answers": "Yes",
        "context": "[21] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.\n[22] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[23] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023.\n[24] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.\n[25] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.\n[26] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n[27] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.\n[28] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.\n[29] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\n[30] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022.\n[31] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv:2305.15023, 2023.\n[32] Microsoft. Phi2: The surprising power of small language models. https://www.microsoft.com/ en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/, 2023.\n[33] NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023.\n[34] OpenAI. Chatgpt. https://chat.openai.com/, 2023.\n[35] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.\n[36] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n[37] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n[38] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022.\n[39] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.\n[40] H. Taud and J.-F. Mas. Multilayer perceptron (mlp). Geomatic approaches for modeling land change scenarios, pages 451-455, 2018.\n[41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[42] I. Team. Internlm: A multilingual language model with progressively enhanced capabilities, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024.",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e31b19d-3b3a-4486-97d6-0b863d43582a",
        "questions": "Which open-source LLM achieved the highest average score across the six multi-modal benchmarks in the evaluation using a 2-shot inference strategy?",
        "answers": "Qwen1.5-72B",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e4bbca8-986c-4bc7-ab91-5f36014340b1",
        "questions": "Comparing the closed-source LLMs, which one performed better on the SEED benchmark, GPT4-Turbo or GeminiPro?",
        "answers": "GeminiPro",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\ GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & 42.4",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e5bc44c-98de-4bf6-8273-4d1c23d83f22",
        "questions": "Based on the 2-shot evaluation results, how much higher was the average score of the LLM 'InternLM2-20B' compared to 'Phi2-2.7B'?",
        "answers": "14.1",
        "context": "Table 2: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2 -shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEED-Image [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 2-shot & 42.0 & 15.5 & 67.5 & 61.3 & 26.8 & 25.6 & 39.8 \\\\\n  GeminiPro[41] & 2-shot & 42.7 & 18.7 & 69.3 & 60.1 & 38.1 & 25.5 & $\\underline{42.4}$ \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 2-shot & 33.0 & 8.6 & 55.6 & 41.3 & 32.1 & 22.7 & 32.2 \\\\\n  Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\\\\n  Yi-6B[49] & 2 -shot & 32.9 & 16.0 & 64.6 & 51.5 & 36.7 & 24.5 & 37.7 \\\\\n  LLaMA2-7B[43] & 2-shot & 25.9 & 7.7 & 57.9 & 42.8 & 32.8 & 22.8 & 31.7 \\\\\n  Qwen-7B[1] & 2-shot & 30.6 & 15.0 & 63.0 & 50.0 & 32.6 & 21.0 & 35.4 \\\\\n  Deepseek-7B[3] & 2-shot & 28.7 & 11.6 & 61.9 & 46.0 & 34.1 & 21.7 & 34.0 \\\\\n  InternLM2-7B[42] & 2-shot & 33.6 & 11.4 & 63.6 & 52.1 & 34.4 & 20.4 & 35.9 \\\\\n  Qwen1.5-7B[1] & 2-shot & 33.3 & 13.1 & 65.1 & 52.1 & 32.1 & 22.8 & 36.4 \\\\\n  Vicuna-v1.5-7B[8] & 2-shot & 31.3 & 9.5 & 58.9 & 45.5 & 32.0 & 20.7 & 33.0 \\\\\n  Baichuan2-7B[47] & 2 -shot & 28.2 & 13.7 & 58.1 & 44.1 & 32.3 & 21.7 & 33.0 \\\\\n  Mistral-7B[18] & 2-shot & 29.8 & 17.2 & 66.1 & 50.0 & 34.4 & 13.4 & 35.2 \\\\\n  LLaMA2-13B[43] & 2-shot & 32.9 & 10.1 & 58.9 & 43.8 & 32.1 & 24.8 & 33.8 \\\\\n  Vicuna-v1.5-13B[8] & 2-shot & 31.3 & 12.8 & 63.0 & 46.8 & 33.6 & 20.8 & 34.7 \\\\\n  Baichuan2-13B[47] & 2-shot & 32.2 & 13.1 & 61.0 & 47.1 & 35.2 & 23.4 & 35.3 \\\\\n  InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8 \\\\\n  Yi-34B[49] & 2-shot & 35.8 & 15.8 & 67.9 & 59.6 & 37.2 & 26.9 & 40.5 \\\\\n  Mixtral-8x7B[19] & 2-shot & 35.1 & 17.3 & 66.3 & 55.1 & 35.8 & 22.7 & 38.7 \\\\\n  Deepseek-67B[3] & 2-shot & 38.3 & 17.2 & 68.3 & 59.7 & 37.3 & 23.4 & 40.7 \\\\\n  LLaMA2-70B[43] & 2-shot & 30.4 & 17.2 & 63.4 & 49.3 & 34.9 & 24.2 & 36.6 \\\\\n  Qwen1.5-72B[1] & 2-shot & 42.4 & 21.1 & 70.1 & 60.9 & 40.7 & 26.3 & 43.6 \\\\\n \n\\end{tabular}\nthe importance of visual understanding. (2) Answers are directly included in the textual questions. As shown in Figure 1(b), LLMs can derive the correct answer \"circle\" through simple reasoning based on the question \"What is the shape of the round dirt circle?\".\nTo quantitatively substantiate our findings, we further experiment to gauge the proportion of these two types of samples in existing benchmarks. Specifically, we evaluate several benchmarks with two closed-source LLMs (GPT4Turbo [34], and GeminiPro [41]) and six opensource heavy LLMs (InternLM2-20B [42], Yi34B [49], Mixtral-8x7B [19], Deepseek-67B [3], LLaMA2-70B [43], and Qwen1.5-72B [1]), recording the hit count for each question. Here, the 'hit' refers to the ability of an LLM to correctly answer the question without relying on vi-\n\n\nFigure 2: LLM hit rate across various benchmarks.\nsual input. We then calculate the percentage of samples with a hit count of six or more (representing $80 \\%$ ) against the total number of samples to determine the abnormal hit rate for each benchmark. As depicted in Figure 2, every benchmark shows a certain degree of samples that visual contents are unnecessary, with ScienceQA [30] and AI2D [20] exhibiting amazing abnormal hit rates of $57.2 \\%$ and $46.2 \\%$, respectively. Based on our observations, most multi-modal benchmarks have yet to fully assess the multi-modal capabilities of LVLMs.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Phi2-2.7B[32] & 2-shot & 19.9 & 4.3 & 50.8 & 41.7 & 6.9 & 18.4 & 23.7 \\ InternLM2-20B[42] & 2-shot & 35.6 & 17.4 & 66.4 & 55.9 & 30.4 & 20.8 & 37.8",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e6b21f3-213a-486e-b22c-feb1026d44c2",
        "questions": "How many Large Vision-Language Models (LVLMs) were included in the experiments, and what are the sizes of these LVLMs?",
        "answers": "Sixteen LVLMs; sizes range from 3B to 60B.",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e71af64-f01b-4b96-b9b7-7be65968b4b7",
        "questions": "Considering only the open-source Large Language Models (LLMs) of sizes ranging from 1.8B to 72B, which series are mentioned in the document?",
        "answers": "Qwen series, LLaMA2 series, Phi2, Vicuna series, Deepseek series, InternLM2 series, Baichuan2 series, Yi series, Mistral series.",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19].",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e75601d-3e14-472e-85c6-554712c8431d",
        "questions": "What additional step is required for evaluating GeminiPro-Vision and CogVLM-Chat under the 'LVLM-text' setting?",
        "answers": "Replacement of the original images with pure grey images to bypass image content input.",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "When evaluating LVLMs under the 'LVLM-text' setting (i.e., answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens. However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly.",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e7cd3cf-9067-4465-a98b-bb3ae69fc01b",
        "questions": "How many candidate samples were shortlisted during the data curation process after the preliminary coarse filtering but before the manual review?",
        "answers": "11,607",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607.",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e831ee8-51b1-47c5-a814-4468fa588902",
        "questions": "What criteria were used by the three experts during the manual review of the samples to ensure they were appropriate for the MMStar benchmark?",
        "answers": "1) Each sample's answer should be based on the understanding of visual content; 2) Selected samples should cover a comprehensive range of capability assessment dimensions; 3) Most samples should require LVLMs to possess advanced multi-modal abilities for resolution.",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution.",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e8472a7-dd33-4379-a88f-58c1237d7b90",
        "questions": "During the manual review process, how were the difficulty levels of the samples determined based on the hit counts of the 16 LVLMs on the coarsely filtered samples?",
        "answers": "The hit counts of all 16 LVLMs on the coarsely filtered samples were tallied, and the samples were split into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough (0-3).",
        "context": "Figure 4: Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark.\n4.2 provides a detailed analysis of the constructed MMStar benchmark. In Section 4.3, we introduce two benchmark-specific metrics developed to evaluate the degree of data leakage as well as the actual performance gains in multimodal capabilities from the multi-modal training.\n\n\n4.1 Data Curation Process\n\n\nCriteria for data curation. The evaluation samples for constructing the MMStar benchmark should meet three fundamental criteria: 1) Visual dependency. The collected samples can be correctly answered only based on understanding the visual content; 2) Minimal data leakage. The collected samples should minimize the risk of unintentional inclusion in LLMs' training corpus, or be effectively transformed from uni-modal to multi-modal formats to prevent LLMs from \"recalling\" the correct answers; 3) Requiring advanced multi-modal capabilities for resolution. In addition to ensuring fairness and reliability by adhering to the above criteria, we also aim for samples to cover various difficulty levels. We expect to comprehensively capture LVLMs' multi-modal capabilities with succinct high-quality samples.\nData filter. We first choose two benchmarks [27, 21] focused on natural images and four centered on scientific and technical knowledge [51, 30, 20, 29] for our sample collection. We then develop an automated pipeline to preliminarily filter out samples that do not meet the first two criteria. Specifically, we employ two closed-source LLMs [41, 34] and six open-source LLMs [1, 42, 49, 3, 19, 43] sizing 20B or larger to serve as inspectors. These open-source LLMs are applied with a 2-shot in-context inference strategy to minimize response refusals and ensure consistency in answer formatting. Following this, we evaluate the sample pool with these LLM inspectors, documenting the hit frequency for each evaluation sample. Finally, we only retain those samples with hit counts of two or fewer hits, indicating that around $75 \\%$ of LLM inspectors fail to provide the correct answer. As illustrated in Figure 4, following this initial coarse filtering, our sample pool was reduced from 22,401 to 11,607 .\nManual review. After the coarse filtering with LLM inspectors, we further employ three experts to conduct the manual review process to ensure: 1) each sample's answer should be based on the understanding of visual content; 2) selected samples should cover a comprehensive range of capability assessment dimensions; 3) most samples should require LVLMs to possess advanced multi-modal abilities for resolution. To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough ( 0 -3). Finally, after considering both the diversity of capability dimensions and difficulty levels, we manually curated $\\mathbf{1 , 5 0 0}$ high-quality samples from the coarsely filtered set. Figure 4 showcases the detailed composition of data sources for our final selection of samples.\n\n4.2 Core Capabilities\n\nWe select and consolidate the dimensions used for assessing LVLMs' multi-modal capabilities in existing benchmarks and identify six core capability dimensions along with eighteen detailed axes.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "To expedite the manual selection of samples with varying difficulty levels for LVLMs, we tally the hit counts of all 16 LVLMs on the coarsely filtered samples and split them into four difficulty categories: easy (12-16), moderate (8-11), hard (4-7), and tough (0-3).",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0e935f4e-83ed-484f-bb35-2b98cfe87700",
        "questions": "What is the average score achieved by InternLM-XC2 for open-source LVLMs in core capabilities?",
        "answers": "55.4",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ea39958-fe3b-44f9-89c9-5e12141c7bda",
        "questions": "How does LLaVA-Next perform in Mathematics (MA) compared to GPT4V and GeminiPro-Vision?",
        "answers": "LLaVA-Next even surpasses GPT4V and GeminiPro-Vision in the mathematics (MA) core capability.",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ea7150e-5115-49b1-ac56-0538c497b5a1",
        "questions": "Which open-source LVLM has the worst performance in Logical Reasoning (LR) among those with 7B parameters?",
        "answers": "LLaVA-1.5",
        "context": "Table 5: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LLMs and 14 open-source LLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science \\& technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n  Model & | LLM & Param. & CP & FP & IR & LR & ST & MA & | Avg. & $\\mathrm{MG} \\uparrow$ & ML $\\downarrow$ \\\\\n  \\multicolumn{12}{|c|}{Baselines} \\\\\n  Random Choice & & - & 23.7 & 24.5 & 25.3 & 24.3 & 24.8 & 25.1 & 24.6 & - & - \\\\\n  \\multicolumn{12}{|c|}{Closed-source LVLMs} \\\\\n  GeminiPro-Vision[41] & GeminiPro[41] & & 51.6 & 28.8 & 50.8 & 46.0 & 28.4 & 50.0 & 42.6 & 27.4 & 0.0 \\\\\n  GPT4V (low)[35] & GPT4-Turbo[34] & & 62.0 & 32.8 & 55.2 & 48.0 & 33.6 & 44.8 & 46.1 & 32.6 & 1.3 \\\\\n  GPT4V (high)[35] & GPT4-Turbo[34] & - & 76.6 & 51.4 & 66.6 & 55.8 & 42.6 & 49.8 & 57.1 & 43.6 & 1.3 \\\\\n  \\multicolumn{12}{|c|}{Open-source LVLMs} \\\\\n  TinyLLaVA[53] & Phi2-2.7B[32] & 3B & 60.4 & 31.6 & 50.8 & 30.4 & 18.0 & 24.8 & $36.0 \\mid$ & 16.4 & 7.6 \\\\\n  Yi-VL[49] & Yi-6B[49] & 6B & 58.0 & 33.6 & 46.4 & 34.8 & 20.4 & 34.0 & 37.9 & 15.6 & 0.0 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0 \\\\\n  ShareGPT4V[5] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 28.0 & 45.6 & 24.4 & 17.2 & 24.0 & 33.0 & 11.9 & $\\underline{0.0}$ \\\\\n  InternLM-XC2[12] & InternLM2-7B[42] & 7B & 70.8 & 48.8 & 65.2 & 56.4 & 42.0 & 49.2 & 55.4 & 28.1 & 7.5 \\\\\n  Qwen-VL-Chat[2] & Qwen-7B[1] & 8 B & $\\overline{59.6}$ & 32.0 & 50.8 & 29.2 & 22.0 & 31.6 & 37.5 & 23.9 & 0.0 \\\\\n  Deepseek-VL[28] & Deepseek-7B[3] & 8B & 64.0 & 30.8 & 49.2 & 36.4 & 21.6 & 20.4 & 37.1 & 15.7 & $\\underline{0.0}$ \\\\\n  Monkey-Chat[23] & Qwen-7B[1] & 10B & 57.6 & 36.4 & 51.6 & 33.2 & 26.4 & 24.4 & 38.3 & 13.5 & 17.6 \\\\\n  LLaVA-1.5[24] & Vicuna-v1.5-13B[8] & $13 B$ & 58.8 & 28.0 & 41.6 & 24.4 & 18.4 & 25.6 & 32.8 & 13.9 & 0.0 \\\\\n  CogVLM-Chat[45] & Vicuna-v1.5-7B[8] & $17 B$ & 66.8 & 36.8 & 49.2 & 31.2 & 23.6 & 11.6 & 36.5 & 14.9 & 0.0 \\\\\n  Yi-VL[49] & Yi-34B[49] & $34 B$ & 53.2 & 31.2 & 52.0 & 32.4 & 12.4 & 35.2 & 36.1 & 18.8 & $\\underline{0.0}$ \\\\\n  LLaVA-Next[25] & NH2-Yi-34B[33] & $34 B$ & 66.4 & 52.0 & 62.4 & 46.0 & 32.4 & 53.6 & 52.1 & 29.4 & 2.4 \\\\\n  InternVL-Chat-V1.2[6] & NH2-Yi-34B[33] & 40B & 67.6 & 43.2 & 61.2 & 47.2 & 24.0 & 19.2 & 43.7 & 32.6 & 0.0 \\\\\n  Sphinx-X-MOE[15] & Mixtral-8x7B[19] & 57B & 58.4 & 40.8 & 47.6 & 35.2 & 19.2 & 32.0 & 38.9 & 14.8 & 1.0 \\\\\n \n\\end{tabular}\nsetting can achieve the best average score of $57.1 \\%$ among all LVLMs. Increasing the resolution and number of image tokens can boost the average score from $46.1 \\%$ to $57.1 \\%$ for GPT4V, offering a positive signal to the research community. Among the open-source LVLMs, InternLMXcomposer2 [12] achieves an impressive score of 55.4\\%. LLaVA-Next [25] even surpasses GPT4V and GeminiPro-Vision [41] in the mathematics (MA) core capability. Notably, no LVLMs managed to reach a passing average score (i.e. $60 \\%$ ) in the core capabilities of fine-grained perception (FP), logical reasoning (LR), science \\& Technology (ST), and mathematics (MA), highlighting these dimensions as particularly challenging for existing LVLMs. Moreover, TinyLLaVA [53], despite its modest 3B scale, outperformed some competitors of 7B and even 13B surprisingly, underscoring the potential of smaller-scale LVLMs. Additionally, even with the same architecture, ShareGPT4V7B [5] even outperforms LLaVA-1.5-13B with high-quality caption data. This result highlights the significance of high-quality caption data for LVLM performance to the community.\n\n\n5.3 Results Analysis of MG/ML\n\n\nIn this section, we present the results of our proposed multi-modal gain (MG) and multi-modal leakage (ML) metrics of 16 LVLMs with varying sizes and architectures on 6 popular benchmarks and our MMStar benchmark. We then detail our observations and analyses from both the model and benchmark perspectives.\n\nAnalysis from the model perspective. In Table 6, we illustrate the MG/ML (Multi-modal Gain/Multi-modal Leakage) metrics for each LVLM across each benchmark and provide an average MG/ML metric across all benchmarks in the final column. For closed-source LVLMs, GPT4V demonstrates notable performance gains attributed to its multi-modal training, while GeminiProVision shows lesser data leakage during multi-modal training. This suggests that GPT4V may have utilized a broader range of multi-modal training data compared to GeminiPro-Vision. Among the open-source LVLMs, InternLM-XComposer2 achieves the highest average multi-modal gain of 28.1 across all benchmarks, whereas LLaVA-1.5-7B records the lowest at 14.8. This outcome is reason-",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "LLaVA-1.5[24] & Vicuna-v1.5-7B[8] & 7B & 58.8 & 24.0 & 38.8 & 24.0 & 13.6 & 22.8 & 30.3 & 10.7 & 0.0",
        "evidence_page_no": 11,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ea8ea20-8b8e-4c1f-b540-49d682ba7396",
        "questions": "What is the average score of GPT4V when evaluated using the LVLM strategy across the six benchmarks?",
        "answers": "66.0",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0eb23a8c-6de8-422d-b610-7a8789cd4710",
        "questions": "How does the performance of TinyLLaVA using the LVLM strategy compare with Monkey-Chat on the SEED benchmark?",
        "answers": "TinyLLaVA scored 70.1, whereas Monkey-Chat scored 69.1.",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 & & \\hline & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0eb8bdbd-227b-402a-905c-4c31a4dd5b19",
        "questions": "Which open-source LVLM, evaluated using the LVLM-text strategy, displayed the highest score on the MMB benchmark, and what was the score?",
        "answers": "InternLM2-XC2 with a score of 26.2.",
        "context": "Table 3: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & $-$ & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[b]{3}{*}{\\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular}} & - & LLM & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  & - & LVLM-text & 45.1 & 17.6 & 68.2 & 62.5 & 28.4 & 25.4 & 41.2 \\\\\n  & - & LVLM & 53.6 & 69.6 & 81.4 & 75.3 & 71.6 & 44.7 & 66.0 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular}} & - & LLM & 42.9 & 18.4 & 68.9 & 59.2 & 35.5 & 23.3 & 41.4 \\\\\n  & - & LVLM-text & 39.4 & 16.7 & 66.3 & 54.5 & 27.9 & 24.5 & 38.2 \\\\\n  & - & LVLM & 44.4 & 68.1 & 80.6 & 68.0 & 64.3 & 36.0 & 60.2 \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular}} & \\multirow{3}{*}{3B} & LLM & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  & & LVLM-text & 30.0 & 21.0 & 62.3 & 51.9 & 37.2 & 23.5 & 37.7 \\\\\n  & & LVLM & 36.0 & 66.9 & 69.1 & 62.4 & 70.1 & 28.9 & 55.6 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 29.9 & 19.5 & 64.1 & 48.7 & 37.5 & 20.3 & 36.7 \\\\\n  & & LVLM & 34.4 & 65.0 & 68.7 & 55.6 & 65.6 & 23.6 & 52.2 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular}} & \\multirow{3}{*}{7B} & LLM & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  & & LVLM-text & 34.2 & $\\frac{26.2}{79.6}$ & 71.9 & 63.3 & 38.1 & $\\frac{29.4}{57.4}$ & 43.9 \\\\\n  & & LVLM & 41.7 & 79.6 & 96.7 & 81.4 & 74.9 & 57.4 & 72.0 \\\\\n  \\multirow[t]{3}{*}{Monkey-Chat[23] (Qwen-7B[1])} & \\multirow{3}{*}{10B} & LLM & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  & & LVLM-text & 32.4 & 15.6 & 71.1 & 56.8 & 36.1 & 25.0 & 39.5 \\\\\n  & & LVLM & 37.1 & 71.0 & 82.4 & 68.5 & 69.1 & 34.0 & 60.4 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular}} & \\multirow{3}{*}{17B} & LLM & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  & & LVLM-text & 30.1 & 15.5 & 54.6 & 52.5 & 36.7 & 25.0 & 35.7 \\\\\n  & & LVLM & 34.2 & 63.4 & 66.3 & 63.3 & 68.7 & 34.7 & 55.1 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$} & \\multirow{3}{*}{34B} & LLM & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  & & LVLM-text & 37.3 & 23.2 & 68.6 & 59.9 & 41.0 & 22.7 & 42.1 \\\\\n  & & LVLM & 43.2 & 71.5 & 75.3 & 65.9 & 68.1 & 25.6 & 58.3 \\\\\n  \\multirow[t]{3}{*}{$$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$} & \\multirow{3}{*}{40B} & LLM & 37.6 & 20.1 & 69.4 & 60.2 & 35.0 & 17.9 & 40.0 \\\\\n  & & LVLM-text & 41.7 & 23.9 & 70.3 & 65.0 & 40.5 & 24.0 & 44.2 \\\\\n  & & LVLM & 49.1 & 82.4 & 82.5 & 78.5 & 75.4 & 47.7 & 69.3 \\\\\n  \\multirow[t]{3}{*}{\\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular}} & \\multirow{3}{*}{57B} & LLM & 25.7 & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  & & LVLM-text & 43.6 & 20.5 & 68.4 & 61.1 & 39.9 & 28.4 & 43.7 \\\\\n  & & LVLM & 44.8 & 69.2 & 72.2 & 65.0 & 71.1 & 38.1 & 60.1 \\\\\n \n\\end{tabular}\n\nSecond issue: unintentional data leaking exists in LLM and LVLM training. Although the community has the trend towards developing new multi-modal benchmarks to assess LVLMs' capabilities from various dimensions, there is scant consideration for fairness and reliability during evaluation. Training LLMs and LVLMs requires vast and diverse data, inevitably leading to the leakage of evaluation samples. Such incidents are usually unintended, as it's impractical to predict which data will be used in future evaluation benchmarks during the preparation for training corpus.\n\nFigure 1 (c) showcases an evaluation sample leaked by LLMs. Though the question requires an understanding of image content, 16 out of 22 tested LLMs astonishingly provide the correct response by \"recalling\" their training data. To quantitatively support our observations, we evaluate 22 leading LLMs across 6 popular benchmarks and report the 0 -shot results in Table 1 and 2 -shot results in Table 2. Specifically, we find the 2 -shot evaluation strategy is more stable than the 0 -shot to reduce refusal for answering and align answer formats. Under the impact of vision-independent samples and data leakage from LLMs, GeminiPro [41] and Qwen1.5-72B [1] achieve a remarkable average accuracy of $41.4 \\%$ and $43.6 \\%$ under the 2 -shot setting, outperforming random choice by $20.4 \\%$ and $22.6 \\%$, respectively. Furthermore, Qwen1.5-72B achieves a score of $42.4 \\%$ on MMMU [51], even surpassing the performance of the majority of LVLMs with accessing images. This result serves as a reminder: if we only consider the final accuracy on benchmarks when evaluating LVLMs, potential data leakage from LLMs could lead to unfair comparisons.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& & LVLM-text & 34.2 & 26.2/79.6 & 71.9 & 63.3 & 38.1 & 29.4/57.4 & 43.9",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ebb9133-0376-4795-873c-238bd9d013b8",
        "questions": "What is the score of GeminiPro on the SEED benchmark?",
        "answers": "35.5",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & \\overline{59.2} & 35.5 & \\overline{23.3} & 41.4",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ebbc767-692e-4c14-9d36-70f55a1e6984",
        "questions": "Which open-source LLM has the highest average score in the benchmark evaluations?",
        "answers": "Yi-34B",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ebbe39e-7904-44d6-bd92-71964c43bc46",
        "questions": "Calculate the difference in performance between the Mistral-7B and InternLM2-20B models on the AI2D benchmark.",
        "answers": "7.2",
        "context": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks. We employ a 0 -shot inference strategy for evaluating all LLMs. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [51]), MMB (MMBench-EN-Dev [27]), ScienceQA (ScienceQA-Test [30]), AI2D (AI2D-Test [20]), SEED (SEEDImage [21]), and MathVista (MathVista-Mini [29]). The best results are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  Model & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{9}{|c|}{Baselines} \\\\\n  Random Choice & $-$ & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{9}{|c|}{Closed-source LLMs} \\\\\n  GPT4-Turbo[34] & 0 -shot & 41.2 & 12.2 & 64.3 & 59.7 & 10.1 & 24.2 & 35.3 \\\\\n  GeminiPro[41] & 0 -shot & 42.9 & 18.4 & 68.9 & $\\overline{59.2}$ & 35.5 & $\\overline{23.3}$ & 41.4 \\\\\n  \\multicolumn{9}{|c|}{Open-source LLMs} \\\\\n  Qwen1.5-1.8B[1] & 0 -shot & 29.0 & 10.0 & 54.3 & 37.9 & 28.9 & 20.4 & 30.1 \\\\\n  Phi2-2.7B[32] & 0 -shot & 20.0 & 7.2 & 47.1 & 38.7 & 26.4 & 22.0 & 26.9 \\\\\n  Yi-6B[49] & 0 -shot & 25.7 & 9.5 & 58.1 & 39.1 & 27.4 & 21.2 & 30.2 \\\\\n  LLaMA2-7B[43] & 0 -shot & 23.6 & 11.5 & 56.8 & 43.5 & 31.7 & 24.1 & 31.9 \\\\\n  Qwen-7B[1] & 0 -shot & 19.8 & 8.4 & 52.7 & 42.6 & 7.6 & 20.5 & 25.3 \\\\\n  Deepseek-7B[3] & 0 -shot & 21.6 & 8.4 & 56.3 & 38.1 & 13.4 & 20.6 & 26.4 \\\\\n  InternLM2-7B[42] & 0 -shot & 32.8 & 8.9 & 64.0 & 48.3 & 31.9 & 18.9 & 34.1 \\\\\n  Qwen1.5-7B[1] & 0 -shot & 25.0 & 11.4 & 62.3 & 49.4 & 19.4 & 19.9 & 31.2 \\\\\n  Vicuna-v1.5-7B[8] & 0 -shot & 29.9 & 10.3 & 58.9 & 42.5 & 32.6 & 22.0 & 32.7 \\\\\n  Baichuan2-7B[47] & 0 -shot & 25.7 & 10.5 & 52.7 & 44.0 & 29.2 & 20.8 & 30.5 \\\\\n  Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3 \\\\\n  LLaMA2-13B[43] & 0 -shot & 24.4 & 10.1 & 59.1 & 45.0 & 33.6 & 23.8 & 32.7 \\\\\n  Vicuna-v1.5-13B[8] & 0 -shot & 28.3 & 11.6 & 59.5 & 45.0 & 26.3 & 19.6 & 31.7 \\\\\n  Baichuan2-13B[47] & 0 -shot & 22.1 & 4.7 & 51.1 & 32.8 & 25.4 & 20.3 & 26.1 \\\\\n  InternLM2-20B[42] & 0 -shot & 32.2 & 15.9 & 63.8 & 55.7 & 26.0 & 21.3 & 35.8 \\\\\n  Yi-34B[49] & 0 -shot & 37.1 & 10.5 & 53.6 & 57.3 & 37.3 & 21.7 & 36.3 \\\\\n  Mixtral-8x7B[19] & 0 -shot & $\\frac{35.7}{25}$ & 8.6 & 57.2 & 48.7 & 13.5 & 23.4 & 29.5 \\\\\n  Deepseek-67B[3] & 0 -shot & 30.9 & 14.8 & 64.3 & 57.5 & 17.1 & 23.2 & 34.6 \\\\\n  LLaMA2-70B[43] & 0 -shot & 28.9 & 12.3 & 62.2 & 48.6 & 34.3 & 25.2 & 35.3 \\\\\n  Qwen1.5-72B[1] & 0 -shot & 21.4 & 10.1 & 57.5 & 44.2 & 8.8 & 19.5 & 26.9 \\\\\n \n\\end{tabular}\nantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multimodal capabilities brought by multi-modal training.\n\n\n3 Two Overlooked Issues for Evaluating LVLMs\n\n\nIn this section, we delve into two commonly overlooked issues in current LVLM evaluation works. Moreover, we present detailed experimental results to further substantiate our observations.\n\nFirst issue: visual content is unnecessary for many evaluation samples. The key distinction between evaluating LLMs and LVLMs lies in the necessity for LVLM evaluations to strictly ensure that the correct answers can only be derived based on a thorough understanding of visual content. Without this, evaluating LVLMs' multi-modal capabilities degrades to merely assessing their LLM backbones' uni-modal abilities. However, upon examining samples from some popular LVLM benchmarks, we find many samples lack vital visual dependency and can yield correct answers even without the image inputs! Through analysis of these failure samples, we categorize them into two groups: (1) Answers can be directly obtained from the world knowledge embedded in LLMs, owing to the LLMs' extensive pertaining on the large corpus of data. For example, as illustrated in Figure 1(a), the question \"What is the capital of Nebraska?\" already provides the key information \"Nebraska\", eliminating the need for extracting relevant location information from visual content. A more appropriate question is \"What is the capital of the highlighted area in the image?\" to emphasize",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Mistral-7B[18] & 0 -shot & 30.0 & 13.2 & 63.4 & 48.5 & 34.3 & 22.6 & 35.3",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ec1e80e-5586-4fd8-bd41-ebd5df1c57ef",
        "questions": "What is the AI2D score for the LVLM-text strategy of the InternVL-Chat-v1.2 model with 40B parameters?",
        "answers": "65.0",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& 60.2 \\ & \\mathbf{6 5 . 0} \\ & 78.5",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ec37460-527f-4bc2-9577-7894468bc959",
        "questions": "Which model achieved the highest score in the SEED benchmark using the LVLM-text strategy among open-source LVLMs?",
        "answers": "Monkey-Chat[23] (Qwen-7B[1])",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& 7.6 \\ & 36.1 \\ & 69.1",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ec7c92e-b36d-4c16-a3cc-0cc1274e9b70",
        "questions": "Compare the performance of GPT4V[35] (GPT4-Turbo[34]) under the LVLM-text strategy in ScienceQA and MathVista benchmarks. Which benchmark had the higher score and what was the score difference?",
        "answers": "ScienceQA had a higher score with 68.2, and the score difference was 42.8.",
        "context": "D Detailed Evaluation Results of LVLMs on Six Multi-modal Benchmarks\n\n\nTable 7: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0 -shot inference strategy for LLMs to align the evaluation protocols of LVLMs. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  Model & Param. & Strategy & MMMU & MMB & ScienceQA & AI2D & SEED & MathVista & Avg. \\\\\n  \\multicolumn{10}{|c|}{Baseline} \\\\\n  Random Choice & - & - & 22.1 & 0.0 & 24.2 & 23.8 & 24.3 & 17.9 & 18.7 \\\\\n  \\multicolumn{10}{|c|}{Closed-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nGPT4V[35] \\\\\n(GPT4-Turbo[34])\n\\end{tabular} & - & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 41.2 \\\\\n& \\frac{45.1}{53.6}\n\\end{aligned}$$ & $$\\begin{aligned}\n& 12.2 \\\\\n& \\mathbf{1 7 . 6} \\\\\n&   69.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 64.3 \\\\\n& \\mathbf{6 8 . 2} \\\\\n&   81.4\n\\end{aligned}$$ &  & $$\\begin{aligned}\n& 10.1 \\\\\n& \\frac{28.4}{71.6}\n\\end{aligned}$$ & $$\\begin{array}{r}\n24.2 \\\\\n\\mathbf{2 5 . 4} \\\\\n  44.7\n\\end{array}$$ & $$\\begin{aligned}\n& 35.3 \\\\\n& 41.2 \\\\\n&   66.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nGeminiPro-Vision[41] \\\\\n(GeminiPro[41])\n\\end{tabular} & $-$ & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 42.9 \\\\\n& 39.4 \\\\\n& 44.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 18.4 \\\\\n& 16.7 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 68.9 \\\\\n& 66.3 \\\\\n& 80.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.2 \\\\\n& 54.5 \\\\\n& 68.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.5 \\\\\n& 27.9 \\\\\n& 64.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.3 \\\\\n& 24.5 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 41.4 \\\\\n& 38.2 \\\\\n& 60.2\n\\end{aligned}$$ \\\\\n  \\multicolumn{10}{|c|}{Open-source LVLMs and corresponding LLM bases} \\\\\n  \\begin{tabular}{l}\nTinyLLaVA[53] \\\\\n(Phi2-2.7B[32])\n\\end{tabular} & 3B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 20.0 \\\\\n& 30.0 \\\\\n& 36.0\n\\end{aligned}$$ & $$\\begin{gathered}\n  7.2 \\\\\n21.0 \\\\\n66.9\n\\end{gathered}$$ & $$\\begin{aligned}\n&   47.1 \\\\\n& 62.3 \\\\\n& 69.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.7 \\\\\n& 51.9 \\\\\n& 62.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 37.2 \\\\\n& 70.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 23.5 \\\\\n& 28.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.9 \\\\\n& 37.7 \\\\\n& 55.6\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-6B[49]) }\n\\end{aligned}$$ & 6B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& 33.1 \\\\\n& 38.4\n\\end{aligned}$$ & $$\\begin{gathered}\n9.5 \\\\\n23.6 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 58.1 \\\\\n& 67.5 \\\\\n& 72.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 39.1 \\\\\n& 55.7 \\\\\n& 59.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 27.4 \\\\\n& 38.3 \\\\\n& 67.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.2 \\\\\n& 24.2 \\\\\n& 28.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 30.2 \\\\\n& 40.4 \\\\\n& 55.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 29.9 \\\\\n& 34.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 19.5 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 64.1 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 48.7 \\\\\n& 55.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.5 \\\\\n& 65.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 20.3 \\\\\n& 23.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 36.7 \\\\\n& 52.2\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nShareGPT4V[5] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 31.7 \\\\\n& 35.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 20.4 \\\\\n& 69.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 65.2 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 49.4 \\\\\n& 57.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 37.7 \\\\\n& 69.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 22.7 \\\\\n& 25.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 37.9 \\\\\n& 54.5\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nInternLM2-XC2[12] \\\\\n(InternLM2-7B[42])\n\\end{tabular} & 7B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 32.8 \\\\\n& 34.2 \\\\\n& 41.7\n\\end{aligned}$$ & $$\\begin{gathered}\n8.9 \\\\\n\\mathbf{2 6 . 2} \\\\\n  79.6\n\\end{gathered}$$ & $$\\begin{aligned}\n& 64.0 \\\\\n& \\mathbf{7 1 . 9} \\\\\n&   96.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.3 \\\\\n& 63.3 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.9 \\\\\n& 38.1 \\\\\n& 74.9\n\\end{aligned}$$ & $$\\begin{array}{r}\n18.9 \\\\\n\\mathbf{2 9 . 4} \\\\\n  57.4\n\\end{array}$$ & $$\\begin{aligned}\n& 34.1 \\\\\n& 43.9 \\\\\n& 72.0\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nQwen-VL-Chat[2] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 24.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n8.7 \\\\\n58.3\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 56.7 \\\\\n& 67.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 49.0 \\\\\n& 61.3\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n19.5 \\\\\n64.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 20.8 \\\\\n& 32.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 29.8 \\\\\n& 52.9\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nDeepseek-VL[28] \\\\\n(Deepseek-7B[3])\n\\end{tabular} & 8B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 21.6 \\\\\n& 32.2 \\\\\n& 35.4\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n23.9 \\\\\n73.5\n\\end{gathered}$$ & $$\\begin{aligned}\n& 56.3 \\\\\n& 67.1 \\\\\n& 81.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 38.1 \\\\\n& 53.0 \\\\\n& 64.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.4 \\\\\n& 36.5 \\\\\n& 70.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.6 \\\\\n& 23.9 \\\\\n& 35.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.4 \\\\\n& 39.4 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nMonkey-Chat[23] \\\\\n(Qwen-7B[1])\n\\end{tabular} & 10B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 19.8 \\\\\n& 32.4 \\\\\n& 37.1\n\\end{aligned}$$ & $$\\begin{gathered}\n8.4 \\\\\n15.6 \\\\\n71.0\n\\end{gathered}$$ & $$\\begin{aligned}\n& 52.7 \\\\\n& 71.1 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.6 \\\\\n& 56.8 \\\\\n& 68.5\n\\end{aligned}$$ & $$\\begin{gathered}\n7.6 \\\\\n36.1 \\\\\n69.1\n\\end{gathered}$$ & $$\\begin{aligned}\n& 20.5 \\\\\n& 25.0 \\\\\n& 34.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 25.3 \\\\\n& 39.5 \\\\\n& 60.4\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nLLaVA-1.5[24] \\\\\n(Vicuna-v1.5-13B[8])\n\\end{tabular} & 13B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 28.3 \\\\\n& 26.0 \\\\\n& 35.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 11.6 \\\\\n& 21.4 \\\\\n& 68.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 59.5 \\\\\n& 66.5 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 45.0 \\\\\n& 52.2 \\\\\n& 60.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 26.3 \\\\\n& 37.0 \\\\\n& 68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 19.6 \\\\\n& 21.1 \\\\\n& 26.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 31.7 \\\\\n& 37.4 \\\\\n& 55.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nCogVLM-Chat[45] \\\\\n(Vicuna-v1.5-7B[8])\n\\end{tabular} & 17B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 29.9 \\\\\n& 30.1 \\\\\n& 34.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.3 \\\\\n& 15.5 \\\\\n& 63.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 58.9 \\\\\n& 54.6 \\\\\n& 66.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 42.5 \\\\\n& 52.5 \\\\\n& 63.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.6 \\\\\n& 36.7 \\\\\n& 68.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 22.0 \\\\\n& 25.0 \\\\\n& 34.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 32.7 \\\\\n& 35.7 \\\\\n& 55.1\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { Yi-VL[49] } \\\\\n& \\text { (Yi-34B[49]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.1 \\\\\n& 37.3 \\\\\n& 43.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 10.5 \\\\\n& 23.2 \\\\\n& 71.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 53.6 \\\\\n& 68.6 \\\\\n& 75.3\n\\end{aligned}$$ & $$\\begin{aligned}\n& 57.3 \\\\\n& 59.9 \\\\\n& 65.9\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.3 \\\\\n& 41.0 \\\\\n&   68.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 21.7 \\\\\n& 22.7 \\\\\n& 25.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 36.3 \\\\\n& 42.1 \\\\\n& 58.3\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { LLaVA-Next[25] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 34B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 40.4 \\\\\n& 47.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 24.9 \\\\\n& 79.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.9 \\\\\n& 82.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& 65.8 \\\\\n& 78.6\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 41.7 \\\\\n& 75.8\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 22.2 \\\\\n& 38.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 37.2 \\\\\n& 44.3 \\\\\n& 67.0\n\\end{aligned}$$ \\\\\n  $$\\begin{aligned}\n& \\text { InternVL-Chat-v1.2[6] } \\\\\n& \\text { (NH2-Yi-34B[33]) }\n\\end{aligned}$$ & 40B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 37.6 \\\\\n& 41.7 \\\\\n& 49.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 20.1 \\\\\n& 23.9 \\\\\n& 82.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 69.4 \\\\\n& 70.3 \\\\\n& 82.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 60.2 \\\\\n& \\mathbf{6 5 . 0} \\\\\n& 78.5\n\\end{aligned}$$ & $$\\begin{aligned}\n& 35.0 \\\\\n& 40.5 \\\\\n& 75.4\n\\end{aligned}$$ & $$\\begin{aligned}\n& 17.9 \\\\\n& 24.0 \\\\\n& 47.7\n\\end{aligned}$$ & $$\\begin{aligned}\n& 40.0 \\\\\n& 44.2 \\\\\n&   69.3\n\\end{aligned}$$ \\\\\n  \\begin{tabular}{l}\nSphinx-X-MoE[15] \\\\\n(Mixtral-8x7B[19])\n\\end{tabular} & 57B & \\begin{tabular}{l}\nLLM \\\\\nLVLM-text \\\\\nLVLM\n\\end{tabular} & $$\\begin{aligned}\n& 25.7 \\\\\n& \\mathbf{4 3 . 6} \\\\\n& 44.8\n\\end{aligned}$$ & $$\\begin{gathered}\n8.6 \\\\\n20.5 \\\\\n69.2\n\\end{gathered}$$ & $$\\begin{aligned}\n& 57.2 \\\\\n& 68.4 \\\\\n& 72.2\n\\end{aligned}$$ & $$\\begin{aligned}\n& 48.7 \\\\\n& 61.1 \\\\\n& 65.0\n\\end{aligned}$$ & $$\\begin{aligned}\n& 13.5 \\\\\n& 39.9 \\\\\n& 71.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 23.4 \\\\\n& 28.4 \\\\\n& 38.1\n\\end{aligned}$$ & $$\\begin{aligned}\n& 29.5 \\\\\n& 43.7 \\\\\n& 60.1\n\\end{aligned}$$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& 64.3 \\ & \\mathbf{6 8 . 2} \\ & \\hline 81.4 & & 35.3 \\ & 41.2 \\ & \\hline 66.0",
        "evidence_page_no": 16,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ed5be73-6d3a-48e8-bbec-7debf136f64e",
        "questions": "How is the multi-modal gain (MG) metric calculated for a given LVLM?",
        "answers": "MG = S_v - S_{w v}",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Equation",
        "evidence_source": "equation",
        "evidence_context": "Then the MG metric can be derived from the following formulation: $$M G=S_{v}-S_{w v}$$",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ee7aed3-f04a-4b9d-a14b-3ebb3845b6bd",
        "questions": "How do you calculate the multi-modal leakage (ML) metric?",
        "answers": "ML = max(0, S_{w v} - S_t)",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Equation",
        "evidence_source": "equation",
        "evidence_context": "Then the ML metric is formulated as follows: $$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2403.20330v2",
        "ID": "0ef2f527-30c4-4834-8dcf-681d449d215d",
        "questions": "Can the multi-modal leakage (ML) metric ever be negative based on the formula provided?",
        "answers": "No",
        "context": "Mathematics (MA). Math is a foundational pillar of logical and analytical reasoning and encompasses a broad spectrum of capabilities essential for understanding, applying, and interpreting quantitative and spatial information. We primarily consider three aspects for evaluating LVLMs' logical thinking prowess: 1) numeric commonsense \\& calculation; 2) geometry; and 3) statistical analysis.\n\n\n4.3 Multi-modal Gain/Leakage\n\n\nGiven our observation of the potential for inadvertent leakage of some evaluation samples during the multi-modal training process, the vanilla evaluation approach struggles to reveal LVLMs' actual performance gains derived from multi-modal training and fails to enable fair comparison with other competitors. Therefore, we propose two novel metrics to separately assess the degree of data leakage and actual performance gain from the multi-modal training process.\nTo calculate the multi-modal gain (MG) metric for a given LVLM on a particular benchmark, we need to compute the scores of the same LVLM with and without visual inputs, separately denoted as $S_{v}$ and $S_{w v}$. Then the MG metric can be derived from the following formulation:\n$$M G=S_{v}-S_{w v}$$\n\nTo calculate the multi-modal leakage (ML) metric, we need to compute the extra score of the given LVLM's LLM base (without any multi-modal training), denoted as $S_{t}$. Then the ML metric is formulated as follows:\n$$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$\n\n5 Experiments\n\nIn this section, we conduct a systematic analysis of the proposed MMStar benchmark along with the MG/ML metrics. We detail the experimental setup in Section 5.1, study and analyze the performance of 16 leading LVLMs on MMStar in Section 5.2, and extensively investigate the MG/ML metrics of 16 LVLMs across 6 popular benchmarks and our MMStar in Section 5.3.\n\n5.1 Experimental Setups\n\nEvaluation models. 1) Baselines: We utilize random choice and frequent choice strategies to serve as the baselines. The former randomly selects an option as the answer, while the latter selects the most frequent option within each benchmark dimension. 2) Large Language Models: We prepare two closed-source LLMs, GPT4 [34] and GeminiPro [41], and 20 popular open-source LLMs sizing from 1.8B to 72B for text-only evaluation, such as Qwen series [1], LLaMA2 series [43], Phi2 [32], Vicuna series [8], Deepseek series [3], InternLM2 series [42], Baichuan2 series [47], Yi series [49], Mistral series [18, 19]. Additionally, all the open-source LLMs we used are their Chat versions. and 3) Large Vision-Language Models: We prepare two closed-source LVLMs, GPT4V [35] and GeminiPro-Vision [41], and 14 popular open-source LVLMs sizing from 3B to 60B, such as TinyLLaVA-3B [53], Yi-VL series [49], Qwen-VL-Chat [2], LLaVA-1.5 series [24], ShareGPT4V-7B [5], Monkey-Chat [23], LLaVA-Next [25], Deepseek-VL-7B [28], LLaVA-Next34B [25], CogVLM-Chat-17B [45], InternVL-Chat-v1.2 [6], Sphinx-X-8x7B [15].\nImplementation details. For evaluating LLMs on existing benchmarks, we employ both 0 -shot and 2 -shot strategies and will specify which is utilized when reporting results. For evaluating LLMs on MMStar, the 0 -shot strategy yields poor scores, making comparisons difficult. Therefore, we exclusively utilize the 2 -shot strategy to decrease the frequency of refusal to answer. Moreover, All LVLMs are evaluated utilizing the 0 -shot strategy across all benchmarks to ensure a fair comparison. When evaluating LVLMs under the 'LVLM-text' setting (i.e. answer without the image), most LVLMs work well by simply removing the image tokens from their default input tokens.However, GeminiPro-Vision [41] and CogVLM-Chat [45] require the replacement of the original images with pure grey images to bypass image content input and operate correctly. Given that all questions are ensured to be converted into a multiple-choice format, we develop some heuristic matching rules to calculate accuracy, avoiding the cumbersome process of re-invoking GPT4 for answer extraction. Moreover, all experiments in this study are conducted within the same codebase modified from VLMEvalKit [10], and utilize NVIDIA A100 GPUs for non-API-based evaluation.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "equation",
        "evidence_context": "Then the ML metric is formulated as follows: $$M L=\\max \\left(0, S_{w v}-S_{t}\\right)$$",
        "evidence_page_no": 9,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f0ac134-2e0d-44ad-8888-d15e7eb7a8f3",
        "questions": "What is the confidence score of the overlap token set in the Selfmem framework?",
        "answers": "0.76",
        "context": "Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented generator (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates to select memory for (a).\nretrieved memory) has already learned to discriminate between different memories in both oracle and random scenarios, without updating the model weights.\nTo evaluate the second conjecture, we first define the token sets of the reference, retrieved memory, and beam memory as $\\mathcal{R}, \\mathcal{M}$, and $\\mathcal{B}$, respectively. The overlap token set, denoted by $\\mathcal{O}$, is defined as the tokens that overlap with the references in the beam memory but not in the retrieved memory, which is represented as $\\mathcal{R} \\cap \\mathcal{B}-\\mathcal{R} \\cap \\mathcal{M}$. $\\mathcal{O}$ is considered as the additional information provided by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set confidence score, $\\psi(\\cdot)$, as follows:\n$$\\psi(\\cdot)=\\frac{1}{|\\cdot|} \\sum_{y^{i} \\in} p\\left(y_{i} \\mid x, y_{<i}\\right)$$\nwhere $p\\left(y_{i} \\mid x, y_{<i}\\right)$ is defined by the generation model. $\\psi(\\cdot)$ measures the confidence with which the generation model generates the tokens. The value of $\\psi(\\mathcal{R})$ is 0.58 , while that of $\\mathcal{O}$ is 0.76 , indicating that the generator is relatively confident in generating tokens in $\\mathcal{O}$, and therefore does not need to resort to external memory [38]. Beam search ranks generated candidates based on $p(y \\mid x)$, where the selected memory falls within the confidence region of the generator and consequently provides no information gain. This observation motivates us to select memory according to metrics other than $p(y \\mid x)$ in the memory selector (\u00a73.3).\n\n\n3.2 Retrieval-augmented Generator\n\n\nGiven a text pair $(x, y)$, where $x=\\left\\{\\mathrm{x}_{1}, \\ldots, \\mathrm{x}_{|x|}\\right\\}$ is the source, $y=\\left\\{\\mathrm{y}_{1}, \\ldots, \\mathrm{y}_{|y|}\\right\\}$ is the target. They could be (document, summary) in summarization, (context, response) in dialogue generation or (source, target) in machine translation. The retrieval-augmented generation would first use $x$ to retrieve memory $m$ from datastore $\\mathbb{D}$. Then the generator $G_{\\xi}(x, m)$, parameterized by $\\xi$, would take both $x$ and $m$ as input to generate the target sentence $y$. In this paper, following standard practice, we choose the training set as $\\mathbb{D}=\\left\\{\\left(x^{i}, y^{i}\\right)\\right\\}_{i=1}^{|\\mathbb{D}|}$. For LLM as $G_{\\xi}$, we use the standard in-context learning format to give $(x, y)$ as demonstration example. For tunable generator $G_{\\xi}$, we only keep the target side of top- 1 retrieval results as memory and we consider two commonly used architectures: Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].\n\nJoint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is the concatenation of $x$ and $m$. The encoder would first map the input into the hidden states $H$ :\n$$H=\\operatorname{Encoder}(x[\\mathrm{SEP}] m)$$",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "The value of $\\psi(\\mathcal{R})$ is 0.58 , while that of $\\mathcal{O}$ is 0.76 , indicating that the generator is relatively confident in generating tokens in $\\mathcal{O}$, and therefore does not need to resort to external memory [38].",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f0e6955-9775-4eaf-aac4-559668c44e05",
        "questions": "In the Selfmem framework, what is the role of the retrieval-augmented generator when given a text pair (x, y)?",
        "answers": "The retrieval-augmented generator uses x to retrieve memory m from datastore D and takes both x and m as input to generate the target sentence y.",
        "context": "Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented generator (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates to select memory for (a).\nretrieved memory) has already learned to discriminate between different memories in both oracle and random scenarios, without updating the model weights.\nTo evaluate the second conjecture, we first define the token sets of the reference, retrieved memory, and beam memory as $\\mathcal{R}, \\mathcal{M}$, and $\\mathcal{B}$, respectively. The overlap token set, denoted by $\\mathcal{O}$, is defined as the tokens that overlap with the references in the beam memory but not in the retrieved memory, which is represented as $\\mathcal{R} \\cap \\mathcal{B}-\\mathcal{R} \\cap \\mathcal{M}$. $\\mathcal{O}$ is considered as the additional information provided by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set confidence score, $\\psi(\\cdot)$, as follows:\n$$\\psi(\\cdot)=\\frac{1}{|\\cdot|} \\sum_{y^{i} \\in} p\\left(y_{i} \\mid x, y_{<i}\\right)$$\nwhere $p\\left(y_{i} \\mid x, y_{<i}\\right)$ is defined by the generation model. $\\psi(\\cdot)$ measures the confidence with which the generation model generates the tokens. The value of $\\psi(\\mathcal{R})$ is 0.58 , while that of $\\mathcal{O}$ is 0.76 , indicating that the generator is relatively confident in generating tokens in $\\mathcal{O}$, and therefore does not need to resort to external memory [38]. Beam search ranks generated candidates based on $p(y \\mid x)$, where the selected memory falls within the confidence region of the generator and consequently provides no information gain. This observation motivates us to select memory according to metrics other than $p(y \\mid x)$ in the memory selector (\u00a73.3).\n\n\n3.2 Retrieval-augmented Generator\n\n\nGiven a text pair $(x, y)$, where $x=\\left\\{\\mathrm{x}_{1}, \\ldots, \\mathrm{x}_{|x|}\\right\\}$ is the source, $y=\\left\\{\\mathrm{y}_{1}, \\ldots, \\mathrm{y}_{|y|}\\right\\}$ is the target. They could be (document, summary) in summarization, (context, response) in dialogue generation or (source, target) in machine translation. The retrieval-augmented generation would first use $x$ to retrieve memory $m$ from datastore $\\mathbb{D}$. Then the generator $G_{\\xi}(x, m)$, parameterized by $\\xi$, would take both $x$ and $m$ as input to generate the target sentence $y$. In this paper, following standard practice, we choose the training set as $\\mathbb{D}=\\left\\{\\left(x^{i}, y^{i}\\right)\\right\\}_{i=1}^{|\\mathbb{D}|}$. For LLM as $G_{\\xi}$, we use the standard in-context learning format to give $(x, y)$ as demonstration example. For tunable generator $G_{\\xi}$, we only keep the target side of top- 1 retrieval results as memory and we consider two commonly used architectures: Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].\n\nJoint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is the concatenation of $x$ and $m$. The encoder would first map the input into the hidden states $H$ :\n$$H=\\operatorname{Encoder}(x[\\mathrm{SEP}] m)$$",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The retrieval-augmented generation would first use $x$ to retrieve memory $m$ from datastore $\\mathbb{D}$. Then the generator $G_{\\xi}(x, m)$, parameterized by $\\xi$, would take both $x$ and $m$ as input to generate the target sentence $y$.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f128210-703f-49ef-94fe-a8e677f8e9a5",
        "questions": "What is the equation used to compute the set confidence score in the Selfmem framework?",
        "answers": "$$\\\\psi(\\\\cdot)=\\\\frac{1}{|\\\\cdot|} \\\\sum_{y^{i} \\\\in} p\\\\left(y_{i} \\\\mid x, y_{<i}\\\\right)$$",
        "context": "Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented generator (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates to select memory for (a).\nretrieved memory) has already learned to discriminate between different memories in both oracle and random scenarios, without updating the model weights.\nTo evaluate the second conjecture, we first define the token sets of the reference, retrieved memory, and beam memory as $\\mathcal{R}, \\mathcal{M}$, and $\\mathcal{B}$, respectively. The overlap token set, denoted by $\\mathcal{O}$, is defined as the tokens that overlap with the references in the beam memory but not in the retrieved memory, which is represented as $\\mathcal{R} \\cap \\mathcal{B}-\\mathcal{R} \\cap \\mathcal{M}$. $\\mathcal{O}$ is considered as the additional information provided by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set confidence score, $\\psi(\\cdot)$, as follows:\n$$\\psi(\\cdot)=\\frac{1}{|\\cdot|} \\sum_{y^{i} \\in} p\\left(y_{i} \\mid x, y_{<i}\\right)$$\nwhere $p\\left(y_{i} \\mid x, y_{<i}\\right)$ is defined by the generation model. $\\psi(\\cdot)$ measures the confidence with which the generation model generates the tokens. The value of $\\psi(\\mathcal{R})$ is 0.58 , while that of $\\mathcal{O}$ is 0.76 , indicating that the generator is relatively confident in generating tokens in $\\mathcal{O}$, and therefore does not need to resort to external memory [38]. Beam search ranks generated candidates based on $p(y \\mid x)$, where the selected memory falls within the confidence region of the generator and consequently provides no information gain. This observation motivates us to select memory according to metrics other than $p(y \\mid x)$ in the memory selector (\u00a73.3).\n\n\n3.2 Retrieval-augmented Generator\n\n\nGiven a text pair $(x, y)$, where $x=\\left\\{\\mathrm{x}_{1}, \\ldots, \\mathrm{x}_{|x|}\\right\\}$ is the source, $y=\\left\\{\\mathrm{y}_{1}, \\ldots, \\mathrm{y}_{|y|}\\right\\}$ is the target. They could be (document, summary) in summarization, (context, response) in dialogue generation or (source, target) in machine translation. The retrieval-augmented generation would first use $x$ to retrieve memory $m$ from datastore $\\mathbb{D}$. Then the generator $G_{\\xi}(x, m)$, parameterized by $\\xi$, would take both $x$ and $m$ as input to generate the target sentence $y$. In this paper, following standard practice, we choose the training set as $\\mathbb{D}=\\left\\{\\left(x^{i}, y^{i}\\right)\\right\\}_{i=1}^{|\\mathbb{D}|}$. For LLM as $G_{\\xi}$, we use the standard in-context learning format to give $(x, y)$ as demonstration example. For tunable generator $G_{\\xi}$, we only keep the target side of top- 1 retrieval results as memory and we consider two commonly used architectures: Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].\n\nJoint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is the concatenation of $x$ and $m$. The encoder would first map the input into the hidden states $H$ :\n$$H=\\operatorname{Encoder}(x[\\mathrm{SEP}] m)$$",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$$\\\\psi(\\\\cdot)=\\\\frac{1}{|\\\\cdot|} \\\\sum_{y^{i} \\\\in} p\\\\left(y_{i} \\\\mid x, y_{<i}\\\\right)$$",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f1b88df-b61c-44e4-aa74-cb72810bd9bd",
        "questions": "Which conference proceedings include the work by Gautier Izacard and Edouard Grave on leveraging passage retrieval with generative models for open domain question answering?",
        "answers": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
        "context": "[26] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal learning of knowledge retriever and response ranker for knowledge-grounded conversations. In Proc. of COLING, 2022.\n[27] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There are a thousand hamlets in a thousand people's eyes: Enhancing knowledge-grounded dialogue with personal memory. In Proc. of ACL, 2022.\n[28] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural machine translation. In Proc. of AAAI, 2018.\n[29] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In Proc. of ICML, 2020.\n[30] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit framework for predicting structured outputs. In Proc. of NeurIPS, 2018.\n[31] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine translation with translation memory. In Proc. of ACL, 2021.\n[32] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in the NICT-2 neural machine translation system at WAT2017. In Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017, Taipei, Taiwan, November 27- December 1, 2017, 2017.\n[33] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, April 2021. Association for Computational Linguistics.\n[34] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Paraphrase generation by learning how to edit from samples. In Proc. of ACL, 2020.\n[35] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation. In Proc. of ICLR, 2021.\n[36] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proc. of ICLR, 2020 .\n[37] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, 2004.\n[38] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In Proc. of ICML, 2016.\n[39] Ann Lee, Michael Auli, and Marc'Aurelio Ranzato. Discriminative reranking for neural machine translation. In Proc. of ACL, 2021.\n[40] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL, 2020 .\n[41] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proc. of NeurIPS, 2020.\n[42] Jinpeng Li, Yingce Xia, Rui Yan, Hongda Sun, Dongyan Zhao, and Tie-Yan Liu. Stylized dialogue generation with multi-pass dual learning. In Proc. of NeurIPS, 2021.\n[43] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proc. of NAACL, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, April 2021. Association for Computational Linguistics.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f204c42-32eb-473d-9210-917fa3838004",
        "questions": "In which year was the paper by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang on retrieval augmented language model pre-training published?",
        "answers": "2020",
        "context": "[26] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal learning of knowledge retriever and response ranker for knowledge-grounded conversations. In Proc. of COLING, 2022.\n[27] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There are a thousand hamlets in a thousand people's eyes: Enhancing knowledge-grounded dialogue with personal memory. In Proc. of ACL, 2022.\n[28] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural machine translation. In Proc. of AAAI, 2018.\n[29] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In Proc. of ICML, 2020.\n[30] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit framework for predicting structured outputs. In Proc. of NeurIPS, 2018.\n[31] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine translation with translation memory. In Proc. of ACL, 2021.\n[32] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in the NICT-2 neural machine translation system at WAT2017. In Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017, Taipei, Taiwan, November 27- December 1, 2017, 2017.\n[33] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, April 2021. Association for Computational Linguistics.\n[34] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Paraphrase generation by learning how to edit from samples. In Proc. of ACL, 2020.\n[35] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation. In Proc. of ICLR, 2021.\n[36] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proc. of ICLR, 2020 .\n[37] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, 2004.\n[38] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In Proc. of ICML, 2016.\n[39] Ann Lee, Michael Auli, and Marc'Aurelio Ranzato. Discriminative reranking for neural machine translation. In Proc. of ACL, 2021.\n[40] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL, 2020 .\n[41] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proc. of NeurIPS, 2020.\n[42] Jinpeng Li, Yingce Xia, Rui Yan, Hongda Sun, Dongyan Zhao, and Tie-Yan Liu. Stylized dialogue generation with multi-pass dual learning. In Proc. of NeurIPS, 2021.\n[43] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proc. of NAACL, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In Proc. of ICML, 2020.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f266902-f574-455f-8e85-6b8622c66d8e",
        "questions": "Did Urvashi Khandelwal contribute to both nearest neighbor machine translation and generalization through memorization in language models?",
        "answers": "Yes",
        "context": "[26] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal learning of knowledge retriever and response ranker for knowledge-grounded conversations. In Proc. of COLING, 2022.\n[27] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There are a thousand hamlets in a thousand people's eyes: Enhancing knowledge-grounded dialogue with personal memory. In Proc. of ACL, 2022.\n[28] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural machine translation. In Proc. of AAAI, 2018.\n[29] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In Proc. of ICML, 2020.\n[30] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit framework for predicting structured outputs. In Proc. of NeurIPS, 2018.\n[31] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine translation with translation memory. In Proc. of ACL, 2021.\n[32] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in the NICT-2 neural machine translation system at WAT2017. In Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017, Taipei, Taiwan, November 27- December 1, 2017, 2017.\n[33] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, April 2021. Association for Computational Linguistics.\n[34] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Paraphrase generation by learning how to edit from samples. In Proc. of ACL, 2020.\n[35] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation. In Proc. of ICLR, 2021.\n[36] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proc. of ICLR, 2020 .\n[37] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, 2004.\n[38] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In Proc. of ICML, 2016.\n[39] Ann Lee, Michael Auli, and Marc'Aurelio Ranzato. Discriminative reranking for neural machine translation. In Proc. of ACL, 2021.\n[40] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL, 2020 .\n[41] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proc. of NeurIPS, 2020.\n[42] Jinpeng Li, Yingce Xia, Rui Yan, Hongda Sun, Dongyan Zhao, and Tie-Yan Liu. Stylized dialogue generation with multi-pass dual learning. In Proc. of NeurIPS, 2021.\n[43] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proc. of NAACL, 2016.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation. In Proc. of ICLR, 2021. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In Proc. of ICLR, 2020.",
        "evidence_page_no": 11,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f29ad2b-e8f6-477b-93c8-aca1d8ffd19e",
        "questions": "Which conference proceedings included the paper by Matt Post calling for clarity in reporting BLEU scores?",
        "answers": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 November 1, 2018",
        "context": "[61] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexander M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Proc. of NAACL, 2004.\n[62] OpenAI. GPT-4 technical report. CoRR, 2023.\n[63] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Proc. of EMNLP Findings, 2021 .\n[64] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text generation with exemplar-based adaptive decoding. In Proc. of NAACL, 2019.\n[65] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Chris Pal. On extractive and abstractive neural document summarization with transformer language models. In Proc. of EMNLP, 2020.\n[66] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 November 1, 2018, 2018.\n[67] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena in neural machine translation. In Proc. of EMNLP Findings, 2020.\n[68] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-ofexperts re-ranking framework for abstractive summarization. In Proc. of ACL, 2022.\n[69] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Towards summary candidates fusion. CoRR, 2022.\n[70] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 2009.\n[71] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model scoring. In Proc. of ACL, 2020.\n[72] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.\n[73] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proc. of ACL, 2019.\n[74] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proc. of ICML, 2018.\n[75] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine translation. In Proc. of NAACL, 2004.\n[76] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu Chen. Joint generator-ranker learning for natural language generation. $\\operatorname{CoRR}, 2022$.\n[77] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.\n[78] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models. CoRR, 2023.\n[79] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models. CoRR, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 November 1, 2018, 2018.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f2a4e48-453d-4e11-b3a6-13e8a01c26cd",
        "questions": "In which year was the paper 'Neural machine translation of rare words with subword units' by Rico Sennrich, Barry Haddow, and Alexandra Birch published?",
        "answers": "2016",
        "context": "[61] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexander M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Proc. of NAACL, 2004.\n[62] OpenAI. GPT-4 technical report. CoRR, 2023.\n[63] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Proc. of EMNLP Findings, 2021 .\n[64] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text generation with exemplar-based adaptive decoding. In Proc. of NAACL, 2019.\n[65] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Chris Pal. On extractive and abstractive neural document summarization with transformer language models. In Proc. of EMNLP, 2020.\n[66] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 November 1, 2018, 2018.\n[67] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena in neural machine translation. In Proc. of EMNLP Findings, 2020.\n[68] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-ofexperts re-ranking framework for abstractive summarization. In Proc. of ACL, 2022.\n[69] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Towards summary candidates fusion. CoRR, 2022.\n[70] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 2009.\n[71] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model scoring. In Proc. of ACL, 2020.\n[72] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.\n[73] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proc. of ACL, 2019.\n[74] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proc. of ICML, 2018.\n[75] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine translation. In Proc. of NAACL, 2004.\n[76] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu Chen. Joint generator-ranker learning for natural language generation. $\\operatorname{CoRR}, 2022$.\n[77] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.\n[78] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models. CoRR, 2023.\n[79] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models. CoRR, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f2f6ba7-0166-472f-a41c-d0b73aa086f4",
        "questions": "Did Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen publish a paper on summary candidates fusion in CoRR in 2022?",
        "answers": "Yes",
        "context": "[61] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexander M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Proc. of NAACL, 2004.\n[62] OpenAI. GPT-4 technical report. CoRR, 2023.\n[63] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Proc. of EMNLP Findings, 2021 .\n[64] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text generation with exemplar-based adaptive decoding. In Proc. of NAACL, 2019.\n[65] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Chris Pal. On extractive and abstractive neural document summarization with transformer language models. In Proc. of EMNLP, 2020.\n[66] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 November 1, 2018, 2018.\n[67] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena in neural machine translation. In Proc. of EMNLP Findings, 2020.\n[68] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-ofexperts re-ranking framework for abstractive summarization. In Proc. of ACL, 2022.\n[69] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Towards summary candidates fusion. CoRR, 2022.\n[70] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 2009.\n[71] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model scoring. In Proc. of ACL, 2020.\n[72] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proc. of ACL, 2016.\n[73] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proc. of ACL, 2019.\n[74] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proc. of ICML, 2018.\n[75] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine translation. In Proc. of NAACL, 2004.\n[76] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu Chen. Joint generator-ranker learning for natural language generation. $\\operatorname{CoRR}, 2022$.\n[77] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.\n[78] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models. CoRR, 2023.\n[79] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models. CoRR, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Towards summary candidates fusion. CoRR, 2022.",
        "evidence_page_no": 13,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f3ae7b3-dbe1-4b6e-9513-31f884f15545",
        "questions": "In which year was the paper 'Neural machine translation with monolingual translation memory' by Deng Cai and others published?",
        "answers": "2021",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f3b07e3-9d7a-474c-bcb2-8b77f5d2e1e6",
        "questions": "Which conference proceedings include the paper 'Encoding gated translation memory into neural machine translation' authored by Qian Cao and Deyi Xiong?",
        "answers": "EMNLP",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f424ffa-8abb-472c-8aae-0c3ad31224e0",
        "questions": "Is the paper 'Towards personalized review summarization by modeling historical reviews from customer and product separately' by Xin Cheng and others a preprint on arXiv?",
        "answers": "Yes",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f45d01a-3b8e-4413-808e-1b9882dade1d",
        "questions": "What is the BLEU score for the Transformer model with self-memory on the En to De translation task in the test set?",
        "answers": "59.49",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Transformer $_{\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49*",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f493112-bc03-4879-abb7-6a3232975fbc",
        "questions": "Which model achieved the highest BLEU score on the De to En translation task in the test set using retrieval memory?",
        "answers": "MonoNMT",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f4b3930-1768-4fd9-9950-c9b19762c946",
        "questions": "Does the self-memory approach in the En to Es translation task show a higher BLEU score for the hypothesis compared to the retrieval memory approach?",
        "answers": "Yes",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f53ffa5-eb8b-45d3-8c1a-98c1f5b546e6",
        "questions": "Which system achieved the highest B-1 score in the dialogue generation task on DailyDialog?",
        "answers": "BART $_{\\text {joint }} \\dagger$ (Self)",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "$B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f541ff5-1ece-4ac1-b27a-1ff4b45bdd37",
        "questions": "What is the D-2 score for the BART model with retrieval memory in the dialogue generation task on DailyDialog?",
        "answers": "26.01",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Retrieval & 29.50 & 21.89 & 4.74 & 26.01",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f65fa0f-1d1c-4f46-87f8-2dcafc84d803",
        "questions": "Does the document suggest that retrieval-augmented models with self-memory augmentation perform better on long-tail inputs compared to parametric models?",
        "answers": "Yes",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f66b32d-e19b-4837-9c6a-4b5e7d859725",
        "questions": "Which summarization system achieved the highest R-1 score on the BigPatent dataset when using self-memory?",
        "answers": "BRIO_joint",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "BRIO_joint \\dagger & Self & 62.9 & 48.1 & 59.6",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f6f5893-5356-4138-b7f4-89744ff04925",
        "questions": "What is the improvement in the B-1 score of BART_joint when self-memory is used for dialogue generation tasks?",
        "answers": "3.08 B-1",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\text {joint }}$.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f784eb0-c966-427b-a9af-9e179a979f42",
        "questions": "Does the cross-lingual LLM with designed examples outperform the supervised baselines in the translation task?",
        "answers": "No",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f7b2413-3e36-44ac-87bd-0b54b97eb522",
        "questions": "What is the BLEU score for the Transformer joint model with self memory in the JRC-Acquis En to De translation task?",
        "answers": "60.11",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Transformer ${ }_{\text {joint }}$ & Self & 60.11 & 73.25 & 32.62",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f8b4ec8-66ae-41bd-b3e5-7e85cd4aa89a",
        "questions": "Which system achieved the highest R-1 score on the XSum dataset according to Table 10?",
        "answers": "Selfmem",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f8f2ec6-64f6-4d04-ad7e-2d589d993a9b",
        "questions": "What is the R-2 score for the system labeled [87](L) on the BigPatent dataset?",
        "answers": "43.3",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "{$[87]$ (L) } & 60.7 & 43.3 & 51.3",
        "evidence_page_no": 17,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f95b88a-8960-4a04-b6bb-de06429a8f86",
        "questions": "Which conference did Yuguang Wang and his team present their work on Sogou neural machine translation systems?",
        "answers": "The Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017",
        "context": "[80] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted translation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit 2009, Ottawa, Canada, August 26-30, 2009, 2009.\n[81] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An ensemble of retrieval- and generation-based dialog systems. CoRR, 2016.\n[82] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and D\u00e1niel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of LREC, 2006.\n[83] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary, rewrite the secondary: A two-stage approach for paraphrase generation. In Proc. of ACL Findings, 2021.\n[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n[85] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Prompting palm for translation: Assessing strategies and performance, 2023.\n[86] Oriol Vinyals and Quoc V. Le. A neural conversational model. CoRR, 2015.\n[87] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. In Proc. of ACL, 2022.\n[88] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017, 2017.\n[89] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2nd International Workshop on SearchOriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018, 2018.\n[90] Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, and Haifeng Wang. BASS: boosting abstractive summarization with unified semantic graph. In Proc. of $A C L, 2021$.\n[91] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response generation by context-aware prototype editing. In Proc. of AAAI, 2019.\n[92] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory for neural machine translation. In Proc. of AAAI, 2019.\n[93] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with similar translations. In Proc. of ACL, 2020.\n[94] Masaru Yamada. The effect of translation memory databases on productivity. Translation research projects, 2011.\n[95] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. CoRR, 2022.\n[96] Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. Trans. Assoc. Comput. Linguistics, 2021.\n[97] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017, 2017.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0f95e151-d3fd-4719-b4aa-1dca8f534838",
        "questions": "In which year did Shuohang Wang and his colleagues publish their work on the value of training data in the ACL proceedings?",
        "answers": "2022",
        "context": "[80] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted translation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit 2009, Ottawa, Canada, August 26-30, 2009, 2009.\n[81] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An ensemble of retrieval- and generation-based dialog systems. CoRR, 2016.\n[82] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and D\u00e1niel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of LREC, 2006.\n[83] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary, rewrite the secondary: A two-stage approach for paraphrase generation. In Proc. of ACL Findings, 2021.\n[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n[85] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Prompting palm for translation: Assessing strategies and performance, 2023.\n[86] Oriol Vinyals and Quoc V. Le. A neural conversational model. CoRR, 2015.\n[87] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. In Proc. of ACL, 2022.\n[88] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017, 2017.\n[89] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2nd International Workshop on SearchOriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018, 2018.\n[90] Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, and Haifeng Wang. BASS: boosting abstractive summarization with unified semantic graph. In Proc. of $A C L, 2021$.\n[91] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response generation by context-aware prototype editing. In Proc. of AAAI, 2019.\n[92] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory for neural machine translation. In Proc. of AAAI, 2019.\n[93] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with similar translations. In Proc. of ACL, 2020.\n[94] Masaru Yamada. The effect of translation memory databases on productivity. Translation research projects, 2011.\n[95] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. CoRR, 2022.\n[96] Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. Trans. Assoc. Comput. Linguistics, 2021.\n[97] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. In Proc. of ACL, 2022.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fa07a3f-2e29-4065-afa2-31e959acdd98",
        "questions": "Did Michihiro Yasunaga contribute to research on retrieval-augmented multimodal language modeling in 2022?",
        "answers": "Yes",
        "context": "[80] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted translation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit 2009, Ottawa, Canada, August 26-30, 2009, 2009.\n[81] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An ensemble of retrieval- and generation-based dialog systems. CoRR, 2016.\n[82] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and D\u00e1niel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of LREC, 2006.\n[83] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary, rewrite the secondary: A two-stage approach for paraphrase generation. In Proc. of ACL Findings, 2021.\n[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.\n[85] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. Prompting palm for translation: Assessing strategies and performance, 2023.\n[86] Oriol Vinyals and Quoc V. Le. A neural conversational model. CoRR, 2015.\n[87] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data. In Proc. of ACL, 2022.\n[88] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi, Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017, 2017.\n[89] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of the 2nd International Workshop on SearchOriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018, 2018.\n[90] Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, and Haifeng Wang. BASS: boosting abstractive summarization with unified semantic graph. In Proc. of $A C L, 2021$.\n[91] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response generation by context-aware prototype editing. In Proc. of AAAI, 2019.\n[92] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory for neural machine translation. In Proc. of AAAI, 2019.\n[93] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with similar translations. In Proc. of ACL, 2020.\n[94] Masaru Yamada. The effect of translation memory databases on productivity. Translation research projects, 2011.\n[95] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. CoRR, 2022.\n[96] Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. Trans. Assoc. Comput. Linguistics, 2021.\n[97] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators, 2023.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. CoRR, 2022.",
        "evidence_page_no": 14,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fa300b3-2f8b-477b-8897-82b2495f3505",
        "questions": "What is the role of the memory selector $S_{\\theta}(x, c)$ in the context of the document?",
        "answers": "The role of memory selector $S_{\\theta}(x, c)$ is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$.",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fa34543-b36c-417a-9162-3bd2cad60db8",
        "questions": "What is the training goal for the memory selector $S_{\\theta}$ according to the document?",
        "answers": "The training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$.",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fa66354-2fca-4ae3-aa97-673e35001aa9",
        "questions": "What is the equation used to calculate the probability distribution $p_{S_{\\theta}}(c_{i} \\mid x)$ over the candidate pool $\\mathbb{C}$?",
        "answers": "$p_{S_{\\theta}}(c_{i} \\mid x)=\\frac{\\exp (S_{\\theta}(x[\\mathrm{SEP}] c_{i}))}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp (S_{\\theta}(x[\\mathrm{SEP}] c_{j}))}$",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Equation",
        "evidence_source": "equation",
        "evidence_context": "$p_{S_{\\theta}}(c_{i} \\mid x)=\\frac{\\exp (S_{\\theta}(x[\\mathrm{SEP}] c_{i}))}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp (S_{\\theta}(x[\\mathrm{SEP}] c_{j}))}$",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fa8aee9-1ba4-4eaf-84f4-d346d3016d9e",
        "questions": "What is the name of the novel framework proposed by Xin Cheng and colleagues for retrieval-augmented text generation?",
        "answers": "Selfmem",
        "context": "Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory\n\n\n\nXin Cheng $^{1 \\quad$ Di Luo $^{2} \\quad$ Xiuying Chen ${ }^{3} \\quad$ Lemao Liu ${ }^{4} \\quad$ Dongyan Zhao $^{1} \\quad$ Rui Yan ${ }^{2}$ \\\\ ${ }^{1}$ Peking University $\\quad{ }^{2}$ Renmin University of China \\\\ ${ }^{3}$ KAUST $\\quad{ }^{4}$ Tencent AI Lab \\\\ chengxin1998@stu.pku.edu.cn\n}\n\n\nWith direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research ${ }^{1}$.\n\n\n1 Introduction\n\nIn recent years, retrieval-augmented text generation has attracted growing interest across various fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned small model or a large language model (LLM) with access to an external database (typically the training corpus) using information retrieval techniques. Subsequently, the generation process is conducted based on both the input text and the retrieved memory.\n\nIn this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits the highest similarity to the current input $[36,96,49]$. This aligns with the human intuition that a more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with the final translation quality, regardless of other factors that may influence translation quality (e.g.,\n\n\\footnotetext{\n${ }^{1}$ Code and data available at: https://github.com/Hannibal046/SelfMemory\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fab2ba6-c8d7-4195-a8c3-bb899d3141ba",
        "questions": "What ROUGE-1 score did the Selfmem framework achieve on the XSum dataset?",
        "answers": "50.3",
        "context": "Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory\n\n\n\nXin Cheng $^{1 \\quad$ Di Luo $^{2} \\quad$ Xiuying Chen ${ }^{3} \\quad$ Lemao Liu ${ }^{4} \\quad$ Dongyan Zhao $^{1} \\quad$ Rui Yan ${ }^{2}$ \\\\ ${ }^{1}$ Peking University $\\quad{ }^{2}$ Renmin University of China \\\\ ${ }^{3}$ KAUST $\\quad{ }^{4}$ Tencent AI Lab \\\\ chengxin1998@stu.pku.edu.cn\n}\n\n\nWith direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research ${ }^{1}$.\n\n\n1 Introduction\n\nIn recent years, retrieval-augmented text generation has attracted growing interest across various fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned small model or a large language model (LLM) with access to an external database (typically the training corpus) using information retrieval techniques. Subsequently, the generation process is conducted based on both the input text and the retrieved memory.\n\nIn this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits the highest similarity to the current input $[36,96,49]$. This aligns with the human intuition that a more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with the final translation quality, regardless of other factors that may influence translation quality (e.g.,\n\n\\footnotetext{\n${ }^{1}$ Code and data available at: https://github.com/Hannibal046/SelfMemory\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Our approach achieves state-of-the-art results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fbc0318-b6e4-416e-b066-8cc99e7f0271",
        "questions": "Does the Selfmem framework utilize a memory selector to choose an output as memory for subsequent generation rounds?",
        "answers": "Yes",
        "context": "Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory\n\n\n\nXin Cheng $^{1 \\quad$ Di Luo $^{2} \\quad$ Xiuying Chen ${ }^{3} \\quad$ Lemao Liu ${ }^{4} \\quad$ Dongyan Zhao $^{1} \\quad$ Rui Yan ${ }^{2}$ \\\\ ${ }^{1}$ Peking University $\\quad{ }^{2}$ Renmin University of China \\\\ ${ }^{3}$ KAUST $\\quad{ }^{4}$ Tencent AI Lab \\\\ chengxin1998@stu.pku.edu.cn\n}\n\n\nWith direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research ${ }^{1}$.\n\n\n1 Introduction\n\nIn recent years, retrieval-augmented text generation has attracted growing interest across various fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned small model or a large language model (LLM) with access to an external database (typically the training corpus) using information retrieval techniques. Subsequently, the generation process is conducted based on both the input text and the retrieved memory.\n\nIn this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits the highest similarity to the current input $[36,96,49]$. This aligns with the human intuition that a more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with the final translation quality, regardless of other factors that may influence translation quality (e.g.,\n\n\\footnotetext{\n${ }^{1}$ Code and data available at: https://github.com/Hannibal046/SelfMemory\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fc50523-3e8a-4ce6-8c52-4949c3248f55",
        "questions": "What is the ROUGE-1 score for the BRIO joint model on the XSum dataset?",
        "answers": "50.3",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "BRIO $_{\text {joint }}$ & 50.3 & $0.49986-0.50602$",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fd29271-b440-44eb-83ed-8da8a35d9f4e",
        "questions": "What is the average input length for the BigPatent dataset in the Generation Latency analysis?",
        "answers": "1024",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Average Input Length Average Output Length & 87 & 512 & 1024 & 71",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fdb691b-18d4-40f2-89f3-66b7b3d6ec5f",
        "questions": "What is the latency factor for Hypothesis Generation using Selfmem on the XSum dataset with CUDA?",
        "answers": "\u00d7 2.99",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$",
        "evidence_page_no": 18,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fde45c5-ddf5-452d-9ad0-53de6fd6f23f",
        "questions": "What is the name of the framework proposed for retrieval-augmented text generation that combines the primal and dual problems?",
        "answers": "Selfmem",
        "context": "7 Conclusion\n\n\nFor the first time, we investigate the fundamental limitation of bounded memory in the current retrieval-augmented literature. We combine the primal and dual problems together and propose Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model with its own output. We conduct comprehensive experiments across various text generation tasks and different generation paradigms, including trainable small model and few-shot prompted LLM. We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors.\n\nLimitations\n\nWe discuss the limitations of our framework as follows:\n(1) Although Selfmem greatly improves the generation quality compared with other retrievalaugmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent), it would become a more crucial problem considering the quadratic time complexity of transformer architecture.\n(2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment with transformer-based architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance.\n\nAcknowledgement\n\nThis work was supported by the National Key Research and Development Program of China (No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan Zhao and Rui Yan are the corresponding authors.\n\nReferences\n[1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proc. of EMNLP, 2021.\n[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. CoRR, 2022.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015.\n[4] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue generation model with discrete latent variable. In Proc. of ACL, 2020.\n[5] Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum learning. In Proc. of ACL Findings, 2021.\n[6] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-response: Dialogue generation guided by retrieval memory. In Proc. of NAACL, 2019 .\n[7] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided dialogue response generation via a matching-to-generation framework. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We combine the primal and dual problems together and propose Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model with its own output.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0ff29be8-5075-42a5-ad0c-6ef6c3b0ab23",
        "questions": "What is one of the limitations of the Selfmem framework when dealing with large datasets with long context, such as BigPatent?",
        "answers": "It requires more computational resources with respect to the memory selection process.",
        "context": "7 Conclusion\n\n\nFor the first time, we investigate the fundamental limitation of bounded memory in the current retrieval-augmented literature. We combine the primal and dual problems together and propose Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model with its own output. We conduct comprehensive experiments across various text generation tasks and different generation paradigms, including trainable small model and few-shot prompted LLM. We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors.\n\nLimitations\n\nWe discuss the limitations of our framework as follows:\n(1) Although Selfmem greatly improves the generation quality compared with other retrievalaugmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent), it would become a more crucial problem considering the quadratic time complexity of transformer architecture.\n(2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment with transformer-based architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance.\n\nAcknowledgement\n\nThis work was supported by the National Key Research and Development Program of China (No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan Zhao and Rui Yan are the corresponding authors.\n\nReferences\n[1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proc. of EMNLP, 2021.\n[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. CoRR, 2022.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015.\n[4] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue generation model with discrete latent variable. In Proc. of ACL, 2020.\n[5] Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum learning. In Proc. of ACL Findings, 2021.\n[6] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-response: Dialogue generation guided by retrieval memory. In Proc. of NAACL, 2019 .\n[7] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided dialogue response generation via a matching-to-generation framework. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Although Selfmem greatly improves the generation quality compared with other retrievalaugmented generation models, it requires more computational resources with respect to the memory selection process.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "0fffa6d4-005e-415d-9981-923a2fb2f8f2",
        "questions": "Which grant supported the research work that led to the development of the Selfmem framework?",
        "answers": "National Key Research and Development Program of China (No.2021YFC3340304)",
        "context": "7 Conclusion\n\n\nFor the first time, we investigate the fundamental limitation of bounded memory in the current retrieval-augmented literature. We combine the primal and dual problems together and propose Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model with its own output. We conduct comprehensive experiments across various text generation tasks and different generation paradigms, including trainable small model and few-shot prompted LLM. We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors.\n\nLimitations\n\nWe discuss the limitations of our framework as follows:\n(1) Although Selfmem greatly improves the generation quality compared with other retrievalaugmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent), it would become a more crucial problem considering the quadratic time complexity of transformer architecture.\n(2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment with transformer-based architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance.\n\nAcknowledgement\n\nThis work was supported by the National Key Research and Development Program of China (No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan Zhao and Rui Yan are the corresponding authors.\n\nReferences\n[1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proc. of EMNLP, 2021.\n[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. CoRR, 2022.\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015.\n[4] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue generation model with discrete latent variable. In Proc. of ACL, 2020.\n[5] Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum learning. In Proc. of ACL Findings, 2021.\n[6] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-response: Dialogue generation guided by retrieval memory. In Proc. of NAACL, 2019 .\n[7] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided dialogue response generation via a matching-to-generation framework. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "This work was supported by the National Key Research and Development Program of China (No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant No.62122089).",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "100aef24-0dd1-4beb-9d98-abc9eae9c546",
        "questions": "Which conference did Yanran Li and colleagues present their work on the Dailydialog dataset?",
        "answers": "Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017",
        "context": "[44] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 December 1, 2017 - Volume 1: Long Papers, 2017.\n[45] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not flat: Modeling the dynamic information flow across dialogue utterances. In Proc. of ACL, 2021 .\n[46] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable dialogue simulation with in-context learning. In Proc. of EMNLP Findings, 2022.\n[47] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, 2004.\n[48] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In Proc. of EMNLP, 2022.\n[49] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 2022.\n[50] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally training the log-linear model for SMT. In Proc. of EMNLP, 2012.\n[51] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proc. of $E M N L P, 2019$.\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. $\\operatorname{CoRR}, 2019$.\n[53] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Proc. of NAACL, 2021.\n[54] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstractive summarization. In Proc. of $A C L, 2021$.\n[55] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to abstractive summarization. In Proc. of $A C L, 2022$.\n[56] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A comparable study on model averaging, ensembling and reranking in NMT. In Proc. of NLPCC, 2018 .\n[57] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In Proc. of CVPR, 2022.\n[58] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning confidence for transformer-based neural machine translation. In Proc. of ACL, 2022.\n[59] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? In Proc. of EMNLP, 2020.\n[60] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proc. of EMNLP, 2018.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 December 1, 2017 - Volume 1: Long Papers, 2017.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "100dcfdb-c413-48e8-b19c-7b43aae0a764",
        "questions": "In which year did Yuchen Liu and colleagues conduct a study on model averaging, ensembling, and reranking in neural machine translation?",
        "answers": "2018",
        "context": "[44] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 December 1, 2017 - Volume 1: Long Papers, 2017.\n[45] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not flat: Modeling the dynamic information flow across dialogue utterances. In Proc. of ACL, 2021 .\n[46] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable dialogue simulation with in-context learning. In Proc. of EMNLP Findings, 2022.\n[47] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, 2004.\n[48] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In Proc. of EMNLP, 2022.\n[49] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 2022.\n[50] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally training the log-linear model for SMT. In Proc. of EMNLP, 2012.\n[51] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proc. of $E M N L P, 2019$.\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. $\\operatorname{CoRR}, 2019$.\n[53] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Proc. of NAACL, 2021.\n[54] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstractive summarization. In Proc. of $A C L, 2021$.\n[55] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to abstractive summarization. In Proc. of $A C L, 2022$.\n[56] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A comparable study on model averaging, ensembling and reranking in NMT. In Proc. of NLPCC, 2018 .\n[57] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In Proc. of CVPR, 2022.\n[58] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning confidence for transformer-based neural machine translation. In Proc. of ACL, 2022.\n[59] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? In Proc. of EMNLP, 2020.\n[60] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proc. of EMNLP, 2018.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A comparable study on model averaging, ensembling and reranking in NMT. In Proc. of NLPCC, 2018 .",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "100de114-0d19-48c8-861a-ac84883f0c01",
        "questions": "Did Alexander Long and his team present their work on retrieval augmented classification for long-tail visual recognition at CVPR in 2022?",
        "answers": "Yes",
        "context": "[44] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 December 1, 2017 - Volume 1: Long Papers, 2017.\n[45] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not flat: Modeling the dynamic information flow across dialogue utterances. In Proc. of ACL, 2021 .\n[46] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable dialogue simulation with in-context learning. In Proc. of EMNLP Findings, 2022.\n[47] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, 2004.\n[48] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In Proc. of EMNLP, 2022.\n[49] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 2022.\n[50] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally training the log-linear model for SMT. In Proc. of EMNLP, 2012.\n[51] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proc. of $E M N L P, 2019$.\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. $\\operatorname{CoRR}, 2019$.\n[53] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Proc. of NAACL, 2021.\n[54] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstractive summarization. In Proc. of $A C L, 2021$.\n[55] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to abstractive summarization. In Proc. of $A C L, 2022$.\n[56] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A comparable study on model averaging, ensembling and reranking in NMT. In Proc. of NLPCC, 2018 .\n[57] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In Proc. of CVPR, 2022.\n[58] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning confidence for transformer-based neural machine translation. In Proc. of ACL, 2022.\n[59] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? In Proc. of EMNLP, 2020.\n[60] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proc. of EMNLP, 2018.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In Proc. of CVPR, 2022.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1016e2d7-00ee-4c67-aa72-dceff9f2f4ed",
        "questions": "What algorithm is used for retrieval purposes in the implementation details of the Selfmem Framework?",
        "answers": "BM25",
        "context": "```\nAlgorithm 1 Selfmem Framework\nRequire: a dataset $\\mathbb{D}$, a retriever $R$, a memory selection metric $\\Delta(\\cdot, \\cdot)$, a retrieval-augmented\n    generator $G_{\\xi}$, and a memory selector $S_{\\theta}$\n    retrieve memory $\\mathbb{M}$ in $\\mathbb{D}$ with $R$\n    train $G_{\\xi}$ with $\\mathbb{D}$ and $\\mathbb{M}$ (if not LLM)\n    use $G_{\\xi}$ to generate candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    train $S_{\\theta}$ on $\\mathbb{C}$ with $\\Delta(\\cdot, \\cdot)$\n    while not converged in the validation set do\n        $S_{\\theta}$ selects memory from $\\mathbb{C}$ as $\\mathbb{M}$\n        $G_{\\xi}$ generates candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    end while\n    $G_{\\xi}$ generates the final hypothesis with $\\mathbb{M}$ in hypothesis mode\n```\nlegislative text of European Union Law. It is the benchmark dataset used in translation memoryaugmented NMT task [28, 92, 8, 17]. We choose 4 translation directions, namely, Spanish $\\leftrightarrow$ English (Es $\\leftrightarrow$ En), German $\\leftrightarrow$ English (De $\\leftrightarrow$ En). Summarization. We evaluate on 2 summarization datasets: 1) XSum [60], extreme summarization, a single-document summarization dataset with highly abstractive articles from British Broadcasting Corporation. 2) BigPatent [73], consisting of 1.3 million records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We experiment on DailyDialog [44], which contains multi-turn dialogs on daily life topics and is used by $[13,4,103]$. The detailed statistics for these datasets can be found in the Appendix A.\n\n\n4.2 Implementation Details\n\n\nWe utilize the BM25 algorithm [70] for retrieval purposes. For all tasks, the candidate generation method consists of beam search with a beam width of 50 . The number of iterations is determined by the performance on the validation set. For translation, we follow the approach of [93, 8, 17], employing a randomly initialized Transformer ${ }_{\\text {base }}$ architecture as $G_{\\xi}$ for trainable small model and XGLM [48] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++ obtained from SacreBLEU[66]. The memory selector $S_{\\theta}$ utilizes an XLM-R ${ }_{\\text {base }}$ [22] as backbone, with BLEU serving as $\\Delta(\\cdot, \\cdot)$. For summarization, we initialize $G_{\\xi}$ with BART $_{\\text {base }}$ [40] for BigPatent and employ BRIO [55] for XSum. The evaluation metric comprises ROUGE (R1/2/L) [47]. For dialogue generation, BART ${ }_{\\text {base }}$ serves as the backbone for $G_{\\xi}$. Our dialogue system is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [43]. For both dialogue and summariza-\n The linear combination of B- $1 / 2$ is chosen as $\\Delta(\\cdot, \\cdot)$ for Dialogue Generation, while R-1/2/L is used for Summarization, following [76]. For further implementation details, please refer to the Appendix B and Appendix C for evaluation metrics.\n\n5 Experimental Results\n\n5.1 Machine Translation\n\nWe select four translation directions and experiment with two generation paradigms: trainable small models and few-shot prompted LLMs [85, 20]. For trainable models, we explore two architectures (joint and dual, as detailed in \u00a73.2). The baselines comprise two types of translation systems: one being the vanilla sequence-to-sequence model $[3,84]$ without memory augmentation, and the other consisting of retrieval-augmented translation models focusing on memory encoding [28, 92], memory construction [101], memory retrieval [8], and memory diversity [17]. Based on the experimental results ${ }^{2}$ shown in Table 2, Selfmem significantly enhances the performance of $G_{\\xi}$ across four translation datasets and two different architectures. This is noteworthy, given that the parameters of the $G_{\\xi}$ remain fixed, with the only variable being the input memory. This finding is consistent with the primal problem which posits that improved memory typically leads to better generation results.\n\n\\footnotetext{\n${ }^{2}$ As higher BLEU scores in this range do not necessarily guarantee a superior translation system [9], we also evaluate our system using TER and chrF++. The results can be found in the Appendix D.\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We utilize the BM25 algorithm [70] for retrieval purposes.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "101fbccc-ded7-43e8-b421-eaeb534097ca",
        "questions": "How many translation directions are chosen for the translation memory-augmented NMT task in the Selfmem Framework?",
        "answers": "4",
        "context": "```\nAlgorithm 1 Selfmem Framework\nRequire: a dataset $\\mathbb{D}$, a retriever $R$, a memory selection metric $\\Delta(\\cdot, \\cdot)$, a retrieval-augmented\n    generator $G_{\\xi}$, and a memory selector $S_{\\theta}$\n    retrieve memory $\\mathbb{M}$ in $\\mathbb{D}$ with $R$\n    train $G_{\\xi}$ with $\\mathbb{D}$ and $\\mathbb{M}$ (if not LLM)\n    use $G_{\\xi}$ to generate candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    train $S_{\\theta}$ on $\\mathbb{C}$ with $\\Delta(\\cdot, \\cdot)$\n    while not converged in the validation set do\n        $S_{\\theta}$ selects memory from $\\mathbb{C}$ as $\\mathbb{M}$\n        $G_{\\xi}$ generates candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    end while\n    $G_{\\xi}$ generates the final hypothesis with $\\mathbb{M}$ in hypothesis mode\n```\nlegislative text of European Union Law. It is the benchmark dataset used in translation memoryaugmented NMT task [28, 92, 8, 17]. We choose 4 translation directions, namely, Spanish $\\leftrightarrow$ English (Es $\\leftrightarrow$ En), German $\\leftrightarrow$ English (De $\\leftrightarrow$ En). Summarization. We evaluate on 2 summarization datasets: 1) XSum [60], extreme summarization, a single-document summarization dataset with highly abstractive articles from British Broadcasting Corporation. 2) BigPatent [73], consisting of 1.3 million records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We experiment on DailyDialog [44], which contains multi-turn dialogs on daily life topics and is used by $[13,4,103]$. The detailed statistics for these datasets can be found in the Appendix A.\n\n\n4.2 Implementation Details\n\n\nWe utilize the BM25 algorithm [70] for retrieval purposes. For all tasks, the candidate generation method consists of beam search with a beam width of 50 . The number of iterations is determined by the performance on the validation set. For translation, we follow the approach of [93, 8, 17], employing a randomly initialized Transformer ${ }_{\\text {base }}$ architecture as $G_{\\xi}$ for trainable small model and XGLM [48] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++ obtained from SacreBLEU[66]. The memory selector $S_{\\theta}$ utilizes an XLM-R ${ }_{\\text {base }}$ [22] as backbone, with BLEU serving as $\\Delta(\\cdot, \\cdot)$. For summarization, we initialize $G_{\\xi}$ with BART $_{\\text {base }}$ [40] for BigPatent and employ BRIO [55] for XSum. The evaluation metric comprises ROUGE (R1/2/L) [47]. For dialogue generation, BART ${ }_{\\text {base }}$ serves as the backbone for $G_{\\xi}$. Our dialogue system is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [43]. For both dialogue and summariza-\n The linear combination of B- $1 / 2$ is chosen as $\\Delta(\\cdot, \\cdot)$ for Dialogue Generation, while R-1/2/L is used for Summarization, following [76]. For further implementation details, please refer to the Appendix B and Appendix C for evaluation metrics.\n\n5 Experimental Results\n\n5.1 Machine Translation\n\nWe select four translation directions and experiment with two generation paradigms: trainable small models and few-shot prompted LLMs [85, 20]. For trainable models, we explore two architectures (joint and dual, as detailed in \u00a73.2). The baselines comprise two types of translation systems: one being the vanilla sequence-to-sequence model $[3,84]$ without memory augmentation, and the other consisting of retrieval-augmented translation models focusing on memory encoding [28, 92], memory construction [101], memory retrieval [8], and memory diversity [17]. Based on the experimental results ${ }^{2}$ shown in Table 2, Selfmem significantly enhances the performance of $G_{\\xi}$ across four translation datasets and two different architectures. This is noteworthy, given that the parameters of the $G_{\\xi}$ remain fixed, with the only variable being the input memory. This finding is consistent with the primal problem which posits that improved memory typically leads to better generation results.\n\n\\footnotetext{\n${ }^{2}$ As higher BLEU scores in this range do not necessarily guarantee a superior translation system [9], we also evaluate our system using TER and chrF++. The results can be found in the Appendix D.\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We choose 4 translation directions, namely, Spanish $\\leftrightarrow$ English (Es $\\leftrightarrow$ En), German $\\leftrightarrow$ English (De $\\leftrightarrow$ En).",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1022f370-1d74-4571-9251-1980069c2773",
        "questions": "Does the Selfmem Framework use a memory selector with an XLM-R base as its backbone?",
        "answers": "Yes",
        "context": "```\nAlgorithm 1 Selfmem Framework\nRequire: a dataset $\\mathbb{D}$, a retriever $R$, a memory selection metric $\\Delta(\\cdot, \\cdot)$, a retrieval-augmented\n    generator $G_{\\xi}$, and a memory selector $S_{\\theta}$\n    retrieve memory $\\mathbb{M}$ in $\\mathbb{D}$ with $R$\n    train $G_{\\xi}$ with $\\mathbb{D}$ and $\\mathbb{M}$ (if not LLM)\n    use $G_{\\xi}$ to generate candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    train $S_{\\theta}$ on $\\mathbb{C}$ with $\\Delta(\\cdot, \\cdot)$\n    while not converged in the validation set do\n        $S_{\\theta}$ selects memory from $\\mathbb{C}$ as $\\mathbb{M}$\n        $G_{\\xi}$ generates candidate pool $\\mathbb{C}$ with $\\mathbb{M}$ in candidate mode\n    end while\n    $G_{\\xi}$ generates the final hypothesis with $\\mathbb{M}$ in hypothesis mode\n```\nlegislative text of European Union Law. It is the benchmark dataset used in translation memoryaugmented NMT task [28, 92, 8, 17]. We choose 4 translation directions, namely, Spanish $\\leftrightarrow$ English (Es $\\leftrightarrow$ En), German $\\leftrightarrow$ English (De $\\leftrightarrow$ En). Summarization. We evaluate on 2 summarization datasets: 1) XSum [60], extreme summarization, a single-document summarization dataset with highly abstractive articles from British Broadcasting Corporation. 2) BigPatent [73], consisting of 1.3 million records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We experiment on DailyDialog [44], which contains multi-turn dialogs on daily life topics and is used by $[13,4,103]$. The detailed statistics for these datasets can be found in the Appendix A.\n\n\n4.2 Implementation Details\n\n\nWe utilize the BM25 algorithm [70] for retrieval purposes. For all tasks, the candidate generation method consists of beam search with a beam width of 50 . The number of iterations is determined by the performance on the validation set. For translation, we follow the approach of [93, 8, 17], employing a randomly initialized Transformer ${ }_{\\text {base }}$ architecture as $G_{\\xi}$ for trainable small model and XGLM [48] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++ obtained from SacreBLEU[66]. The memory selector $S_{\\theta}$ utilizes an XLM-R ${ }_{\\text {base }}$ [22] as backbone, with BLEU serving as $\\Delta(\\cdot, \\cdot)$. For summarization, we initialize $G_{\\xi}$ with BART $_{\\text {base }}$ [40] for BigPatent and employ BRIO [55] for XSum. The evaluation metric comprises ROUGE (R1/2/L) [47]. For dialogue generation, BART ${ }_{\\text {base }}$ serves as the backbone for $G_{\\xi}$. Our dialogue system is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [43]. For both dialogue and summariza-\n The linear combination of B- $1 / 2$ is chosen as $\\Delta(\\cdot, \\cdot)$ for Dialogue Generation, while R-1/2/L is used for Summarization, following [76]. For further implementation details, please refer to the Appendix B and Appendix C for evaluation metrics.\n\n5 Experimental Results\n\n5.1 Machine Translation\n\nWe select four translation directions and experiment with two generation paradigms: trainable small models and few-shot prompted LLMs [85, 20]. For trainable models, we explore two architectures (joint and dual, as detailed in \u00a73.2). The baselines comprise two types of translation systems: one being the vanilla sequence-to-sequence model $[3,84]$ without memory augmentation, and the other consisting of retrieval-augmented translation models focusing on memory encoding [28, 92], memory construction [101], memory retrieval [8], and memory diversity [17]. Based on the experimental results ${ }^{2}$ shown in Table 2, Selfmem significantly enhances the performance of $G_{\\xi}$ across four translation datasets and two different architectures. This is noteworthy, given that the parameters of the $G_{\\xi}$ remain fixed, with the only variable being the input memory. This finding is consistent with the primal problem which posits that improved memory typically leads to better generation results.\n\n\\footnotetext{\n${ }^{2}$ As higher BLEU scores in this range do not necessarily guarantee a superior translation system [9], we also evaluate our system using TER and chrF++. The results can be found in the Appendix D.\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "The memory selector $S_{\theta}$ utilizes an XLM-R ${ }_{\text {base }}$ [22] as backbone, with BLEU serving as $\\Delta(\\cdot, \\cdot)$.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1032223d-590e-41e1-a2a3-8a1062a68b1f",
        "questions": "What is the BLEU score achieved by the Transformer model with Self memory on the JRC-Acquis English to German translation task?",
        "answers": "60.11",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Transformer ${ }_{\text {joint }}$ & Self & 60.11 & 73.25 & 32.62",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1032fb80-f62c-4c39-accd-4523e11fe602",
        "questions": "Which system achieved the highest R-2 score in the XSum benchmark according to Table 10?",
        "answers": "Selfmem",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10339310-2b7b-4e99-878f-40ed2c805ac7",
        "questions": "Compare the TER scores for the Transformer with dual and joint memory with retrieval on the JRC-Acquis English to German translation task. Which configuration has the lower TER score?",
        "answers": "Transformer ${ }_{\text {joint }}$ with retrieval",
        "context": "Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package ${ }^{4}$ for evaluation. Following [55], we use PTB tokenizer ${ }^{5}$ for tokenization. And the parameters for ROUGE are \"-c 95 -r 1000 -n 2 -m\".\n\nDialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU ${ }^{6}$ with space as tokenizer and smoothing method1. The Distinction score is from [42].\n\n\nD More results on translation tasks\n\n\nTable 9: Evaluation results on JRC-Acquis En $\\rightarrow$ De measured by BLEU, TER and chrF++.\n\\begin{tabular}{|c|c|c|c|c|}\n  System & Memory & BLEU $\\uparrow$ & chrF++ $\\uparrow$ & TER $\\downarrow$ \\\\\n  Transformer & None & 55.43 & 70.31 & 36.35 \\\\\n  Transformer ${ }_{\\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39 \\\\\n  Transformer $_{\\text {dual }}$ & Self & 59.49 & 72.62 & 34.04 \\\\\n  Transformer ${ }_{\\text {joint }}$ & Self & 60.11 & 73.25 & 32.62 \\\\\n \n\\end{tabular}\n\nE More Summarization Baselines\n\nIn this Table 10, we include more baselines on the benchmark dataset XSum and BigPatent. We also report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.\n\nTable 10: More baselines on XSum and BigPatent.\n\\begin{tabular}{llll}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{l}{ XSum } \\\\\n $[51]$ & 38.8 & 16.5 & 31.3 \\\\\n{$[40]$} & 45.1 & 22.3 & 37.3 \\\\\n{$[100]$} & 47.2 & 24.6 & 39.3 \\\\\n{$[54]$} & 47.6 & 24.6 & 39.4 \\\\\n{$[55]$} & 49.1 & 25.6 & 40.4 \\\\\n{$[87](\\mathrm{PG})$} & 48.2 & 26.0 & 40.2 \\\\\n{$[87](\\mathrm{B})$} & 43.1 & 21.0 & 35.5 \\\\\n[87](L) & 46.5 & 24.1 & 38.6 \\\\\n[68] & 48.1 & 25.0 & 40.0 \\\\\n[69] & 47.1 & 24.1 & 38.8 \\\\\n{$[16]$} & 47.8 & 25.0 & 39.7 \\\\\n  Selfmem & $\\mathbf{5 0 . 3}$ & $\\mathbf{2 6 . 7}$ & $\\mathbf{4 1 . 6}$ \\\\\n \n\\end{tabular}\n\\begin{tabular}{lccc}\n  System & R-1 & R-2 & R-L \\\\\n  \\multicolumn{4}{c}{ BigPatent } \\\\\n $[100]$ & 53.6 & 33.1 & 42.3 \\\\\n{$[40]$} & 44.4 & 21.3 & 31.0 \\\\\n{$[98]$} & 60.6 & 42.5 & 50.0 \\\\\n{$[65]$} & 38.7 & 12.3 & 34.1 \\\\\n{$[90]$} & 45.0 & 20.3 & 39.2 \\\\\n{$[1]$} & 52.3 & 33.5 & 42.8 \\\\\n{$[87](\\mathrm{B})$} & 59.5 & 42.6 & 50.6 \\\\\n{$[87]$ (L) } & 60.7 & 43.3 & 51.3 \\\\\n{$[87](\\mathrm{PG})$} & 44.6 & 21.5 & 33.3 \\\\\n  Selfmem & $\\mathbf{6 2 . 9}$ & $\\mathbf{4 8 . 1}$ & $\\mathbf{5 9 . 6}$ \\\\\n \n\\end{tabular}\n\nF Empirical analysis of latency\n\nIn Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare Selfmem with a retrieval-augmented baseline model across various datasets and computational platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one AMD EPYC 7V13 64-Core Processor.\n\n\\footnotetext{\n${ }^{4}$ https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5\n${ }^{5}$ https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/ PTBTokenizer.html\n${ }^{6}$ https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Transformer ${ }_{\text {dual }}$ & Retrieval & 58.06 & 71.58 & 35.41 \n Transformer ${ }_{\text {joint }}$ & Retrieval & 58.58 & 72.22 & 34.39",
        "evidence_page_no": 17,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10375004-14c8-414e-86e3-ac57ff1cfc80",
        "questions": "What is the ROUGE-1 score for the BART joint system on the BigPatent dataset?",
        "answers": "62.9",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "BART $_{\text {joint }}$ & 62.9 & $0.62664-0.63080$",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "103eaba1-de75-4cb9-b6c4-e48f436bd6b0",
        "questions": "Which dataset has the highest average output length in the generation latency analysis?",
        "answers": "BigPatent",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& 44 & 75 & 127 & 16",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1046e73e-9da2-44fc-8b2a-5500f54d5acc",
        "questions": "Using CUDA, what is the total time taken by Selfmem for all three steps (Candidate Generation, Memory Selection, and Hypothesis Generation) on the XSum dataset?",
        "answers": "1.45",
        "context": "Table 11: Confidence region for SOTA model in XSum and BigPatent.\n\\begin{tabular}{lcc}\n  System & ROUGE-1/2/L & 95\\% -conf.int \\\\\n  \\multicolumn{3}{c}{ XSum } \\\\\n  BRIO $_{\\text {joint }}$ & 50.3 & $0.49986-0.50602$ \\\\\n& 26.7 & $0.26300-0.26989$ \\\\\n& 41.6 & $0.41231-0.41900$ \\\\\n  \\multicolumn{3}{c}{ BigPatent } \\\\\nBART $_{\\text {joint }}$ & 62.9 & $0.62664-0.63080$ \\\\\n& 48.1 & $0.47783-0.48333$ \\\\\n& 59.6 & $0.59401-0.59847$ \\\\\n \n\\end{tabular}\n\nTable 12: Generation Latency analysis.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  \\multicolumn{2}{|l|}{} & NMT & XSum & BigPatent & DailyDialog \\\\\n  \\multicolumn{2}{|r|}{\\multirow[t]{2}{*}{Average Input Length Average Output Length}} & 87 & 512 & 1024 & 71 \\\\\n  & & 44 & 75 & 127 & 16 \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CPU & & & \\\\\n  & & 0.97 & 1.79 & 3.16 & 0.32 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 3.20 & 7.50 & 15.00 & 1.02 \\\\\n  & Memory Selection & 0.50 & 0.52 & 0.95 & 0.14 \\\\\n  & Hypothesis Generation & 0.97 & 1.79 & 3.00 & 0.32 \\\\\n  & & $\\times 4.80$ & $\\times 5.47$ & $\\times 6.04$ & $\\times 4.63$ \\\\\n  \\multicolumn{2}{|l|}{\\multirow[b]{2}{*}{Retrieval-augmented Baseline}} & CUDA & & & \\\\\n  & & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  \\multirow{4}{*}{Selfmem} & Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\\\\n  & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\\\\n  & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10 \\\\\n  & & $\\times 2.76$ & $\\times 2.99$ & $\\times 3.35$ & $\\times 2.91$ \\\\\n \n\\end{tabular}",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Candidate Generation & 0.51 & 1.00 & 1.72 & 0.18 \\ & Memory Selection & 0.01 & 0.01 & 0.01 & 0.01 \\ & Hypothesis Generation & 0.29 & 0.44 & 0.75 & 0.10",
        "evidence_page_no": 18,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1047bcba-678c-40c3-b0fc-f3a02a143784",
        "questions": "What is the ROUGE-2 score for PEGASUS on the XSum dataset when no memory is used?",
        "answers": "24.6",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "PEGASUS & None & 47.2 & 24.6 & 39.3",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "104dff66-23f1-467e-81d3-4690a795ac54",
        "questions": "Which system achieved the highest ROUGE-1 score on the BigPatent dataset when using self-memory?",
        "answers": "62.9",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Self & 62.9 & 48.1 & 59.6",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "106861d3-633d-4c54-b304-59cc30c42d61",
        "questions": "How much does REINA (B) improve BRIO's ROUGE-L score on the XSum dataset when both use retrieval memory?",
        "answers": "0.6",
        "context": "size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the primal problem, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.\n\n\n5.2 Summarization\n\n\nIn this paper, we compare the performance of our trainable model with those of REINA [87], PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the primal problem. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO ( +1.2 R 1 ) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.\n\nTable 5: Results of summarization task on XSum and BigPatent measured by ROUGE.\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}\n  System & Memory & R-1 & R-2 & R-L & System & Memory & R-1 & R-2 & R-L \\\\\n  \\multicolumn{5}{|c|}{XSum} & \\multicolumn{5}{|c|}{BigPatent} \\\\\n  PEGASUS & None & 47.2 & 24.6 & 39.3 & PEGASUS & None & 53.6 & 33.2 & 43.2 \\\\\n  BRIO & None & 49.1 & 25.6 & 40.4 & BART & None & 44.4 & 21.3 & 31.0 \\\\\n  REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 & REINA (B) & Retrieval & 59.5 & 42.6 & 50.6 \\\\\n  REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 & REINA (L) & Retrieval & 60.7 & 43.3 & 51.3 \\\\\n  REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 & REINA (PG) & Retrieval & 44.6 & 21.5 & 33.3 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Retrieval & 49.5 & 26.5 & 41.2 & $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 59.6 & 43.4 & 51.0 \\\\\n  $\\mathrm{BRIO}_{\\text {dual }}$ \\ & Self & 49.2 & 26.2 & 40.8 & $\\mathrm{BART}_{\\text {dual }}$ * & Self & 61.2 & 44.6 & 52.3 \\\\\n  $\\mathrm{BRIO}_{\\text {joint }} \\dagger$ & Self & 50.3 & 26.7 & 41.6 &  & Self & 62.9 & 48.1 & 59.6 \\\\\n \n\\end{tabular}\n\n5.3 Dialogue Generation\n\nAs demonstrated in Table 6, the self-memory significantly enhances the performance of the retrievalaugmented generator for dialogue generation tasks. By optimizing memory using BLEU as $\\Delta(\\cdot, \\cdot)$, the self-memory improves the B-1,2 score over retrieved memory by $3.08 \\mathrm{~B}-1$ and $0.6 \\mathrm{~B}-2$ on $\\mathrm{BART}_{\\text {joint }}$. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as $\\Delta(\\cdot, \\cdot)$ when optimizing $S_{\\theta}$, denoted as $\\mathrm{BART}_{\\text {joint }} \\dagger$ (D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n\n6 Further Analysis\n\nTo gain a deeper insight into Selfmem, we first examine the impact of each key component, namely $G_{\\xi}$ and $S_{\\theta}$. Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En $\\rightarrow$ De dataset. We also include latency analysis and human evaluation on Appendix F and G.\n\nTuning $S_{\\theta}$ We explored various $S_{\\theta}$ by direct selection from the candidate pool based on gold rankings. As shown in Figure 3a, both architectures with enhanced $S_{\\theta}$ significantly outperform the current SOTA performance ( 60.11 BLEU ). Moreover, we assessed the candidate pool quality during this iterative process using an oracle $S_{\\theta}$, as displayed in Figure 3b. A clear pattern emerges",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "$\\mathrm{BRIO}_{\\text {dual }}$ * & Retrieval & 48.6 & 26.1 & 40.6 & $B A R T_{\\text {dual }}$ * & Retrieval & 57.4 & 43.3 & 49.7",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "106cb112-d7a7-40e8-bb38-d6d08870582a",
        "questions": "In which year was the paper by Eugene Charniak and Mark Johnson on coarse-to-fine n-best parsing and maxent discriminative reranking published?",
        "answers": "2005",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10710096-cf60-4ea3-9d37-a7eb07323bec",
        "questions": "How many authors contributed to the paper 'Neural machine translation with contrastive translation memories' presented at EMNLP 2022?",
        "answers": "5",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1088a81b-85b1-4cd4-aea0-026b9cbbb38e",
        "questions": "List all the authors of the 2017 paper 'Reading Wikipedia to answer open-domain questions' presented at the 55th Annual Meeting of the Association for Computational Linguistics.",
        "answers": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes",
        "context": "[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021.\n[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006.\n[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018.\n[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of $A C L, 2005$.\n[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.\n[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR, 2022.\n[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao, and Xiangliang Zhang. A topic-aware summarization framework with different modal side information. Proc. of SIGIR, 2023.\n[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness in abstractive summarization. In Proc. of NeurIPS, 2022.\n[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. In Proc. of EMNLP, 2022.\n[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan Zhao, and Rui Yan. Towards personalized review summarization by modeling historical reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.\n[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from paramters for plug-and-play language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 14288-14308, Toronto, Canada, July 2023. Association for Computational Linguistics.\n[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale: Synergized collaboration of asymmetric language translation engines, 2023.\n[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput. Linguistics, 2005.\n[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.\n[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc' Aurelio Ranzato. Residual energy-based models for text generation. In Proc. of ICLR, 2020.\n[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings, 2022 .\n[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable models for text generation. In Proc. of EMNLP, 2019.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879, 2017.",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "108956c0-6a54-4593-b68f-80528f38b308",
        "questions": "Which system achieved the highest B-1 score with Retrieval memory in the dialogue generation task on DailyDialog?",
        "answers": "$B BRT_{\text{joint }} \\dagger$",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "$B BRT_{\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "108e0777-95b7-4925-a86c-deaf9dc701aa",
        "questions": "Compare the D-1 and D-2 scores of DialoFlow and $\\mathrm{BART}_{\text{joint }}(\\mathrm{D})$. Which system has a higher average diversity score?",
        "answers": "$\\mathrm{BART}_{\text{joint }}(\\mathrm{D})$",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12...\n$\\mathrm{BART}_{\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "1094625b-f102-40d0-859c-5fe3ad031c4d",
        "questions": "What is the memory type and B-2 score of the system that has the highest D-1 score?",
        "answers": "Memory type: Self, B-2 score: 32.09",
        "context": "Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. $\\mathrm{BART}_{\\text {joint }}(\\mathrm{D})$ denotes the metric $\\Delta(\\cdot, \\cdot)$ for $S_{\\theta}$ is the average of D-1 and D-2.\n\\begin{tabular}{|c|c|c|c|c|c|}\n  System & Memory & B-1 & B-2 & D-1 & D-2 \\\\\n  NCM [86] & None & 33.60 & 26.80 & 3.00 & 12.80 \\\\\n  iVAE [25] & None & 30.90 & 24.90 & 2.90 & 25.00 \\\\\n  PLATO-2 [5] & None & 34.80 & 25.12 & 3.54 & 25.11 \\\\\n  DialoFlow [45] & None & 36.17 & 27.67 & 4.56 & 27.12 \\\\\n  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\\\\n   & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\\\\n  $B$ BRT $_{\\text {joint }} \\dagger$ & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\\\\n  $\\mathrm{BART}_{\\text {dual }}$ \\ & Self & 33.43 & 22.85 & 4.66 & 26.16 \\\\\n  $B A R T_{\\text {joint }} \\dagger$ & Self & 39.80 & 32.15 & 5.84 & 32.16 \\\\\n  BART $_{\\text {joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05 \\\\\n \n\\end{tabular}\n\n\nFigure 3: (a) shows generation quality in the iteration process with different $S_{\\theta}$ in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle $S_{\\theta}$.\nin this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.\n\nTuning $G_{\\xi} \\quad$ As discussed in $\\S 3.1$, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between \"good\" and \"bad\" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the $G_{\\xi}$ is not the current bottleneck of the Selfmem.\n\nFrequency Analysis We conduct a comprehensive tokenlevel analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted\n\n\nFigure 4: 1-gram F1 score sorted by training corpus frequency.\nin Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrievalaugmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67, 57].",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "$\\mathrm{BART}_{\text{joint }} \\dagger$ (D) & Self & 36.92 & 32.09 & 9.12 & 37.05",
        "evidence_page_no": 8,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "109d5c8d-4562-4aff-96b2-64357bb8be94",
        "questions": "What is the BLEU score for the Transformer model using self-memory on the En-Es test set?",
        "answers": "64.67",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Transformer $_{\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49*",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10a74c94-af37-48b6-b981-069c0886aa83",
        "questions": "How does the BLEU score of XGLM-7.5B compare with kNN and self-memory methods on the En-De translation task in terms of improvement?",
        "answers": "Self-memory improves to 48.32 compared to 47.82 with kNN.",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "En-De & $\rightarrow$ & 18.48 & 47.82 & 48.32",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10ab5ce4-f9a9-449a-8958-45edd65e5de6",
        "questions": "For the comparison between retrieval memory and self-memory, what is the BLEU score difference when translating from English to Spanish using hypothesis in both methods?",
        "answers": "The BLEU score is 65.94 for self-memory hypothesis and 64.12 for retrieval memory hypothesis.",
        "context": "Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol ( $\\star$ and $\\dagger$ ) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. * denotes the system is significantly better than baselines with $p$-value $<0.05$ tested by [37].\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n  \\multirow[t]{2}{*}{System} & \\multicolumn{2}{|c|}{Es $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ Es} & \\multicolumn{2}{|r|}{De $\\rightarrow$ En} & \\multicolumn{2}{|r|}{En $\\rightarrow$ De} \\\\\n  & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\\\\n  \\multicolumn{9}{|c|}{None Memory} \\\\\n  RNNsearch [3] & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\\\\n  Transformer [84] & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\\\\n  \\multicolumn{9}{|c|}{Retrieval Memory} \\\\\n  SEG-NMT [28] & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\\\\n  NMT-pieces [101] & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\\\\n  G-TFM [92] & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\\\\n  MonoNMT [8] & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\\\\n  CMM [17] & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\\\\n  Transformer $_{\\text {dual }} \\star$ & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\\\\n  Transformer $_{\\text {uni }} \\dagger$ & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\\\\n  \\multicolumn{9}{|c|}{Self-Memory} \\\\\n  Transformer $_{\\text {dual }} \\star$ & $68.6{ }^{*}$ & 69.20* & 64.12* & 64.67* & $65.06^{*}$ & 64.98* & $59.26^{*}$ & 59.49* \\\\\n  Transformer uni ${ }^{\\text {t }}$ & $68.26^{*}$ & $68.80^{*}$ & 66.07* & $65.94^{*}$ & 65.32* & $6^{6.65} *$ & 59.88* & $60.11^{*}$ \\\\\n \n\\end{tabular}\n\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer ${ }_{\\text {joint }}$ on JRC-Acquis.\n\\begin{tabular}{cccccc}\n  & & \\multicolumn{2}{c}{ Retrieval } & \\multicolumn{2}{c}{ Self } \\\\\n\\cline { 3 - 6 } & & memory & hypothesis & memory & hypothesis \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 38.89 & 58.58 & 57.92 & 60.11 \\\\\n& $\\leftarrow$ & 42.56 & 64.40 & 64.32 & 65.65 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94 \\\\\n& $\\leftarrow$ & 43.05 & 67.32 & 67.78 & 68.80 \\\\\n \n\\end{tabular}\n\nThe dual problem is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works [39,68]. Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.\n\nTable 4: Evaluation results of in-context learning with self-memory.\n\\begin{tabular}{ccccccccccc}\n  & & \\multicolumn{3}{c}{ XGLM-1.7B } & \\multicolumn{3}{c}{ XGLM-4.5B } & \\multicolumn{2}{c}{ XGLM-7.5B } \\\\\n\\cline { 3 - 11 } & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\\\\n  \\multirow{2}{*}{ En-De } & $\\rightarrow$ & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\\\\n& $\\leftarrow$ & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\\\\n  \\multirow{2}{*}{ En-Es } & $\\rightarrow$ & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\\\\n& $\\leftarrow$ & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\\\\n \n\\end{tabular}\n\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in [48]. We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From the table, we first observe a general trend where few-shot translation performance improves as the",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "& $\rightarrow$ & 40.67 & 64.12 & 63.57 & 65.94",
        "evidence_page_no": 6,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10b94515-0727-41ba-a43c-f05a62e973d6",
        "questions": "What is the mathematical representation of the probability distribution function in the decoder of $G_{\\xi}$ using the autoregressive manner?",
        "answers": "P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10c23883-2a16-4138-b5cf-b04b087a49f5",
        "questions": "How does the memory selector $S_{\\theta}(x, c)$ assign probabilities to candidates from the candidate pool $\\mathbb{C}$?",
        "answers": "p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2305.02437v3",
        "ID": "10c565d5-5405-4bed-9fb8-61a396d53ecf",
        "questions": "Under what transformation do probabilities $p_{M}(c_{i})$ for candidates $c_{i}$ in the candidate pool $\\mathbb{C}$ smooth out during training?",
        "answers": "p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}",
        "context": "And the decoder would incorporate $H$ by attention mechanism and generate tokens in an autoregressive manner:\n$$h^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}(H), y_{<i}\\right) \\quad P_{G_{\\xi}}\\left(\\cdot \\mid x, y_{<i}\\right)=\\operatorname{Softmax}\\left(h^{i}\\right)$$\n\nDual-Encoder Instead of treating $x$ and $m$ as a long sequence, this architecture has two encoders, one for $x$ and the other for $m$. Their outputs are sequentially attended by the decoder with dual cross attention as in [17]:\n$$\\begin{gathered}\nH_{x}=\\operatorname{SourceEncoder}(x) \\quad H_{m}=\\text { MemoryEncoder }(m) \\\\\nh^{i}=\\operatorname{Decoder}\\left(\\operatorname{Cross} \\operatorname{Attn}\\left(H_{x}, H_{m}\\right), y_{<i}\\right)\n\\end{gathered}$$\n\nWe use Transformer [84] as the building block for both architectures and optimize $G_{\\xi}$ with NLL loss:\n$$\\mathcal{L}_{\\mathrm{nll}}=-\\sum_{t=1}^{|y|} \\log P_{G_{\\xi}}\\left(y_{t} \\mid x, m, y_{<t}\\right)$$\n\n\n3.3 Memory Selector\n\n\nThe role of memory selector $S_{\\theta}(x, c)$, parameterized by $\\theta$, is to select one candidate $c$ from the candidate pool $\\mathbb{C}$ generated by $G_{\\xi}$ based on a specific metric $\\Delta(\\cdot, \\cdot)$. The chosen candidate $c$ is then utilized as memory $m$ for the subsequent generation round of $G_{\\xi}$. As discussed in $\\S 3.1$, using $p_{G_{\\xi}}(y \\mid x)$ as the metric $\\Delta(\\cdot, \\cdot)$ would result in falling into the confidence region of $G_{\\xi}$, leading to no information gain. Moreover, a larger value of $p_{G_{\\xi}}(y \\mid x)$ does not necessarily guarantee improved generation quality [59]. Consequently, we define $\\Delta(\\cdot, \\cdot)$ as model-free metrics that are widely employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT) and ROUGE for Summarization. Our memory selector takes the concatenation of the source $x$ and candidate $c_{i}$ as input, and produces a multinomial distribution $p_{S_{\\theta}}(\\cdot \\mid x)$ over $\\mathbb{C}$.\nIn this paper, we focus on the role of the memory selector, $S_{\\theta}(x, c)$, which is parameterized by $\\theta$. The objective of this selector is to choose a single candidate $c$ from the candidate pool $\\mathbb{C}$, generated by $G_{\\xi}$, based on a specific metric, $\\Delta(\\cdot, \\cdot)$.\n$$p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)=\\frac{\\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{i}\\right)\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(S_{\\theta}\\left(x[\\mathrm{SEP}] c_{j}\\right)\\right)}$$\n\nIn accordance with [39], the training goal for $S_{\\theta}$ is to minimize the discrepancy between the $S_{\\theta}$ 's predictions and the scores determined by $\\Delta(\\cdot, \\cdot)$. This divergence is quantified using the KullbackLeibler (KL) divergence.\n$$\\mathcal{L}_{\\mathrm{kl}}=-\\sum_{i=1}^{|\\mathbb{C}|} p_{M}\\left(c_{i}\\right) \\log p_{S_{\\theta}}\\left(c_{i} \\mid x\\right) \\quad \\text { where } \\quad p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$$\n$\\tau$ is the temperature to control the smoothness of the distribution. At inference, the output of the $S_{\\theta}$ is $\\arg \\max p_{S_{\\theta}}\\left(c_{i} \\mid x\\right)$.\n$$c_{i} \\in \\mathbb{C}$$\n\n3.4 Combine Generator and Selector\n\nWe define two generation modes for $G_{\\xi}$. The first mode, referred to as the hypothesis mode, generates a single output for each input, which is utilized for system evaluation. The second mode, known as the candidate mode, produces N outputs for a given input, and is employed for training $S_{\\theta}$ as well as memory selection. By integrating two modes together, we present the complete framework of our proposed model, Selfmem, as illustrated in Algorithm 1.\n\n4 Experimental Setup\n\n4.1 Dataset\n\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets. Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "$p_{M}\\left(c_{i}\\right)=\\frac{\\exp \\left(\\Delta\\left(c_{i}, y\\right) / \\tau\\right)}{\\sum_{j=1}^{|\\mathbb{C}|} \\exp \\left(\\Delta\\left(c_{j}, y\\right) / \\tau\\right)}$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10c8cc63-2c98-4a95-8122-dbb0d671ac8e",
        "questions": "Who are the authors of the paper titled 'RAGAS: Automated evaluation of retrieval augmented generation' published in September 2023?",
        "answers": "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert",
        "context": "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation of retrieval augmented generation. September 2023a.\n\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023 b.\n\nPhilip Feldman Foulds, R James, and Shimei Pan. Ragged edges: The double-edged sword of retrieval-augmented chatbots. arXiv preprint arXiv:2403.01193, 2024.\n\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\nGemini Team. Gemini: A family of highly capable multimodal models. December 2023.\nRobert Hart. Google restricts ai search tool after \"nonsensical\" answers told people to eat rocks and put glue on pizza, May 2024. URL https://www.forbes.com/sites/roberthart/2024/05/31/ google-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-gl ?sh=64183b617f61.\n\nYasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi. RaLLe: A framework for developing and evaluating Retrieval-Augmented large language models. August 2023.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.\n\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.\n\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, and Others. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst., 33:9459-9474, 2020.\n\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.\n\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-Augmented retrieval for open-domain question answering. September 2020.\n\nE Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. DetectGPT: Zero-shot machine-generated text detection using probability curvature. ICML, pages 24950-24962, January 2023.\n\nAnthony J Nastasi, Katherine R Courtright, Scott D Halpern, and Gary E Weissman. Does ChatGPT provide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across care contexts. March 2023.\n\nOpenAI. GPT-4 technical report. March 2023.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain hallucination test for large language models. July 2023.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated evaluation framework for Retrieval-Augmented generation systems. November 2023a.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023b.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation of retrieval augmented generation. September 2023a.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10d41a34-2ce2-48dc-931c-617d97a922a2",
        "questions": "In which month and year was the paper 'Med-HALT: Medical domain hallucination test for large language models' published?",
        "answers": "July 2023",
        "context": "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation of retrieval augmented generation. September 2023a.\n\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023 b.\n\nPhilip Feldman Foulds, R James, and Shimei Pan. Ragged edges: The double-edged sword of retrieval-augmented chatbots. arXiv preprint arXiv:2403.01193, 2024.\n\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\nGemini Team. Gemini: A family of highly capable multimodal models. December 2023.\nRobert Hart. Google restricts ai search tool after \"nonsensical\" answers told people to eat rocks and put glue on pizza, May 2024. URL https://www.forbes.com/sites/roberthart/2024/05/31/ google-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-gl ?sh=64183b617f61.\n\nYasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi. RaLLe: A framework for developing and evaluating Retrieval-Augmented large language models. August 2023.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.\n\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.\n\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, and Others. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst., 33:9459-9474, 2020.\n\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.\n\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-Augmented retrieval for open-domain question answering. September 2020.\n\nE Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. DetectGPT: Zero-shot machine-generated text detection using probability curvature. ICML, pages 24950-24962, January 2023.\n\nAnthony J Nastasi, Katherine R Courtright, Scott D Halpern, and Gary E Weissman. Does ChatGPT provide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across care contexts. March 2023.\n\nOpenAI. GPT-4 technical report. March 2023.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain hallucination test for large language models. July 2023.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated evaluation framework for Retrieval-Augmented generation systems. November 2023a.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023b.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain hallucination test for large language models. July 2023.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10f0cb05-dcf8-4eb1-a478-33785d531aa7",
        "questions": "Is the paper 'Ever: Mitigating hallucination in large language models through real-time verification and rectification' a preprint on arXiv?",
        "answers": "Yes",
        "context": "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. RAGAS: Automated evaluation of retrieval augmented generation. September 2023a.\n\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023 b.\n\nPhilip Feldman Foulds, R James, and Shimei Pan. Ragged edges: The double-edged sword of retrieval-augmented chatbots. arXiv preprint arXiv:2403.01193, 2024.\n\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\nGemini Team. Gemini: A family of highly capable multimodal models. December 2023.\nRobert Hart. Google restricts ai search tool after \"nonsensical\" answers told people to eat rocks and put glue on pizza, May 2024. URL https://www.forbes.com/sites/roberthart/2024/05/31/ google-restricts-ai-search-tool-after-nonsensical-answers-told-people-to-eat-rocks-and-put-gl ?sh=64183b617f61.\n\nYasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, and Jun Deguchi. RaLLe: A framework for developing and evaluating Retrieval-Augmented large language models. August 2023.\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.\n\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.\n\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, and Others. Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv. Neural Inf. Process. Syst., 33:9459-9474, 2020.\n\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052, 2021.\n\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-Augmented retrieval for open-domain question answering. September 2020.\n\nE Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. DetectGPT: Zero-shot machine-generated text detection using probability curvature. ICML, pages 24950-24962, January 2023.\n\nAnthony J Nastasi, Katherine R Courtright, Scott D Halpern, and Gary E Weissman. Does ChatGPT provide appropriate and equitable medical advice?: A vignette-based, clinical evaluation across care contexts. March 2023.\n\nOpenAI. GPT-4 technical report. March 2023.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Med-HALT: Medical domain hallucination test for large language models. July 2023.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. ARES: An automated evaluation framework for Retrieval-Augmented generation systems. November 2023a.\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: An automated evaluation framework for retrieval-augmented generation systems. arXiv preprint arXiv:2311.09476, 2023b.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Haoqiang Kang, Juntong Ni, and Huaxiu Yao. Ever: Mitigating hallucination in large language models through real-time verification and rectification. arXiv preprint arXiv:2311.09114, 2023.",
        "evidence_page_no": 9,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10f143e2-1abc-418e-8785-b14453158ee1",
        "questions": "Which model has the lowest context bias according to the comparison of six top-performing models?",
        "answers": "Claude Opus",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157.",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10fc9eee-0a17-47f7-aa5f-24b6de6fec4b",
        "questions": "What is the accuracy range for the model named GPT-4o in the comparison of six top-performing models?",
        "answers": "0.594 to 0.633",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10fcbacf-84bf-4051-8aff-0f904cecbd32",
        "questions": "Does the model Llama-3 have a higher or lower prior bias compared to Claude Sonnet in the comparison of six top-performing models?",
        "answers": "Lower",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "table",
        "evidence_context": "Llama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$",
        "evidence_page_no": 10,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10fe6843-9f0f-47a7-a424-24067c637708",
        "questions": "What is the context bias percentage of GPT-4o when presented with incorrect information in retrieved documents according to the ClashEval benchmark dataset?",
        "answers": "over 60%",
        "context": "5 Discussion\n\n\nThe ClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate between their own internal knowledge and contextual information when the two are in conflict.\n\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias, overriding their own correct prior knowledge over $60 \\%$ of the time when presented with incorrect information in the retrieved documents. However, this bias is not absolute - the degree to which the retrieved content deviates from truth negatively correlates with the context preference rate. Interestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such that the same perturbation level affects each model differently. For instance, for a given magnitude of deviation, Claude Opus adheres to incorrect contextual information 30\\% less often than GPT-4o. While GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context bias compared to smaller models like Claude Sonnet. This finding suggests that performance on knowledge-based benchmarks may not automatically mean it is most suitable for RAG settings. Additionally, we find that LLMs are calibrated to selectively defer to external evidence when they are less certain about a given query. However, each model differs in how well-calibrated they are. While strong priors are not inherently problematic, the lack of explicit expectations around how models will decide to use contextual information remains a risk. We propose a simple method for improving models under ClashEval, and hope that future work can improve upon this baseline.\n\nOur analyses have several key limitations. First, RAG systems can be deployed to many more domains than can be covered by our analyses. Second, to make our experiments tractable, our question-generation process is strictly fact-based and does not require multi-step logic, document synthesis, or other higher-level reasoning. Third, our dataset contains an enriched rate of contextual errors, so the reported metrics are not meant to represent bias rates in the wild. Fourth, our proposed token probability method only applies to models which provide probability outputs. Finally, even though this dataset is intended to improve an LLM's ability to provide users with accurate information, bad actors could use such information to exploit the shortcomings of certain models described in this paper.\nAs retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights spur further research into improving the robustness and calibration of such models. Resolving the tension between parametric priors and retrieved information is a crucial challenge on the path to safe and trustworthy language models.\n\nReferences\n\nMuhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy LLMs: Dealing with hallucinations in healthcare AI. September 2023.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in Retrieval-Augmented generation. AAAI, 38(16):17754-17762, March 2024a.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages $17754-17762$, 2024b.\n\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, and Nigam H Shah. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. April 2023.\n\nRyan Daws. Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. https://www.artificialintelligence-news.com/2020/10/28/ medical-chatbot-openai-gpt3-patient-kill-themselves/, October 2020. Accessed: 2024-1-19.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "A key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias, overriding their own correct prior knowledge over $60 \\%$ of the time when presented with incorrect information in the retrieved documents.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "10ff4de8-e6ff-43b5-b36c-ddb8beabdee5",
        "questions": "How does the context adherence of Claude Opus compare to GPT-4o for a given magnitude of deviation in the ClashEval benchmark dataset?",
        "answers": "Claude Opus adheres to incorrect contextual information 30% less often than GPT-4o.",
        "context": "5 Discussion\n\n\nThe ClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate between their own internal knowledge and contextual information when the two are in conflict.\n\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias, overriding their own correct prior knowledge over $60 \\%$ of the time when presented with incorrect information in the retrieved documents. However, this bias is not absolute - the degree to which the retrieved content deviates from truth negatively correlates with the context preference rate. Interestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such that the same perturbation level affects each model differently. For instance, for a given magnitude of deviation, Claude Opus adheres to incorrect contextual information 30\\% less often than GPT-4o. While GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context bias compared to smaller models like Claude Sonnet. This finding suggests that performance on knowledge-based benchmarks may not automatically mean it is most suitable for RAG settings. Additionally, we find that LLMs are calibrated to selectively defer to external evidence when they are less certain about a given query. However, each model differs in how well-calibrated they are. While strong priors are not inherently problematic, the lack of explicit expectations around how models will decide to use contextual information remains a risk. We propose a simple method for improving models under ClashEval, and hope that future work can improve upon this baseline.\n\nOur analyses have several key limitations. First, RAG systems can be deployed to many more domains than can be covered by our analyses. Second, to make our experiments tractable, our question-generation process is strictly fact-based and does not require multi-step logic, document synthesis, or other higher-level reasoning. Third, our dataset contains an enriched rate of contextual errors, so the reported metrics are not meant to represent bias rates in the wild. Fourth, our proposed token probability method only applies to models which provide probability outputs. Finally, even though this dataset is intended to improve an LLM's ability to provide users with accurate information, bad actors could use such information to exploit the shortcomings of certain models described in this paper.\nAs retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights spur further research into improving the robustness and calibration of such models. Resolving the tension between parametric priors and retrieved information is a crucial challenge on the path to safe and trustworthy language models.\n\nReferences\n\nMuhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy LLMs: Dealing with hallucinations in healthcare AI. September 2023.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in Retrieval-Augmented generation. AAAI, 38(16):17754-17762, March 2024a.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages $17754-17762$, 2024b.\n\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, and Nigam H Shah. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. April 2023.\n\nRyan Daws. Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. https://www.artificialintelligence-news.com/2020/10/28/ medical-chatbot-openai-gpt3-patient-kill-themselves/, October 2020. Accessed: 2024-1-19.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "For instance, for a given magnitude of deviation, Claude Opus adheres to incorrect contextual information 30\\% less often than GPT-4o.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1100e837-af2d-418f-a993-58c828e47422",
        "questions": "Does the ClashEval benchmark dataset suggest that performance on knowledge-based benchmarks automatically indicates suitability for RAG settings?",
        "answers": "No",
        "context": "5 Discussion\n\n\nThe ClashEval benchmark dataset and evaluations provide novel insights into how LLMs arbitrate between their own internal knowledge and contextual information when the two are in conflict.\n\nA key finding is that even the most advanced LLMs like GPT-4o exhibit a strong context bias, overriding their own correct prior knowledge over $60 \\%$ of the time when presented with incorrect information in the retrieved documents. However, this bias is not absolute - the degree to which the retrieved content deviates from truth negatively correlates with the context preference rate. Interestingly, each LLM exhibits a different prior distribution over truthfulness across domains, such that the same perturbation level affects each model differently. For instance, for a given magnitude of deviation, Claude Opus adheres to incorrect contextual information 30\\% less often than GPT-4o. While GPT-4o achieves state-of-the-art results on general-purpose tasks, it exhibits higher context bias compared to smaller models like Claude Sonnet. This finding suggests that performance on knowledge-based benchmarks may not automatically mean it is most suitable for RAG settings. Additionally, we find that LLMs are calibrated to selectively defer to external evidence when they are less certain about a given query. However, each model differs in how well-calibrated they are. While strong priors are not inherently problematic, the lack of explicit expectations around how models will decide to use contextual information remains a risk. We propose a simple method for improving models under ClashEval, and hope that future work can improve upon this baseline.\n\nOur analyses have several key limitations. First, RAG systems can be deployed to many more domains than can be covered by our analyses. Second, to make our experiments tractable, our question-generation process is strictly fact-based and does not require multi-step logic, document synthesis, or other higher-level reasoning. Third, our dataset contains an enriched rate of contextual errors, so the reported metrics are not meant to represent bias rates in the wild. Fourth, our proposed token probability method only applies to models which provide probability outputs. Finally, even though this dataset is intended to improve an LLM's ability to provide users with accurate information, bad actors could use such information to exploit the shortcomings of certain models described in this paper.\nAs retrieval-augmented AI systems become increasingly prevalent, we hope our dataset and insights spur further research into improving the robustness and calibration of such models. Resolving the tension between parametric priors and retrieved information is a crucial challenge on the path to safe and trustworthy language models.\n\nReferences\n\nMuhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy LLMs: Dealing with hallucinations in healthcare AI. September 2023.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in Retrieval-Augmented generation. AAAI, 38(16):17754-17762, March 2024a.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages $17754-17762$, 2024b.\n\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, Rachel Pedreira, Ethan Goh, Angel Arnaout, Garret Kenn Morris, Honor Magon, Matthew P Lungren, Eric Horvitz, and Nigam H Shah. Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. April 2023.\n\nRyan Daws. Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves. https://www.artificialintelligence-news.com/2020/10/28/ medical-chatbot-openai-gpt3-patient-kill-themselves/, October 2020. Accessed: 2024-1-19.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "This finding suggests that performance on knowledge-based benchmarks may not automatically mean it is most suitable for RAG settings.",
        "evidence_page_no": 8,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11029ff4-9930-436a-b2ad-9dfb1f24f379",
        "questions": "What is the name of the question-answering benchmark dataset introduced in the document that includes over 1200 questions spanning six domains?",
        "answers": "ClashEval",
        "context": "Figure 1: A schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. We then observe whether the LLM prefers the modified information or its own prior answer.\nrelevant retrieved content in the LLM prompt and can significantly improve model accuracy [Mao et al., 2020, Chen et al., 2024a, Lewis et al., 2020].\n\nMost commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplexity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search, whereas Gemini accesses Google Search results. While this can greatly enhance the model's ability to answer questions, it also raises concern for when the retrieved documents or webpages contain incorrect or harmful information [Dash et al., 2023, Daws, 2020, Nastasi et al., 2023]. Indeed, examples of this behavior have already surfaced in widely deployed LLMs. For example, recent headlines showed Google's AI Summary recommending people to \"eat rocks\" or \"put glue on their pizza\" [Hart, 2024, Williams, 2024], presumably due to erroneous or satirical webpages being retrieved. While stricter document filtering or improved retrieval may help reduce this occurrence, it by no means is a cure-all against this problem. At its core, LLMs should not blindly repeat information presented in context but should be able to arbitrate when external information conflicts with its own internal knowledge. While the aforementioned example is one in which the retrieved document is the source of error, the converse is also a significant problem: when the LLM insists on its own incorrect prior answer despite correct external information.\nSome studies have previously investigated the nature of this tension between a model's internal prior knowledge and contextual information. Longpre et al. [2021] found that LLMs exhibited a strong preference for information in the training data even when facts in the context were substituted with similar but incorrect information. More recently, Xie et al. [2023] showed that models can either be highly susceptible to context or very biased towards its priors depending on how the context is framed. Our study extends these works in two important ways. First, we present a dataset that contains examples not only when the context is wrong and the model is right but the converse (where the context is right but the model is wrong). This is important since a dataset that only measures the LLM's ability to reject wrong context can trivially excel at this task by simply always ignoring the context. Instead, our dataset uniquely tests the LLM's ability to arbitrate between its own parametric knowledge and the contextual information to determine the most accurate response. Second, we elicit a quantitative relationship between the LLM's preference of prior or context and two important variables: (1) the model's confidence in its prior response (via measuring the token probabilities of the initial response), and (2) the degree to which the contextual information provided deviates from the reference answer. Measuring these two dynamics is important for understanding how models transition between choosing the prior and the context and their inherent biases towards their priors or the context.\n\n\nOur contributions\n\n- We introduce ClashEval, a question-answering benchmark dataset of over 1200 questions spanning six domains that include the relevant contextual document for answering each",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We introduce ClashEval, a question-answering benchmark dataset of over 1200 questions spanning six domains that include the relevant contextual document for answering each",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "111004d0-3dd0-463c-bbbd-cd207c9814b7",
        "questions": "Which commercial LLM employs a Bing search to enhance its ability to answer questions?",
        "answers": "ChatGPT",
        "context": "Figure 1: A schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. We then observe whether the LLM prefers the modified information or its own prior answer.\nrelevant retrieved content in the LLM prompt and can significantly improve model accuracy [Mao et al., 2020, Chen et al., 2024a, Lewis et al., 2020].\n\nMost commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplexity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search, whereas Gemini accesses Google Search results. While this can greatly enhance the model's ability to answer questions, it also raises concern for when the retrieved documents or webpages contain incorrect or harmful information [Dash et al., 2023, Daws, 2020, Nastasi et al., 2023]. Indeed, examples of this behavior have already surfaced in widely deployed LLMs. For example, recent headlines showed Google's AI Summary recommending people to \"eat rocks\" or \"put glue on their pizza\" [Hart, 2024, Williams, 2024], presumably due to erroneous or satirical webpages being retrieved. While stricter document filtering or improved retrieval may help reduce this occurrence, it by no means is a cure-all against this problem. At its core, LLMs should not blindly repeat information presented in context but should be able to arbitrate when external information conflicts with its own internal knowledge. While the aforementioned example is one in which the retrieved document is the source of error, the converse is also a significant problem: when the LLM insists on its own incorrect prior answer despite correct external information.\nSome studies have previously investigated the nature of this tension between a model's internal prior knowledge and contextual information. Longpre et al. [2021] found that LLMs exhibited a strong preference for information in the training data even when facts in the context were substituted with similar but incorrect information. More recently, Xie et al. [2023] showed that models can either be highly susceptible to context or very biased towards its priors depending on how the context is framed. Our study extends these works in two important ways. First, we present a dataset that contains examples not only when the context is wrong and the model is right but the converse (where the context is right but the model is wrong). This is important since a dataset that only measures the LLM's ability to reject wrong context can trivially excel at this task by simply always ignoring the context. Instead, our dataset uniquely tests the LLM's ability to arbitrate between its own parametric knowledge and the contextual information to determine the most accurate response. Second, we elicit a quantitative relationship between the LLM's preference of prior or context and two important variables: (1) the model's confidence in its prior response (via measuring the token probabilities of the initial response), and (2) the degree to which the contextual information provided deviates from the reference answer. Measuring these two dynamics is important for understanding how models transition between choosing the prior and the context and their inherent biases towards their priors or the context.\n\n\nOur contributions\n\n- We introduce ClashEval, a question-answering benchmark dataset of over 1200 questions spanning six domains that include the relevant contextual document for answering each",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Most commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplexity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search, whereas Gemini accesses Google Search results.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11168820-a76d-4b32-a121-78c70ce131dc",
        "questions": "According to the document, what are the two important variables that the study measures to understand how models transition between choosing the prior and the context?",
        "answers": "The model's confidence in its prior response and the degree to which the contextual information provided deviates from the reference answer.",
        "context": "Figure 1: A schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. We then observe whether the LLM prefers the modified information or its own prior answer.\nrelevant retrieved content in the LLM prompt and can significantly improve model accuracy [Mao et al., 2020, Chen et al., 2024a, Lewis et al., 2020].\n\nMost commercial LLMs, like ChatGPT [OpenAI, 2023], Gemini [Gemini Team, 2023], and Perplexity.ai, already employ RAG in their Web interfaces. For example, ChatGPT employs a Bing search, whereas Gemini accesses Google Search results. While this can greatly enhance the model's ability to answer questions, it also raises concern for when the retrieved documents or webpages contain incorrect or harmful information [Dash et al., 2023, Daws, 2020, Nastasi et al., 2023]. Indeed, examples of this behavior have already surfaced in widely deployed LLMs. For example, recent headlines showed Google's AI Summary recommending people to \"eat rocks\" or \"put glue on their pizza\" [Hart, 2024, Williams, 2024], presumably due to erroneous or satirical webpages being retrieved. While stricter document filtering or improved retrieval may help reduce this occurrence, it by no means is a cure-all against this problem. At its core, LLMs should not blindly repeat information presented in context but should be able to arbitrate when external information conflicts with its own internal knowledge. While the aforementioned example is one in which the retrieved document is the source of error, the converse is also a significant problem: when the LLM insists on its own incorrect prior answer despite correct external information.\nSome studies have previously investigated the nature of this tension between a model's internal prior knowledge and contextual information. Longpre et al. [2021] found that LLMs exhibited a strong preference for information in the training data even when facts in the context were substituted with similar but incorrect information. More recently, Xie et al. [2023] showed that models can either be highly susceptible to context or very biased towards its priors depending on how the context is framed. Our study extends these works in two important ways. First, we present a dataset that contains examples not only when the context is wrong and the model is right but the converse (where the context is right but the model is wrong). This is important since a dataset that only measures the LLM's ability to reject wrong context can trivially excel at this task by simply always ignoring the context. Instead, our dataset uniquely tests the LLM's ability to arbitrate between its own parametric knowledge and the contextual information to determine the most accurate response. Second, we elicit a quantitative relationship between the LLM's preference of prior or context and two important variables: (1) the model's confidence in its prior response (via measuring the token probabilities of the initial response), and (2) the degree to which the contextual information provided deviates from the reference answer. Measuring these two dynamics is important for understanding how models transition between choosing the prior and the context and their inherent biases towards their priors or the context.\n\n\nOur contributions\n\n- We introduce ClashEval, a question-answering benchmark dataset of over 1200 questions spanning six domains that include the relevant contextual document for answering each",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Second, we elicit a quantitative relationship between the LLM's preference of prior or context and two important variables: (1) the model's confidence in its prior response (via measuring the token probabilities of the initial response), and (2) the degree to which the contextual information provided deviates from the reference answer.",
        "evidence_page_no": 1,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1117bacb-6e88-418c-a99e-12fe095c2be6",
        "questions": "What is the name of the dataset curated to evaluate the ability of large language models to handle conflicting retrieved content?",
        "answers": "ClashEval",
        "context": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence\n\n\n\nKevin Wu* \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ kevinywu@stanford.edu\n\n\n\nEric Wu* \\\\ Department of Electrical Engineering \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ wue@stanford.edu\n\n\n\nJames Zou \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ jamesz@stanford.edu\n\n\n\nRetrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \\%$ of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs - namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.\n\n\n1 Introduction\n\nLarge language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023, Sun et al., 2024, Ahmad et al., 2023]. Additionally, they are constrained to knowledge contained in their training corpus and are unable to answer queries about recent events or publicly restricted information. Retrieval augmented generation (RAG) is a commonly used framework that provides",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \\%$ of the time.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11192c5a-7120-478e-9a45-d8b8ea8605a6",
        "questions": "What percentage of the time do large language models override their own correct prior knowledge with incorrect retrieved content according to the ClashEval study?",
        "answers": "Over 60%",
        "context": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence\n\n\n\nKevin Wu* \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ kevinywu@stanford.edu\n\n\n\nEric Wu* \\\\ Department of Electrical Engineering \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ wue@stanford.edu\n\n\n\nJames Zou \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ jamesz@stanford.edu\n\n\n\nRetrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \\%$ of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs - namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.\n\n\n1 Introduction\n\nLarge language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023, Sun et al., 2024, Ahmad et al., 2023]. Additionally, they are constrained to knowledge contained in their training corpus and are unable to answer queries about recent events or publicly restricted information. Retrieval augmented generation (RAG) is a commonly used framework that provides",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \\%$ of the time.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "111f8adc-ef95-4b05-a507-509e35805023",
        "questions": "Is the ClashEval dataset and its evaluations open-sourced for future benchmarking on top-performing models?",
        "answers": "Yes",
        "context": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence\n\n\n\nKevin Wu* \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ kevinywu@stanford.edu\n\n\n\nEric Wu* \\\\ Department of Electrical Engineering \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ wue@stanford.edu\n\n\n\nJames Zou \\\\ Department of Biomedical Data Science \\\\ Stanford University \\\\ Stanford, CA 94305 \\\\ jamesz@stanford.edu\n\n\n\nRetrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over $60 \\%$ of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs - namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.\n\n\n1 Introduction\n\nLarge language models (LLMs) are prone to hallucinations and incorrect answers [Pal et al., 2023, Sun et al., 2024, Ahmad et al., 2023]. Additionally, they are constrained to knowledge contained in their training corpus and are unable to answer queries about recent events or publicly restricted information. Retrieval augmented generation (RAG) is a commonly used framework that provides",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.",
        "evidence_page_no": 0,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1122da51-f90c-41ce-9c4c-cadbf2f1553b",
        "questions": "What is the accuracy of GPT-4o when using calibrated token probability correction?",
        "answers": "0.754",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Calibrated Token Prob. Correction & \\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )} & \\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )} & 0.085(0.072,0.098)",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11235205-3b9f-4f0d-9c01-75d8c3d199f2",
        "questions": "How much does the calibrated token probability correction improve the overall accuracy of all models?",
        "answers": "14%",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "112a2174-9d27-4596-b945-2195d03ce606",
        "questions": "Does the calibrated token probability correction introduce more prior bias compared to the baseline of randomly replacing the final response with its prior?",
        "answers": "Yes",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "At the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method.",
        "evidence_page_no": 7,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1133add6-90a9-4e49-88e8-0b6993c198e1",
        "questions": "What is the context bias percentage for the Claude Opus model when resolving prior vs. context conflicts?",
        "answers": "15.7%",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1138b1e2-488d-4d7a-a312-810215328479",
        "questions": "Which model has the highest context bias among the models listed, excluding GPT-3.5?",
        "answers": "GPT-4o",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "113c1836-77f4-431b-bde9-d47dd89331ac",
        "questions": "What is the accuracy percentage of the Llama-3 model when discerning between prior and context answers?",
        "answers": "Near random accuracy",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer.",
        "evidence_page_no": 5,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "113c88f4-d3ca-4477-8fa2-d2731240c5fa",
        "questions": "What is the effect of using a 'Strict' prompt with GPT-4 on context preference rate compared to a 'Loose' prompt?",
        "answers": "The 'Strict' prompt strongly enforces literal adherence to the retrieved context, while the 'Loose' prompt encourages the model to make a reasonable judgment in light of the provided context.",
        "context": "Figure 6: Effect of different prompts using GPT-4 on context preference rate vs prior probability. The \"Strict\" prompt strongly enforces literal adherence to the retrieved context, while the \"Loose\" prompt encourages the model to make a reasonable judgment in light of the provided context. We observe lower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling context preference. Full prompts are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "The 'Strict' prompt strongly enforces literal adherence to the retrieved context, while the 'Loose' prompt encourages the model to make a reasonable judgment in light of the provided context.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1140b40c-0fa1-43f1-9171-649d930d25ab",
        "questions": "Does the wording of prompts affect context preference when using GPT-4?",
        "answers": "Yes",
        "context": "Figure 6: Effect of different prompts using GPT-4 on context preference rate vs prior probability. The \"Strict\" prompt strongly enforces literal adherence to the retrieved context, while the \"Loose\" prompt encourages the model to make a reasonable judgment in light of the provided context. We observe lower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling context preference. Full prompts are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We observe lower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling context preference.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1140e8ae-8923-44d6-bffd-d31f91c73e1d",
        "questions": "Where can the full prompts used in the study on GPT-4's context preference be found?",
        "answers": "GitHub repository",
        "context": "Figure 6: Effect of different prompts using GPT-4 on context preference rate vs prior probability. The \"Strict\" prompt strongly enforces literal adherence to the retrieved context, while the \"Loose\" prompt encourages the model to make a reasonable judgment in light of the provided context. We observe lower and steeper drops in context preference with the loose vs strict prompts, suggesting that prompt wording plays a significant factor in controlling context preference. Full prompts are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Full prompts are provided in our GitHub repository.",
        "evidence_page_no": 12,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1146749b-beff-45eb-a0cf-822975201d7f",
        "questions": "What is the relationship between the token probability of the model's prior answer and the RAG preference rate for QA datasets?",
        "answers": "A consistent negative relationship",
        "context": "Figure 3: We observe an inverse relationship between the context preference rate (y-axis) and the amount of deviation from the prior (x-axis). Each plot visualizes absolute deviation from the reference information (for numerical datasets, up to two log-fold changes (along with the trendline); for \"Years\", the absolute number of years; for categorical datasets, a total of four modification categories) against context preference rate.\n\n\n4.3 Context Preference Rate vs. Prior Token Probability\n\n\nIn Figure 4, we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate for all six QA datasets. To visualize an even distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of $[0.0,1.0]$. The slope indicates the effect of stronger model confidence on the model's preference for the information presented in the retrieved context; we observe different slopes (ranging from - 0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model's internal prior knowledge confidence. Specifically, a slope of -0.45 , for instance, can be interpreted as expecting a $4.5 \\%$ decrease in the likelihood of the LLM preferring the contextual information for every $10 \\%$ increase in the probability of the model's prior response.\n\n4.3.1 Initial Methods for Improving Prior vs. Context Conflict Resolution\n\nBased on our observations from the relationship between the token probabilities and the rates of preference for context, we posit that comparing token probabilities between $r(q)$ and $r(q \\mid c)$ can improve the abilities of models to resolve conflicts. In Table 3, Token Probability Correction is done by comparing the mean token probabilities of the model's response with and without context. If the probability is higher for the prior than the contextual response, then we use the model's generation without context as its final response. Otherwise, we just use the response with context. We find that this method improves the overall accuracy of all three models with a moderate increase in the prior bias of each model. Next, we observe that the probability distributions between prior responses and context-given responses are uncalibrated, where context-given response probabilities are extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we compare the percentiles rather than raw probability scores of each score, or the Calibrated Token",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "In Figure 4, we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate for all six QA datasets.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "114acf8e-89b5-4d29-b44d-732592a90fd2",
        "questions": "What is the effect of a slope of -0.45 on the likelihood of the LLM preferring contextual information?",
        "answers": "A 4.5% decrease in the likelihood of the LLM preferring the contextual information for every 10% increase in the probability of the model's prior response.",
        "context": "Figure 3: We observe an inverse relationship between the context preference rate (y-axis) and the amount of deviation from the prior (x-axis). Each plot visualizes absolute deviation from the reference information (for numerical datasets, up to two log-fold changes (along with the trendline); for \"Years\", the absolute number of years; for categorical datasets, a total of four modification categories) against context preference rate.\n\n\n4.3 Context Preference Rate vs. Prior Token Probability\n\n\nIn Figure 4, we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate for all six QA datasets. To visualize an even distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of $[0.0,1.0]$. The slope indicates the effect of stronger model confidence on the model's preference for the information presented in the retrieved context; we observe different slopes (ranging from - 0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model's internal prior knowledge confidence. Specifically, a slope of -0.45 , for instance, can be interpreted as expecting a $4.5 \\%$ decrease in the likelihood of the LLM preferring the contextual information for every $10 \\%$ increase in the probability of the model's prior response.\n\n4.3.1 Initial Methods for Improving Prior vs. Context Conflict Resolution\n\nBased on our observations from the relationship between the token probabilities and the rates of preference for context, we posit that comparing token probabilities between $r(q)$ and $r(q \\mid c)$ can improve the abilities of models to resolve conflicts. In Table 3, Token Probability Correction is done by comparing the mean token probabilities of the model's response with and without context. If the probability is higher for the prior than the contextual response, then we use the model's generation without context as its final response. Otherwise, we just use the response with context. We find that this method improves the overall accuracy of all three models with a moderate increase in the prior bias of each model. Next, we observe that the probability distributions between prior responses and context-given responses are uncalibrated, where context-given response probabilities are extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we compare the percentiles rather than raw probability scores of each score, or the Calibrated Token",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Specifically, a slope of -0.45 , for instance, can be interpreted as expecting a $4.5 \\%$ decrease in the likelihood of the LLM preferring the contextual information for every $10 \\%$ increase in the probability of the model's prior response.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1157c073-b3e1-4908-93f0-e4b4387138d4",
        "questions": "Does the Token Probability Correction method improve the overall accuracy of the models?",
        "answers": "Yes",
        "context": "Figure 3: We observe an inverse relationship between the context preference rate (y-axis) and the amount of deviation from the prior (x-axis). Each plot visualizes absolute deviation from the reference information (for numerical datasets, up to two log-fold changes (along with the trendline); for \"Years\", the absolute number of years; for categorical datasets, a total of four modification categories) against context preference rate.\n\n\n4.3 Context Preference Rate vs. Prior Token Probability\n\n\nIn Figure 4, we observe a consistent negative relationship between the token probability of the model's prior answer and the associated RAG preference rate for all six QA datasets. To visualize an even distribution across probabilities, we bin the probabilities into ten equidistant bins in the range of $[0.0,1.0]$. The slope indicates the effect of stronger model confidence on the model's preference for the information presented in the retrieved context; we observe different slopes (ranging from - 0.1 to -0.45), suggesting that the effectiveness of RAG in different QA domains can be characterized as being relatively susceptible (e.g., with Dates questions) or robust (e.g., with News questions) to the model's internal prior knowledge confidence. Specifically, a slope of -0.45 , for instance, can be interpreted as expecting a $4.5 \\%$ decrease in the likelihood of the LLM preferring the contextual information for every $10 \\%$ increase in the probability of the model's prior response.\n\n4.3.1 Initial Methods for Improving Prior vs. Context Conflict Resolution\n\nBased on our observations from the relationship between the token probabilities and the rates of preference for context, we posit that comparing token probabilities between $r(q)$ and $r(q \\mid c)$ can improve the abilities of models to resolve conflicts. In Table 3, Token Probability Correction is done by comparing the mean token probabilities of the model's response with and without context. If the probability is higher for the prior than the contextual response, then we use the model's generation without context as its final response. Otherwise, we just use the response with context. We find that this method improves the overall accuracy of all three models with a moderate increase in the prior bias of each model. Next, we observe that the probability distributions between prior responses and context-given responses are uncalibrated, where context-given response probabilities are extremely right-tailed while prior probabilities are nearly uniform. As a simple adjustment, we compare the percentiles rather than raw probability scores of each score, or the Calibrated Token",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "We find that this method improves the overall accuracy of all three models with a moderate increase in the prior bias of each model.",
        "evidence_page_no": 6,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "115d3e44-3adf-4a11-862d-d64a97fb334b",
        "questions": "What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults according to the document?",
        "answers": "30",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Reference & 30 & 30 & v",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11625ccc-c0b9-489a-b3bf-052d24d210fb",
        "questions": "In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?",
        "answers": "1976",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Reference & 1976 & 1976 & v",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1167d3f5-dfa6-4abe-9fe8-80c09e7f4775",
        "questions": "Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?",
        "answers": "Sandy Gumulya",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Reference & Sandy Gumulya & Sandy Gumulya & v",
        "evidence_page_no": 4,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "116b47e2-e046-4545-a0ab-75bf34b2a7da",
        "questions": "How many questions are included in the Drug Dosage dataset?",
        "answers": "249",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Drug Dosage & 249 & 10 & \\begin{tabular}{l} What is the maximum daily dosage in mg for extended release oxybutynin in adults with overactive bladder? \\end{tabular}",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "116c00eb-243f-42b4-ab25-68c03c82b15f",
        "questions": "Which dataset includes questions about Olympic records across multiple sports such as athletics and swimming?",
        "answers": "Sports Records",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "Sports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "116d5d35-e4a8-4e2b-9286-3cd3658df18f",
        "questions": "Does the News dataset contain any personally identifiable information or offensive content according to the document?",
        "answers": "No",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Yes/No",
        "evidence_source": "text",
        "evidence_context": "As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge.",
        "evidence_page_no": 3,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "116febe7-aa0c-4327-9daa-42fc90b15e3d",
        "questions": "Which models were benchmarked on the dataset in the study that evaluates RAG question-answering capabilities?",
        "answers": "GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet",
        "context": "question. The answer in each document is perturbed across a range of erroneous values, from subtle to extreme.\n- We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.\n- We provide a systematic analysis of context preference rates across three models on (1) varying degrees of perturbation on the contextual information and (2) the token probabilities of the prior responses.\n- We propose a simple way to improve performance on ClashEval by incorporating token probabilities.\n\n\n2 Related Works\n\n\nThe issue of hallucination in LLMs has been explored in multiple contexts and models [Ji et al., 2023, Kaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster et al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks in various settings [Es et al., 2023a, Hoshi et al., 2023, Saad-Falcon et al., 2023a, Zhang et al., 2024]. For example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context relevance of RAG systems by using GPT-3.5 as an evaluator [Es et al., 2023b, Saad-Falcon et al., 2023b]. In another study, the authors propose metrics such as noise robustness, negative rejection, information integration, and counterfactual robustness [Chen et al., 2024b]. Multiple studies have shown that RAG can mislead LLMs in the presence of complex or misleading search results and that such models can still make mistakes even when given the correct response [Foulds et al., 2024, Shuster et al., 2021]. In relation to understanding model priors, other works have used log probabilities to assess the LLM's confidence in responses [Mitchell et al., 2023, Zhao et al., 2024]. However, so far there has not been a systematic exploration of a model's confidence (via logprobs) and the model's preference for RAG-provided information. Previous work has also focused on ways to address model adherence to incorrect context. For example, Longpre et al. [2021] suggests pretraining on substituted facts to improve future robustness and Xiang et al. [2024] proposes ensembling isolated answers across multiple documents. In this work, we focus on the case where LLMs are available only via inference, and only one document is being used as context.\n\n3 Methods\n\n3.1 Definitions and Metrics\n\nFollowing the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance $x=(q, c)$ where $q$ is the query and $c$ is the context provided to answer the query. A model's prior response is $r(q)$, where the model is asked to answer the question with only its parametric knowledge. A model's contextual response is $r(q \\mid c)$, where its response to the query is conditioned on the provided context.\n\nIn our study, we define the following metrics:\n- Accuracy $=\\operatorname{Pr}[r(q \\mid c)$ is right $\\mid c$ is right or $r(q)$ is right $]$, the probability the model responds correctly given that either the context is right or the prior is right.\n- Prior Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is right and $r(q)$ is wrong $]$, the probability the model uses its prior while the context is correct.\n- Context Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is wrong and $r(q)$ is right $]$, the probability the model uses the context while the prior is correct.\n\nOur main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when introducing varying levels of perturbations on the RAG documents. For this study, our dataset consists of 1,294 total questions across 6 different domains. We evaluate the following models: GPT-4o, GPT3.5 (gpt-3.5-turbo-0125), Llama-3 (Llama-3-7B-Instruct), Claude Opus, Claude Sonnet, and Gemini 1.5 Flash. For our contextual responses, we use a standard prompt template that is based on RAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March 2024 (LangChain and LlamaIndex). In addition to this standard prompt, we experiment with \"strict\" and \"loose\" prompts, with results in 6 . Full prompts used are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "text",
        "evidence_context": "We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "117d0b17-70f4-48cd-acb0-08dcb00c1552",
        "questions": "What is the probability that a model responds correctly given that either the context is right or the prior is right, according to the study's defined metrics?",
        "answers": "Accuracy",
        "context": "question. The answer in each document is perturbed across a range of erroneous values, from subtle to extreme.\n- We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.\n- We provide a systematic analysis of context preference rates across three models on (1) varying degrees of perturbation on the contextual information and (2) the token probabilities of the prior responses.\n- We propose a simple way to improve performance on ClashEval by incorporating token probabilities.\n\n\n2 Related Works\n\n\nThe issue of hallucination in LLMs has been explored in multiple contexts and models [Ji et al., 2023, Kaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster et al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks in various settings [Es et al., 2023a, Hoshi et al., 2023, Saad-Falcon et al., 2023a, Zhang et al., 2024]. For example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context relevance of RAG systems by using GPT-3.5 as an evaluator [Es et al., 2023b, Saad-Falcon et al., 2023b]. In another study, the authors propose metrics such as noise robustness, negative rejection, information integration, and counterfactual robustness [Chen et al., 2024b]. Multiple studies have shown that RAG can mislead LLMs in the presence of complex or misleading search results and that such models can still make mistakes even when given the correct response [Foulds et al., 2024, Shuster et al., 2021]. In relation to understanding model priors, other works have used log probabilities to assess the LLM's confidence in responses [Mitchell et al., 2023, Zhao et al., 2024]. However, so far there has not been a systematic exploration of a model's confidence (via logprobs) and the model's preference for RAG-provided information. Previous work has also focused on ways to address model adherence to incorrect context. For example, Longpre et al. [2021] suggests pretraining on substituted facts to improve future robustness and Xiang et al. [2024] proposes ensembling isolated answers across multiple documents. In this work, we focus on the case where LLMs are available only via inference, and only one document is being used as context.\n\n3 Methods\n\n3.1 Definitions and Metrics\n\nFollowing the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance $x=(q, c)$ where $q$ is the query and $c$ is the context provided to answer the query. A model's prior response is $r(q)$, where the model is asked to answer the question with only its parametric knowledge. A model's contextual response is $r(q \\mid c)$, where its response to the query is conditioned on the provided context.\n\nIn our study, we define the following metrics:\n- Accuracy $=\\operatorname{Pr}[r(q \\mid c)$ is right $\\mid c$ is right or $r(q)$ is right $]$, the probability the model responds correctly given that either the context is right or the prior is right.\n- Prior Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is right and $r(q)$ is wrong $]$, the probability the model uses its prior while the context is correct.\n- Context Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is wrong and $r(q)$ is right $]$, the probability the model uses the context while the prior is correct.\n\nOur main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when introducing varying levels of perturbations on the RAG documents. For this study, our dataset consists of 1,294 total questions across 6 different domains. We evaluate the following models: GPT-4o, GPT3.5 (gpt-3.5-turbo-0125), Llama-3 (Llama-3-7B-Instruct), Claude Opus, Claude Sonnet, and Gemini 1.5 Flash. For our contextual responses, we use a standard prompt template that is based on RAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March 2024 (LangChain and LlamaIndex). In addition to this standard prompt, we experiment with \"strict\" and \"loose\" prompts, with results in 6 . Full prompts used are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "equation",
        "evidence_context": "Accuracy $=\\operatorname{Pr}[r(q \\mid c)$ is right $\\mid c$ is right or $r(q)$ is right $]$, the probability the model responds correctly given that either the context is right or the prior is right.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11870fe4-41b4-49bd-aa80-c8090af84578",
        "questions": "How many total questions were included in the dataset used to evaluate the RAG question-answering capabilities of the six LLMs?",
        "answers": "1,294",
        "context": "question. The answer in each document is perturbed across a range of erroneous values, from subtle to extreme.\n- We benchmark six top-performing LLMs (GPT-4o, GPT-3.5, Llama-3-8b-instruct, Gemini 1.5, Claude Opus, and Claude Sonnet) on this dataset and report three relevant metrics.\n- We provide a systematic analysis of context preference rates across three models on (1) varying degrees of perturbation on the contextual information and (2) the token probabilities of the prior responses.\n- We propose a simple way to improve performance on ClashEval by incorporating token probabilities.\n\n\n2 Related Works\n\n\nThe issue of hallucination in LLMs has been explored in multiple contexts and models [Ji et al., 2023, Kaddour et al., 2023]. As a response, RAG systems have been shown to reduce hallucination [Shuster et al., 2021, Kang et al., 2023]. Previous works have explored automated RAG evaluation frameworks in various settings [Es et al., 2023a, Hoshi et al., 2023, Saad-Falcon et al., 2023a, Zhang et al., 2024]. For example, some studies use LLMs to evaluate the faithfulness, answer relevance, and context relevance of RAG systems by using GPT-3.5 as an evaluator [Es et al., 2023b, Saad-Falcon et al., 2023b]. In another study, the authors propose metrics such as noise robustness, negative rejection, information integration, and counterfactual robustness [Chen et al., 2024b]. Multiple studies have shown that RAG can mislead LLMs in the presence of complex or misleading search results and that such models can still make mistakes even when given the correct response [Foulds et al., 2024, Shuster et al., 2021]. In relation to understanding model priors, other works have used log probabilities to assess the LLM's confidence in responses [Mitchell et al., 2023, Zhao et al., 2024]. However, so far there has not been a systematic exploration of a model's confidence (via logprobs) and the model's preference for RAG-provided information. Previous work has also focused on ways to address model adherence to incorrect context. For example, Longpre et al. [2021] suggests pretraining on substituted facts to improve future robustness and Xiang et al. [2024] proposes ensembling isolated answers across multiple documents. In this work, we focus on the case where LLMs are available only via inference, and only one document is being used as context.\n\n3 Methods\n\n3.1 Definitions and Metrics\n\nFollowing the notation from Longpre et al. [2021], Xie et al. [2023], we start with a QA instance $x=(q, c)$ where $q$ is the query and $c$ is the context provided to answer the query. A model's prior response is $r(q)$, where the model is asked to answer the question with only its parametric knowledge. A model's contextual response is $r(q \\mid c)$, where its response to the query is conditioned on the provided context.\n\nIn our study, we define the following metrics:\n- Accuracy $=\\operatorname{Pr}[r(q \\mid c)$ is right $\\mid c$ is right or $r(q)$ is right $]$, the probability the model responds correctly given that either the context is right or the prior is right.\n- Prior Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is right and $r(q)$ is wrong $]$, the probability the model uses its prior while the context is correct.\n- Context Bias $=\\operatorname{Pr}[r(q \\mid c)$ is wrong $\\mid c$ is wrong and $r(q)$ is right $]$, the probability the model uses the context while the prior is correct.\n\nOur main analysis consists of evaluating the RAG question-answering capabilities of six LLMs when introducing varying levels of perturbations on the RAG documents. For this study, our dataset consists of 1,294 total questions across 6 different domains. We evaluate the following models: GPT-4o, GPT3.5 (gpt-3.5-turbo-0125), Llama-3 (Llama-3-7B-Instruct), Claude Opus, Claude Sonnet, and Gemini 1.5 Flash. For our contextual responses, we use a standard prompt template that is based on RAG prompts used on popular LLM open-source libraries, with over 800k downloads as of March 2024 (LangChain and LlamaIndex). In addition to this standard prompt, we experiment with \"strict\" and \"loose\" prompts, with results in 6 . Full prompts used are provided in our GitHub repository.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "text",
        "evidence_context": "For this study, our dataset consists of 1,294 total questions across 6 different domains.",
        "evidence_page_no": 2,
        "is_extra_qa": false
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "1197a1ae-a4ad-4b24-90ae-180f7af5613d",
        "questions": "Which model shows the highest accuracy when resolving prior vs. context conflict, according to the given results?",
        "answers": "Claude Opus",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Claude Opus, with an accuracy of $74.3 \\%$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "119a7034-8817-446b-b100-a5ca6589a70b",
        "questions": "What is the context bias percentage for the GPT-3.5 model?",
        "answers": "0.841",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "119aa71b-f6bf-47b5-9f46-fe0222087f22",
        "questions": "Compare the context correct rate for Claude Sonnet and GPT-4o models. Which model has a higher rate and what is the difference?",
        "answers": "GPT-4o has a higher context correct rate by 0.022",
        "context": "drug dosages and news. Each modified fact is replaced in the original retrieved text. Then, both the question and context are posed to GPT-4, from which the answers, along with the log probabilities of the output tokens, are collected.\n\n\n4 Results\n\n\\begin{tabular}{lllc}\n  Model & Chosen & Prior Correct & Context Correct \\\\\n  \\multirow{3}{*}{ Claude Opus } & Prior & $0.585(0.550,0.619)$ & $0.042(0.027,0.058)$ \\\\\n& Context & $0.313(0.282,0.346)$ & $0.901(0.879,0.923)$ \\\\\n& Neither & $0.102(0.082,0.125)$ & $0.057(0.040,0.075)$ \\\\\n  \\multirow{3}{*}{ Claude Sonnet } & Prior & $0.436(0.403,0.469)$ & $0.051(0.037,0.067)$ \\\\\n& Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ \\\\\n& Neither & $0.163(0.138,0.186)$ & $0.068(0.052,0.086)$ \\\\\n  \\multirow{3}{*}{ Gemini 1.5 } & Prior & $0.388(0.362,0.416)$ & $0.074(0.058,0.091)$ \\\\\n& Context & $0.490(0.461,0.521)$ & $0.860(0.838,0.881)$ \\\\\n& Neither & $0.122(0.103,0.143)$ & $0.066(0.051,0.082)$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & Prior & $0.327(0.293,0.358)$ & $0.041(0.027,0.056)$ \\\\\n& Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$ \\\\\n& Neither & $0.065(0.047,0.083)$ & $0.056(0.040,0.072)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & Prior & $0.237(0.213,0.263)$ & $0.057(0.043,0.072)$ \\\\\n& Context & $0.626(0.598,0.657)$ & $0.841(0.817,0.865)$ \\\\\n& Neither & $0.137(0.113,0.160)$ & $0.102(0.082,0.123)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & Prior & $0.208(0.185,0.230)$ & $0.041(0.029,0.054)$ \\\\\n& Context & $0.529(0.499,0.558)$ & $0.793(0.767,0.818)$ \\\\\n& Neither & $0.263(0.236,0.291)$ & $0.166(0.145,0.191)$ \\\\\n \n\\end{tabular}\n\nTable 2: We report model behavior given a subset of the data where either the prior or the context is correct. A model exhibits prior bias by choosing its prior when only the context is correct, while it exhibits context bias by choosing the context when only the prior is correct. We also report when neither the prior nor context answer is used in the model response.\n\n4.1 Prior vs. Context Conflict Resolution\n\nIn Table 2, Table 4, and Figure 5, we report the responses for each of the six models when only the prior is correct or only the context is correct. On one end, models like Llama-3 and GPT-3.5 are at near random accuracy at the task of discerning when to use the prior or context answer. On the other hand, the top performing model on all three metrics is Claude Opus, with an accuracy of $74.3 \\%$, a context bias of $15.7 \\%$, and a prior bias of $2.1 \\%$. Interestingly, while GPT-4o is the current highest performing model on LMSYS Chatbot Area (as of June 2024), it has a higher context bias than all other models but GPT-3.5. While Llama-3 has a lower context bias than GPT-4o, it also has a lower accuracy because it has a higher rate of choosing neither the prior nor the context in its response. Examples of questions and model responses are shown in 2.\n\n4.2 Context Preference Rate vs. Degree of Context Modification\n\nWe consider the degree of deviation between the model's prior response and the value contained in the retrieved context (Figure 3). After fitting a linear model over the data, we find a clear negative correlation between the degree of modification in the context to the context preference rate. Models that perform stronger on ClashEval exhibit both a lower intercept and a more negative slope, indicating higher resistance to incorrect context. For example, Claude Opus adheres to incorrect contextual information $30 \\%$ less than GPT- 4 o for the same degrees of modification. Interestingly, these results suggest that each model has a different prior distribution over truthfulness across each domain.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Claude Sonnet & Context & $0.401(0.374,0.434)$ & $0.881(0.859,0.903)$ ... GPT-4o & Context & $0.608(0.571,0.643)$ & $0.903(0.881,0.923)$",
        "evidence_page_no": 5,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11a4e0a9-f52a-4dbe-8279-1be8bbee3ce8",
        "questions": "How many unique question-answer pairs were generated for the Drug Dosage dataset?",
        "answers": "249",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Drug Dosage & 249 & 10 & \\begin{tabular}{l} What is the maximum daily dosage in mg for extended release oxybutynin in adults with overactive bladder? \\end{tabular}",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11aa387b-f313-4a72-aafe-38ae9ac6a4d6",
        "questions": "What is the total number of perturbations applied to the Sports Records dataset questions?",
        "answers": "3",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Sports Records & 191 & 3 & \\begin{tabular}{l} What is the Olympic record for Men's 100 metres in athletics (time)? \\end{tabular}",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11aeb9d2-1be1-49e1-8f4a-3366f66aad2e",
        "questions": "Which dataset has a higher number of question-answer pairs than the Sports Records dataset, but a lower number of perturbations?",
        "answers": "Wikipedia Dates",
        "context": "3.2 Dataset\n\n\\begin{tabular}{lrrl}\n  Dataset Name & \\# Questions & \\# Perturbations & Example Question \\\\\n  Drug Dosage & 249 & 10 & \\begin{tabular}{l} \nWhat is the maximum daily dosage in mg \\\\\nfor extended release oxybutynin in adults \\\\\nwith overactive bladder?\n\\end{tabular} \\\\\n  News & 238 & 10 & \\begin{tabular}{l} \nHow many points did Paige Bueckers score \\\\\nin the Big East Tournament title game on \\\\\nMarch 6, 2023?\n\\end{tabular} \\\\\n  Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} \nIn which year was the census conducted that \\\\\nreported the population of Lukhi village in \\\\\nIran as 35, in 8 families?\n\\end{tabular} \\\\\n  Sports Records & 191 & 3 & \\begin{tabular}{l} \nWhat is the Olympic record for Men's 100 \\\\\nmetres in athletics (time)?\n\\end{tabular} \\\\\n  Locations & 200 & \\begin{tabular}{l} \nWhich former United States Senator, born \\\\\nin 1955, also shares the surname with other \\\\\nsenators at the state level in Wisconsin, Min- \\\\\nnesota, Massachusetts, Puerto Rico, and \\\\\nNew York City?\n\\end{tabular} \\\\\n \n\\end{tabular}\n\nTable 1: Statistics for each dataset, including number of questions, number of perturbations applied to each question, and an example question.\n\nWe generate questions from six subject domains (summarized in 1. To generate a large set of question-and-answer pairs, we extract a corpus of content webpages and then query GPT-4o to generate a question based on the text, along with the ground truth answer and the excerpt used to generate the question. Additionally, we select six different datasets to cover diverse knowledge domains and difficulties. For example, news articles are included as examples of out-of-distribution questions that cannot be answered properly without context. For each dataset below, we provide the full prompts used to generate questions in our GitHub repository. Generated questions significantly transform the original data and are covered under fair use; full document content may be covered under copyright, but we provide the accompanying code to reproduce the data. As our data is sourced from the Associated Press and Wikipedia, there is no personally identifiable information or offensive content to our knowledge. UpToDate contains drug information and does not contain PHI or offensive content.\n\nDrug Dosages We initially randomly sampled 500 drug information pages from UpToDate.com, a medical reference website widely used by clinicians. To constrain the scope of questions, we specify in the prompt that the answer must be numerical and in milligrams. To filter out generated questions that did not meet the specified criteria (e.g. ambiguous question, incorrect units, etc.), we perform an additional quality control step, where we ask GPT-4o to verify that the generated question fulfills all criteria. After this step, we have 249 question-answer pairs.\n\nSports Records We pulled Olympic records pages from Wikipedia.org across 9 sports: athletics, weightlifting, swimming, archery, track cycling, rowing, shooting, short-track speed skating, and speed skating. Records are extracted in a table format, from which questions are generated for each record entry. In total, after filtering, we extracted 191 unique questions and answers.\n\nNews Top headlines are pulled from the Associated Press RSS feed for dates ranging from 03/15/24 to $03 / 25 / 24$. From an initial corpus of 1486 news articles, we use GPT- 4 o to generate one question per article, instructing it to produce questions for which there is a clear numerical answer. We performed another GPT-4o quality control step, which resulted in 238 unique question-answer pairs.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Wikipedia Dates & 200 & 10 & \\begin{tabular}{l} In which year was the census conducted that reported the population of Lukhi village in Iran as 35, in 8 families? \\end{tabular}",
        "evidence_page_no": 3,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11cbb930-94d0-4262-a965-2ff98a2e8edf",
        "questions": "What is the reference maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?",
        "answers": "30",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Reference & 30 & 30 & v",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11d56acf-680f-45e2-a0c3-6cdd9937f592",
        "questions": "What is the Olympic record for Men's 10,000 metres in speed skating?",
        "answers": "49.45",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Reference & 49.45 & 49.45 & $\\checkmark$",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11d7ad85-6e2d-4604-9ac0-407b8f06d1a1",
        "questions": "In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?",
        "answers": "1976",
        "context": "\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n  Dataset & \\begin{tabular}{l}\nExample \\\\\nQuestion\n\\end{tabular} & Answer & Response w/o Context & Modification & Value in document & Response w/ Context & Preferred Context? \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nDrug \\\\\nDosages\n\\end{tabular}} & \\multirow[t]{5}{*}{What is the maximum daily dosage of olanzapine for the treatment of agitation/aggression associated with psychiatric disorders in adults?} & \\multirow{5}{*}{30} & \\multirow{5}{*}{20} & 0.1 x & 3 & 20 & $\\times$ \\\\\n  & & & & $0.4 x$ & 12 & 20 & $\\times$ \\\\\n  & & & & Reference & 30 & 30 & v \\\\\n  & & & & $1.5 x$ & 45 & 45 & v \\\\\n  & & & & $10 x$ & 300 & 20 & $\\times$ \\\\\n  \\multirow{5}{*}{\\begin{tabular}{l}\nSports \\\\\nRecords\n\\end{tabular}} & \\multirow{5}{*}{What is the Olympic record for Men's 10,000 metres in speed skating (time)?} & \\multirow{5}{*}{49.45} & \\multirow{5}{*}{49.45} & $0.1 x$ & 4.904 & 49.45 & X \\\\\n  & & & & $0.4 x$ & 19.618 & 19.618 & v \\\\\n  & & & & Reference & 49.45 & 49.45 & $\\checkmark$ \\\\\n  & & & & $1.5 x$ & $1: 13.567$ & $1: 13.567$ & v \\\\\n  & & & & 10 x & 8:10.450 & 8:10.450 & $\\checkmark$ \\\\\n  \\multirow{5}{*}{Dates} & \\multirow{5}{*}{In what year did Frank Thompson Jr. become the chairman of the House Administration Committee?} & \\multirow{5}{*}{1976} & \\multirow{5}{*}{1975} & $-77$ & 1899 & 1975 & $\\times$ \\\\\n  & & & & $-11$ & 1965 & 1965 & $\\checkmark$ \\\\\n  & & & & Reference & 1976 & 1976 & v \\\\\n  & & & & 11 & 1987 & 1977 & $\\times$ \\\\\n  & & & & 77 & 2053 & 1975 & $x$ \\\\\n  \\multirow{3}{*}{Names} & \\multirow[b]{3}{*}{Who did Whitney Jones partner with in the doubles draw at the 2007 Sunfeast Open?} & \\multirow{3}{*}{\\begin{tabular}{l}\nSandy \\\\\nGumulya\n\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}{l}\nTatiana \\\\\nPoutchek\n\\end{tabular}} & Reference & Sandy Gumulya & Sandy Gumulya & v \\\\\n  & & & & Slight & Sandra Gumulya & \\begin{tabular}{l}\nSandra \\\\\nGumulya\n\\end{tabular} & v \\\\\n  & & & & Comical & \\begin{tabular}{l}\nSandy \\\\\nBubbleyumya\n\\end{tabular} & Sandy Gumulya & $x$ \\\\\n  \\multirow{3}{*}{Locations} & \\multirow[t]{3}{*}{\\begin{tabular}{l}\nWhich city was Ivan \\\\\nRybovalov born in on \\\\\nNovember 29, 1981?\n\\end{tabular}} & \\multirow{3}{*}{Simferopol} & \\multirow{3}{*}{Kharkiv} & Reference & Simferopol & Simferopol & $\\checkmark$ \\\\\n  & & & & Slight & Sevastopol & Sevastopol & $\\checkmark$ \\\\\n  & & & & Comical & Simpsonsopolis & Simferopol & $\\times$ \\\\\n \n\\end{tabular}\n\nFigure 2: Examples from three datasets demonstrating differential LLM responses across various types of context modifications. Responses in red indicate wrong responses (different than the answer); responses in green indicate correct responses.\n\nDates, Names, and Cities We begin with a random sample of 1000 articles from Huggingface's Wikipedia dataset (20220301.en, [Foundation]). We use GPT-4o to generate questions related to each field (dates, names, and cities) and filter out responses where the excerpt is not exactly found in the context. To reduce ambiguity when matching groundtruth answers, we restrict the answers to fit certain formats. For dates, we require that the answer adheres to a four-digit year (YYYY). For names, we require a first and last name (eg. George Washington). For cities, we remove any other identities (eg. Seattle, not Seattle, WA). For each domain, among the remaining question-answer pairs that fit these criteria, we randomly sample 200 for our evaluation set.\n\n\n3.3 Modifying the Retrieved Documents\n\n\nWe perform systematic perturbations on each question/answer pair (as visualized in Figure 1. In three datasets with numerical answers (Drug Dosages, Sports Records, Latest News), we produce ten modifications that act as multipliers on the original value: $0.1,0.2,0.4,0.8,1.2,1.5,2.0,3.0,5.0,10.0$. In the Wikipedia Years dataset, we perform ten absolute modifications in increments of 20 years for a range of $[-100,100]$. For the Wikipedia Names and Locations, the discrete categories required more hand-crafted levels of variation. For each, we performed three categorical perturbations via prompting: slight, significant, and comical. We provide the full prompts used in our study in our GitHub repository. For example, for a name like Bob Green, a slight modification implies a small tweak to another real name (Rob Greene), whereas a significant modification produces a similar but fictitious name (Bilgorn Grevalle), and a comical modification is an absurd variant (Blob Lawnface). For a city name like Miami, a slight modification changes the name of the most similar city (Fort Lauderdale), a significant modification produces a fictitious city name (Marisole), and a comical modification produces an absurd variant (Miameme). Because of differences in how each modified fact might appear in the retrieved text, we utilize GPT-4o to generate the perturbed excerpts for",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Reference & 1976 & 1976 & v",
        "evidence_page_no": 4,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11df13ab-c687-4c27-a89c-ae081512ae13",
        "questions": "Which model has the lowest context bias value in the tabular data comparing six top-performing language models?",
        "answers": "Claude Opus",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Claude Opus & \\mathbf{0 . 1 5 7}(0.141,0.174)",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11e4a4de-bcd4-423a-adc5-4265f82e4074",
        "questions": "For the model Gemini 1.5, what is the range of its prior bias as shown in the performance comparison table of six language models?",
        "answers": "0.029 to 0.046",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Gemini 1.5 & $0.037(0.029,0.046)",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11ebcd49-428a-4c22-95fd-b76a19fbeb1d",
        "questions": "Considering the accuracy metric, what is the difference between the maximum and minimum accuracy rates among the models listed in the table for language model performance comparison?",
        "answers": "0.243",
        "context": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. TrustLLM: Trustworthiness in large language models. January 2024.\n\nRhiannon Williams. Why google's AI overviews gets things wrong. MIT Technology Review, May 2024.\n\nChong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556, 2024.\n\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint arXiv:2305.13300, 2023.\n\nZihan Zhang, Meng Fang, and Ling Chen. RetrievalQA: Assessing adaptive Retrieval-Augmented generation for short-form Open-Domain question answering. February 2024.\n\nQinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, and Stephen Gould. The first to know: How token distributions reveal hidden knowledge in large Vision-Language models? March 2024.\n\n\nA Appendix\n\n\\begin{tabular}{lccc}\n  Model & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ & Accuracy $\\uparrow$ \\\\\n  Claude Opus & $\\mathbf{0 . 1 5 7}(0.141,0.174)$ & $\\mathbf{0 . 0 2 1}(0.014,0.029)$ & $\\mathbf{0 . 7 4 3}(0.723,0.763)$ \\\\\nClaude Sonnet & $0.201(0.184,0.215)$ & $0.025(0.018,0.033)$ & $0.658(0.641,0.678)$ \\\\\nGemini 1.5 & $0.245(0.231,0.260)$ & $0.037(0.029,0.046)$ & $0.624(0.607,0.641)$ \\\\\nGPT-4o & $0.304(0.287,0.321)$ & $0.021(0.013,0.028)$ & $0.615(0.594,0.633)$ \\\\\nGPT-3.5 & $0.313(0.298,0.329)$ & $0.028(0.021,0.036)$ & $0.539(0.522,0.558)$ \\\\\nLlama-3 & $0.264(0.250,0.280)$ & $0.021(0.015,0.027)$ & $0.500(0.482,0.518)$ \\\\\n \n\\end{tabular}\n\nTable 4: We compare six top-performing models across three metrics. Context bias is when the model chooses the context answer when its prior was correct. Prior bias is when the model chooses its prior when the context answer is correct. Finally, accuracy is a straightforward measure of the fraction of times it can produce the correct answer. We find that Claude Opus performs the best across all metrics with a context bias rate of 0.157 .",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "Claude Opus & \\mathbf{0 . 7 4 3}(0.723,0.763)...Llama-3 & $0.500(0.482,0.518)",
        "evidence_page_no": 10,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11ecdee5-6f5f-4b4f-b305-23a06ac27493",
        "questions": "What is the accuracy achieved by GPT-4o using calibrated token probability correction?",
        "answers": "0.754 (0.733, 0.775)",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Easy",
        "answer_form": "Numeric",
        "evidence_source": "table",
        "evidence_context": "GPT-4o & Calibrated Token Prob. Correction & \\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11f0458c-f767-4ead-aa00-246fa6034d21",
        "questions": "How does the context bias of Llama-3 change from no correction to token probability correction?",
        "answers": "Decreases from 0.264 (0.250, 0.279) to 0.235 (0.220, 0.249)",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Medium",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Llama-3 & Token Probability Correction & 0.556(0.537,0.574) & 0.235(0.220,0.249)",
        "evidence_page_no": 7,
        "is_extra_qa": true
    },
    {
        "doc_name": "paper/2404.10198v2",
        "ID": "11fc637a-2d7e-4291-a19f-c948095d4879",
        "questions": "Which model exhibits the highest prior bias when using calibrated token probability correction?",
        "answers": "Llama-3 with 0.188 (0.173, 0.204)",
        "context": "Figure 4: We additionally observe an inverse relationship between the context preference rate (yaxis) and the model's prior response probability (x-axis). Context preference rate is defined as the proportion of responses that align with the information presented in the prompt as context. The model's prior response probability is computed from the average log probability of the response tokens queried without context. Each plot visualizes the prior probability (grouped into 10 bins) against the context preference rate, along with the best-fit trend line and slope. Models that allow access to token probabilities are shown.\n\\begin{tabular}{llccc}\n  Model & Correction & Accuracy $\\uparrow$ & Context Bias $\\downarrow$ & Prior Bias $\\downarrow$ \\\\\n  \\multirow{3}{*}{ GPT-4o } & No correction (Baseline) & $0.615(0.595,0.636)$ & $0.304(0.287,0.321)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 4 , 0 . 0 2 8 )}$ \\\\\n& Token Probability Correction & $0.693(0.672,0.714)$ & $0.194(0.177,0.210)$ & $0.043(0.032,0.053)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 5 4 ( 0 . 7 3 3 , 0 . 7 7 5 )}$ & $\\mathbf{0 . 1 0 7 ( 0 . 0 9 3 , 0 . 1 2 2 )}$ & $0.085(0.072,0.098)$ \\\\\n  \\multirow{3}{*}{ GPT-3.5 } & No correction (Baseline) & $0.539(0.521,0.557)$ & $0.313(0.298,0.328)$ & $\\mathbf{0 . 0 2 8 ( 0 . 0 2 1 , 0 . 0 3 6 )}$ \\\\\n& Token Probability Correction & $0.596(0.575,0.616)$ & $0.253(0.237,0.269)$ & $0.056(0.046,0.067)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 7 0 1 ( 0 . 6 7 8 , 0 . 7 2 2 )}$ & $\\mathbf{0 . 1 1 0 ( 0 . 0 9 8 , 0 . 1 2 4 )}$ & $0.147(0.132,0.164)$ \\\\\n  \\multirow{3}{*}{ Llama-3 } & No correction (Baseline) & $0.500(0.483,0.515)$ & $0.264(0.250,0.279)$ & $\\mathbf{0 . 0 2 1}(\\mathbf{0 . 0 1 5 , 0 . 0 2 7 )}$ \\\\\n& Token Probability Correction & $0.556(0.537,0.574)$ & $0.235(0.220,0.249)$ & $0.046(0.037,0.055)$ \\\\\n& Calibrated Token Prob. Correction & $\\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9})$ & $\\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )}$ & $0.188(0.173,0.204)$ \\\\\n \n\\end{tabular}\n\nTable 3: For models which provide token probabilities, we evaluate the accuracy, context bias, and prior bias under three conditions: (1) No correction, which is the baseline result from this paper, (2) the token probability correction, and (3) the calibrated token probability correction.\n\nProbability Correction. We find that calibrated token probability correction improves all models' overall accuracy by $14 \\%$ and context bias by $20 \\%$. At the same time, this introduces more prior bias, from $2 \\%$ to $8.5 \\%$. However, this method outperforms a baseline of randomly replacing the final response with its prior - at the same bias rate of $8.5 \\%$, the random baseline has an accuracy of $57.5 \\%$ as compared to the $75.4 \\%$ from the method. While this paper focuses on developing the ClashEval benchmark, these results suggest that probability calibration is a promising approach to reduce prior and context bias deserving further investigation. It also is a natural baseline for future methods.",
        "doc_type": "paper",
        "difficulty_level": "Hard",
        "answer_form": "Short Answer",
        "evidence_source": "table",
        "evidence_context": "Llama-3 & Calibrated Token Prob. Correction & \\mathbf{0 . 6 4 9 ( 0 . 6 2 7 , 0 . 6 6 9}) & \\mathbf{0 . 1 1 1 ( 0 . 0 9 9 , 0 . 1 2 2 )} & 0.188(0.173,0.204)",
        "evidence_page_no": 7,
        "is_extra_qa": true
    }
]